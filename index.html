<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-17T00:00:00Z">2025-01-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">61</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BoK: Introducing Bag-of-Keywords Loss for Interpretable Dialogue
  Response Generation <span class="chip">SIGDIAL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suvodip Dey, Maunendra Sankar Desarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard language modeling (LM) loss by itself has been shown to be
inadequate for effective dialogue modeling. As a result, various training
approaches, such as auxiliary loss functions and leveraging human feedback, are
being adopted to enrich open-domain dialogue systems. One such auxiliary loss
function is Bag-of-Words (BoW) loss, defined as the cross-entropy loss for
predicting all the words/tokens of the next utterance. In this work, we propose
a novel auxiliary loss named Bag-of-Keywords (BoK) loss to capture the central
thought of the response through keyword prediction and leverage it to enhance
the generation of meaningful and interpretable responses in open-domain
dialogue systems. BoK loss upgrades the BoW loss by predicting only the
keywords or critical words/tokens of the next utterance, intending to estimate
the core idea rather than the entire response. We incorporate BoK loss in both
encoder-decoder (T5) and decoder-only (DialoGPT) architecture and train the
models to minimize the weighted sum of BoK and LM (BoK-LM) loss. We perform our
experiments on two popular open-domain dialogue datasets, DailyDialog and
Persona-Chat. We show that the inclusion of BoK loss improves the dialogue
generation of backbone models while also enabling post-hoc interpretability. We
also study the effectiveness of BoK-LM loss as a reference-free metric and
observe comparable performance to the state-of-the-art metrics on various
dialogue evaluation datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGDIAL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large language model</span>s for automated scholarly paper <span class="highlight-title">review</span>: A <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenzhen Zhuang, Jiandong Chen, Hongfeng Xu, Yuwen Jiang, Jialiang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly impacted human society,
influencing various domains. Among them, academia is not simply a domain
affected by LLMs, but it is also the pivotal force in the development of LLMs.
In academic publications, this phenomenon is represented during the
incorporation of LLMs into the peer review mechanism for reviewing manuscripts.
We proposed the concept of automated scholarly paper review (ASPR) in our
previous paper. As the incorporation grows, it now enters the coexistence phase
of ASPR and peer review, which is described in that paper. LLMs hold
transformative potential for the full-scale implementation of ASPR, but they
also pose new issues and challenges that need to be addressed. In this survey
paper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin
with a survey to find out which LLMs are used to conduct ASPR. Then, we review
what ASPR-related technological bottlenecks have been solved with the
incorporation of LLM technology. After that, we move on to explore new methods,
new datasets, new source code, and new online systems that come with LLMs for
ASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and
investigate the attitudes and reactions of publishers and academia to ASPR.
Lastly, we discuss the challenges associated with the development of LLMs for
ASPR. We hope this survey can serve as an inspirational reference for the
researchers and promote the progress of ASPR for its actual implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Autoregressive <span class="highlight-title">Transformer</span>s: Combining Byte-~and Word-Level
  Processing for Robust, Adaptable Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pit Neitemeier, Björn Deiseroth, Constantin Eichenberg, Lukas Balles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is a fundamental step in natural language processing, breaking
text into units that computational models can process. While learned subword
tokenizers have become the de-facto standard, they present challenges such as
large vocabularies, limited adaptability to new domains or languages, and
sensitivity to spelling errors and variations. To overcome these limitations,
we investigate a hierarchical architecture for autoregressive language
modelling that combines character-level and word-level processing. It employs a
lightweight character-level encoder to convert character sequences into word
embeddings, which are then processed by a word-level backbone model and decoded
back into characters via a compact character-level decoder. This method retains
the sequence compression benefits of word-level tokenization without relying on
a rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion
parameters, that hierarchical transformers match the downstream task
performance of subword-tokenizer-based models while exhibiting significantly
greater robustness to input perturbations. Additionally, during continued
pretraining on an out-of-domain language, our model trains almost twice as
fast, achieves superior performance on the target language, and retains more of
its previously learned knowledge. Hierarchical transformers pave the way for
NLP systems that are more robust, flexible, and generalizable across languages
and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Processing of Privacy Policies: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrick Adhikari, Sanchari Das, Rinku Dewri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) is an essential subset of artificial
intelligence. It has become effective in several domains, such as healthcare,
finance, and media, to identify perceptions, opinions, and misuse, among
others. Privacy is no exception, and initiatives have been taken to address the
challenges of usable privacy notifications to users with the help of NLP. To
this aid, we conduct a literature review by analyzing 109 papers at the
intersection of NLP and privacy policies. First, we provide a brief
introduction to privacy policies and discuss various facets of associated
problems, which necessitate the application of NLP to elevate the current state
of privacy notices and disclosures to users. Subsequently, we a) provide an
overview of the implementation and effectiveness of NLP approaches for better
privacy policy communication; b) identify the methodologies that can be further
enhanced to provide robust privacy policies; and c) identify the gaps in the
current state-of-the-art research. Our systematic analysis reveals that several
research papers focus on annotating and classifying privacy texts for analysis
but need to adequately dwell on other aspects of NLP applications, such as
summarization. More specifically, ample research opportunities exist in this
domain, covering aspects such as corpus generation, summarization vectors,
contextualized word embedding, identification of privacy-relevant statement
categories, fine-grained classification, and domain-specific model tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Preventing Overreliance on Task-Oriented Conversational AI
  Through Accountability Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suvodip Dey, Yi-Jyun Sun, Gokhan Tur, Dilek Hakkani-Tur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent LLMs have enabled significant advancements for conversational agents.
However, they are also well-known to hallucinate, i.e., they often produce
responses that seem plausible but are not factually correct. On the other hand,
users tend to over-rely on LLM-based AI agents; they accept the AI's suggestion
even when it is wrong. Adding good friction, such as explanations or getting
user confirmations, has been proposed as a mitigation in AI-supported
decision-making systems. In this paper, we propose an accountability model for
LLM-based task-oriented dialogue agents to address user overreliance via
friction turns in cases of model uncertainty and errors associated with
dialogue state tracking (DST). The accountability model is an augmented LLM
with an additional accountability head, which functions as a binary classifier
to predict the slots of the dialogue states. We perform our experiments with
three backbone LLMs (Llama, Mistral, Gemma) on two established task-oriented
datasets (MultiWOZ and Snips). Our empirical findings demonstrate that this
approach not only enables reliable estimation of AI agent errors but also
guides the LLM decoder in generating more accurate actions. We observe around
3% absolute improvement in joint goal accuracy by incorporating accountability
heads in modern LLMs for the MultiWOZ dataset. We also show that this method
enables the agent to self-correct its actions, further boosting its performance
by 3%. Finally, we discuss the application of accountability modeling to
prevent user overreliance by introducing friction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Protein Science in the Era of <span class="highlight-title">Large Language Model</span>s (LLMs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Fan, Yi Zhou, Shijie Wang, Yuyao Yan, Hui Liu, Qian Zhao, Le Song, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering the significance of proteins, computational protein science has
always been a critical scientific field, dedicated to revealing knowledge and
developing applications within the protein sequence-structure-function
paradigm. In the last few decades, Artificial Intelligence (AI) has made
significant impacts in computational protein science, leading to notable
successes in specific protein modeling tasks. However, those previous AI models
still meet limitations, such as the difficulty in comprehending the semantics
of protein sequences, and the inability to generalize across a wide range of
protein modeling tasks. Recently, LLMs have emerged as a milestone in AI due to
their unprecedented language processing & generalization capability. They can
promote comprehensive progress in fields rather than solving individual tasks.
As a result, researchers have actively introduced LLM techniques in
computational protein science, developing protein Language Models (pLMs) that
skillfully grasp the foundational knowledge of proteins and can be effectively
generalized to solve a diversity of sequence-structure-function reasoning
problems. While witnessing prosperous developments, it's necessary to present a
systematic overview of computational protein science empowered by LLM
techniques. First, we summarize existing pLMs into categories based on their
mastered protein knowledge, i.e., underlying sequence patterns, explicit
structural and functional information, and external scientific languages.
Second, we introduce the utilization and adaptation of pLMs, highlighting their
remarkable achievements in promoting protein structure prediction, protein
function prediction, and protein design studies. Then, we describe the
practical application of pLMs in antibody design, enzyme design, and drug
discovery. Finally, we specifically discuss the promising future directions in
this fast-growing field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple but Effective Closed-form Solution for Extreme Multi-label
  Learning <span class="chip">ECIR25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuma Onishi, Katsuhiko Hayashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extreme multi-label learning (XML) is a task of assigning multiple labels
from an extremely large set of labels to each data instance. Many current
high-performance XML models are composed of a lot of hyperparameters, which
complicates the tuning process. Additionally, the models themselves are adapted
specifically to XML, which complicates their reimplementation. To remedy this
problem, we propose a simple method based on ridge regression for XML. The
proposed method not only has a closed-form solution but also is composed of a
single hyperparameter. Since there are no precedents on applying ridge
regression to XML, this paper verified the performance of the method by using
various XML benchmark datasets. Furthermore, we enhanced the prediction of
low-frequency labels in XML, which hold informative content. This prediction is
essential yet challenging because of the limited amount of data. Here, we
employed a simple frequency-based weighting. This approach greatly simplifies
the process compared with existing techniques. Experimental results revealed
that it can achieve levels of performance comparable to, or even exceeding,
those of models with numerous hyperparameters. Additionally, we found that the
frequency-based weighting significantly improved the predictive performance for
low-frequency labels, while requiring almost no changes in implementation. The
source code for the proposed method is available on github at
https://github.com/cars1015/XML-ridge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages, Accepted at ECIR25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-stage Training of Bilingual Islamic LLM for Neural Passage
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vera Pavlova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the use of Natural Language Processing (NLP) technology
within the Islamic domain, focusing on developing an Islamic neural retrieval
model. By leveraging the robust XLM-R model, the research employs a language
reduction technique to create a lightweight bilingual large language model
(LLM). Our approach for domain adaptation addresses the unique challenges faced
in the Islamic domain, where substantial in-domain corpora exist only in Arabic
while limited in other languages, including English.
  The work utilizes a multi-stage training process for retrieval models,
incorporating large retrieval datasets, such as MS MARCO, and smaller,
in-domain datasets to improve retrieval performance. Additionally, we have
curated an in-domain retrieval dataset in English by employing data
augmentation techniques and involving a reliable Islamic source. This approach
enhances the domain-specific dataset for retrieval, leading to further
performance gains.
  The findings suggest that combining domain adaptation and a multi-stage
training method for the bilingual Islamic neural retrieval model enables it to
outperform monolingual models on downstream retrieval tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair
  Language Modeling and Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomasz Limisiewicz, David Mareček, Tomáš Musil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigation of biases, such as language models' reliance on gender
stereotypes, is a crucial endeavor required for the creation of reliable and
useful language technology. The crucial aspect of debiasing is to ensure that
the models preserve their versatile capabilities, including their ability to
solve language tasks and equitably represent various genders. To address this
issue, we introduce a streamlined Dual Dabiasing Algorithm through Model
Adaptation (2DAMA). Novel Dual Debiasing enables robust reduction of
stereotypical bias while preserving desired factual gender information encoded
by language models. We show that 2DAMA effectively reduces gender bias in
English and is one of the first approaches facilitating the mitigation of
stereotypical tendencies in translation. The proposed method's key advantage is
the preservation of factual gender cues, which are useful in a wide range of
natural language processing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling
  under Long-Context Scenario 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing large language models (LLMs) with real-time APIs can help generate
more accurate and up-to-date responses. However, evaluating the function
calling abilities of LLMs in real-world scenarios remains under-explored due to
the complexity of data collection and evaluation. In this work, we introduce
ComplexFuncBench, a benchmark for complex function calling across five
real-world scenarios. Compared to existing benchmarks, ComplexFuncBench
encompasses multi-step and constrained function calling, which requires
long-parameter filing, parameter value reasoning, and 128k long context.
Additionally, we propose an automatic framework, ComplexEval, for
quantitatively evaluating complex function calling tasks. Through comprehensive
experiments, we demonstrate the deficiencies of state-of-the-art LLMs in
function calling and suggest future directions for optimizing these
capabilities. The data and code are available at
\url{https://github.com/THUDM/ComplexFuncBench}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BBPOS: <span class="highlight-title">BERT</span>-based Part-of-Speech Tagging for Uzbek 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Latofat Bobojonova, Arofat Akhundjanova, Phil Ostheimer, Sophie Fellenz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper advances NLP research for the low-resource Uzbek language by
evaluating two previously untested monolingual Uzbek BERT models on the
part-of-speech (POS) tagging task and introducing the first publicly available
UPOS-tagged benchmark dataset for Uzbek. Our fine-tuned models achieve 91%
average accuracy, outperforming the baseline multi-lingual BERT as well as the
rule-based tagger. Notably, these models capture intermediate POS changes
through affixes and demonstrate context sensitivity, unlike existing rule-based
taggers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Author-Specific Linguistic Patterns Unveiled: A Deep Learning Study on
  Word Class Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Krauss, Achim Schilling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning methods have been increasingly applied to computational
linguistics to uncover patterns in text data. This study investigates
author-specific word class distributions using part-of-speech (POS) tagging and
bigram analysis. By leveraging deep neural networks, we classify literary
authors based on POS tag vectors and bigram frequency matrices derived from
their works. We employ fully connected and convolutional neural network
architectures to explore the efficacy of unigram and bigram-based
representations. Our results demonstrate that while unigram features achieve
moderate classification accuracy, bigram-based models significantly improve
performance, suggesting that sequential word class patterns are more
distinctive of authorial style. Multi-dimensional scaling (MDS) visualizations
reveal meaningful clustering of authors' works, supporting the hypothesis that
stylistic nuances can be captured through computational methods. These findings
highlight the potential of deep learning and linguistic feature analysis for
author profiling and literary studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OMoE: Diversifying Mixture of Low-Rank Adaptation by Orthogonal
  Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyuan Feng, Zhiqiang Pu, Tianyi Hu, Dongmin Li, Xiaolin Ai, Huimu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building mixture-of-experts (MoE) architecture for Low-rank adaptation (LoRA)
is emerging as a potential direction in parameter-efficient fine-tuning (PEFT)
for its modular design and remarkable performance. However, simply stacking the
number of experts cannot guarantee significant improvement. In this work, we
first conduct qualitative analysis to indicate that experts collapse to similar
representations in vanilla MoE, limiting the capacity of modular design and
computational efficiency. Ulteriorly, Our analysis reveals that the performance
of previous MoE variants maybe limited by a lack of diversity among experts.
Motivated by these findings, we propose Orthogonal Mixture-of-Experts (OMoE), a
resource-efficient MoE variant that trains experts in an orthogonal manner to
promote diversity. In OMoE, a Gram-Schmidt process is leveraged to enforce that
the experts' representations lie within the Stiefel manifold. By applying
orthogonal constraints directly to the architecture, OMoE keeps the learning
objective unchanged, without compromising optimality. Our method is simple and
alleviates memory bottlenecks, as it incurs minimal experts compared to vanilla
MoE models. Experiments on diverse commonsense reasoning benchmarks demonstrate
that OMoE can consistently achieve stable and efficient performance improvement
when compared with the state-of-the-art methods while significantly reducing
the number of required experts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSTS: A Multimodal Safety Test Suite for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Röttger, Giuseppe Attanasio, Felix Friedrich, Janis Goldzycher, Alicia Parrish, Rishabh Bhardwaj, Chiara Di Bonaventura, Roman Eng, Gaia El Khoury Geagea, Sujata Goswami, Jieun Han, Dirk Hovy, Seogyeong Jeong, Paloma Jeretič, Flor Miriam Plaza-del-Arco, Donya Rooein, Patrick Schramowski, Anastassia Shaitarova, Xudong Shen, Richard Willats, Andrea Zugarini, Bertie Vidgen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs), which process image and text inputs, are
increasingly integrated into chat assistants and other consumer AI
applications. Without proper safeguards, however, VLMs may give harmful advice
(e.g. how to self-harm) or encourage unsafe behaviours (e.g. to consume drugs).
Despite these clear hazards, little work so far has evaluated VLM safety and
the novel risks created by multimodal inputs. To address this gap, we introduce
MSTS, a Multimodal Safety Test Suite for VLMs. MSTS comprises 400 test prompts
across 40 fine-grained hazard categories. Each test prompt consists of a text
and an image that only in combination reveal their full unsafe meaning. With
MSTS, we find clear safety issues in several open VLMs. We also find some VLMs
to be safe by accident, meaning that they are safe because they fail to
understand even simple test prompts. We translate MSTS into ten languages,
showing non-English prompts to increase the rate of unsafe model responses. We
also show models to be safer when tested with text only rather than multimodal
prompts. Finally, we explore the automation of VLM safety assessments, finding
even the best safety classifiers to be lacking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Speech Recognition for Sanskrit with Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bidit Sadhukhan, Swami Punyeshwarananda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sanskrit, one of humanity's most ancient languages, has a vast collection of
books and manuscripts on diverse topics that have been accumulated over
millennia. However, its digital content (audio and text), which is vital for
the training of AI systems, is profoundly limited. Furthermore, its intricate
linguistics make it hard to develop robust NLP tools for wider accessibility.
Given these constraints, we have developed an automatic speech recognition
model for Sanskrit by employing transfer learning mechanism on OpenAI's Whisper
model. After carefully optimising the hyper-parameters, we obtained promising
results with our transfer-learned model achieving a word error rate of 15.42%
on Vaksancayah dataset. An online demo of our model is made available for the
use of public and to evaluate its performance firsthand thereby paving the way
for improved accessibility and technological support for Sanskrit learning in
the modern era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper has been accepted at the 4th International Conference on
  Computer, Communication, Control & Information Technology (C3IT), Hooghly,
  India, 2024, pp. 1-5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-guided Self-reflection for Zero-shot Hallucination Detection
  in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Liu, Xinlong Chen, Yue Ding, Shizhen Xu, Shu Wu, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination has emerged as a significant barrier to the effective
application of Large Language Models (LLMs). In this work, we introduce a novel
Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination
detection in LLMs. The AGSER method utilizes attention contributions to
categorize the input query into attentive and non-attentive queries. Each query
is then processed separately through the LLMs, allowing us to compute
consistency scores between the generated responses and the original answer. The
difference between the two consistency scores serves as a hallucination
estimator. In addition to its efficacy in detecting hallucinations, AGSER
notably reduces computational complexity, requiring only three passes through
the LLM and utilizing two sets of tokens. We have conducted extensive
experiments with four widely-used LLMs across three different hallucination
benchmarks, demonstrating that our approach significantly outperforms existing
methods in zero-shot hallucination detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-as-Judge for Factual Summarization of Long Narratives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeonseok Jeong, Minsoo Kim, Seung-won Hwang, Byung-Hak Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated near-human performance in
summarization tasks based on traditional metrics such as ROUGE and BERTScore.
However, these metrics do not adequately capture critical aspects of
summarization quality, such as factual accuracy, particularly for long
narratives (>100K tokens). Recent advances, such as LLM-as-a-Judge, address the
limitations of metrics based on lexical similarity but still exhibit factual
inconsistencies, especially in understanding character relationships and
states. In this work, we introduce NarrativeFactScore, a novel
"Agent-as-a-Judge" framework for evaluating and refining summaries. By
leveraging a Character Knowledge Graph (CKG) extracted from input and generated
summaries, NarrativeFactScore assesses the factual consistency and provides
actionable guidance for refinement, such as identifying missing or erroneous
facts. We demonstrate the effectiveness of NarrativeFactScore through a
detailed workflow illustration and extensive validation on widely adopted
benchmarks, achieving superior performance compared to competitive methods. Our
results highlight the potential of agent-driven evaluation systems to improve
the factual reliability of LLM-generated summaries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RichSpace: Enriching Text-to-Video <span class="highlight-title">Prompt</span> Space via Text Embedding
  Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuefan Cao, Chengyue Gong, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video generation models have made impressive progress, but they still
struggle with generating videos with complex features. This limitation often
arises from the inability of the text encoder to produce accurate embeddings,
which hinders the video generation model. In this work, we propose a novel
approach to overcome this challenge by selecting the optimal text embedding
through interpolation in the embedding space. We demonstrate that this method
enables the video generation model to produce the desired videos. Additionally,
we introduce a simple algorithm using perpendicular foot embeddings and cosine
similarity to identify the optimal interpolation embedding. Our findings
highlight the importance of accurate text embeddings and offer a pathway for
improving text-to-video generation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Multi-Turn Interaction Capabilities of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhang, Xinyi Dai, Yaxiong Wu, Qu Yang, Yasheng Wang, Ruiming Tang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-turn interaction in the dialogue system research refers to a system's
ability to maintain context across multiple dialogue turns, enabling it to
generate coherent and contextually relevant responses. Recent advancements in
large language models (LLMs) have significantly expanded the scope of
multi-turn interaction, moving beyond chatbots to enable more dynamic agentic
interactions with users or environments. In this paper, we provide a focused
review of the multi-turn capabilities of LLMs, which are critical for a wide
range of downstream applications, including conversational search and
recommendation, consultation services, and interactive tutoring. This survey
explores four key aspects: (1) the core model capabilities that contribute to
effective multi-turn interaction, (2) how multi-turn interaction is evaluated
in current practice, (3) the general algorithms used to enhance multi-turn
interaction, and (4) potential future directions for research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Draft Version, 14 pages, Ongoing refinement over time</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRAG: A Flexible Modular Framework for Retrieval-Augmented Generation
  based on <span class="highlight-title">Knowledge Graph</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengyi Gao, Yukun Cao, Hairu Wang, Ao Ke, Yuan Feng, Xike Xie, S Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To mitigate the hallucination and knowledge deficiency in large language
models (LLMs), Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG)
has shown promising potential by utilizing KGs as external resource to enhance
LLMs reasoning.However, existing KG-RAG approaches struggle with a trade-off
between flexibility and retrieval quality.Modular methods prioritize
flexibility by avoiding the use of KG-fine-tuned models during retrieval,
leading to fixed retrieval strategies and suboptimal retrieval
quality.Conversely, coupled methods embed KG information within models to
improve retrieval quality, but at the expense of flexibility.In this paper, we
propose a novel flexible modular KG-RAG framework, termed FRAG, which
synergizes the advantages of both approaches.FRAG estimates the hop range of
reasoning paths based solely on the query and classify it as either simple or
complex.To match the complexity of the query, tailored pipelines are applied to
ensure efficient and accurate reasoning path retrieval, thus fostering the
final reasoning process.By using the query text instead of the KG to infer the
structural information of reasoning paths and employing adaptable retrieval
strategies, FRAG improves retrieval quality while maintaining
flexibility.Moreover, FRAG does not require extra LLMs fine-tuning or calls,
significantly boosting efficiency and conserving resources.Extensive
experiments show that FRAG achieves state-of-the-art performance with high
efficiency and low resource consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sympathy over Polarization: A Computational Discourse Analysis of Social
  Media Posts about the July 2024 Trump Assassination Attempt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingcheng Zeng, Guanhong Liu, Zhaoqian Xue, Diego Ford, Rob Voigt, Loni Hagen, Lingyao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On July 13, 2024, at the Trump rally in Pennsylvania, someone attempted to
assassinate Republican Presidential Candidate Donald Trump. This attempt
sparked a large-scale discussion on social media. We collected posts from X
(formerly known as Twitter) one week before and after the assassination attempt
and aimed to model the short-term effects of such a ``shock'' on public
opinions and discussion topics. Specifically, our study addresses three key
questions: first, we investigate how public sentiment toward Donald Trump
shifts over time and across regions (RQ1) and examine whether the assassination
attempt itself significantly affects public attitudes, independent of the
existing political alignments (RQ2). Finally, we explore the major themes in
online conversations before and after the crisis, illustrating how discussion
topics evolved in response to this politically charged event (RQ3). By
integrating large language model-based sentiment analysis,
difference-in-differences modeling, and topic modeling techniques, we find that
following the attempt the public response was broadly sympathetic to Trump
rather than polarizing, despite baseline ideological and regional disparities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Indigenous Languages Spoken in Argentina: A <span class="highlight-title">Survey</span> of NLP and Speech
  Resources <span class="chip">COLING</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Belu Ticona, Fernando Carranza, Viviana Cotik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Argentina has a diverse, yet little-known, Indigenous language heritage. Most
of these languages are at risk of disappearing, resulting in a significant loss
of world heritage and cultural knowledge. Currently, no unified information on
speakers and computational tools is available for these languages. In this
work, we present a systematization of the Indigenous languages spoken in
Argentina, along with national demographic data on the country's Indigenous
population. The languages are classified into seven families: Mapuche,
Tup\'i-Guaran\'i, Guaycur\'u, Quechua, Mataco-Mataguaya, Aymara, and Chon. We
also provide an introductory survey of the computational resources available
for these languages, whether or not they are specifically developed for
Argentine varieties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING Main 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Passage Segmentation of Documents for Extractive Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zuhong Liu, Charles-Elie Simon, Fabien Caspani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has proven effective in open-domain
question answering. However, the chunking process, which is essential to this
pipeline, often receives insufficient attention relative to retrieval and
synthesis components. This study emphasizes the critical role of chunking in
improving the performance of both dense passage retrieval and the end-to-end
RAG pipeline. We then introduce the Logits-Guided Multi-Granular Chunker
(LGMGC), a novel framework that splits long documents into contextualized,
self-contained chunks of varied granularity. Our experimental results,
evaluated on two benchmark datasets, demonstrate that LGMGC not only improves
the retrieval step but also outperforms existing chunking methods when
integrated into a RAG pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering <span class="highlight-title">Large Language Model</span>s with Feature Guided Activation Additions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Soo, Wesley Teng, Chandrasekaran Balaganesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective and reliable control over large language model (LLM) behavior is a
significant challenge. While activation steering methods, which add steering
vectors to a model's hidden states, are a promising approach, existing
techniques often lack precision and interpretability in how they influence
model outputs. We introduce Feature Guided Activation Additions (FGAA), a novel
activation steering method that leverages insights from Contrastive Activation
Addition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating
in the latent space of a Sparse Autoencoder (SAE) and employing optimization
techniques to select desired SAE features, FGAA constructs precise steering
vectors that provide better steering effects while maintaining coherence of
steered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B
models across various steering tasks demonstrate that FGAA outperforms existing
steering methods of CAA, SAE decoder steering, and SAE-TS. Our results also
highlight important trade-offs between steering scale and general model
capabilities that are consistent across all tested steering methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 maintext pages, 14 appendix pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialogue Benchmark Generation from <span class="highlight-title">Knowledge Graph</span>s with Cost-Effective
  Retrieval-Augmented LLMs <span class="chip">SIGMOD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reham Omar, Omij Mangukiya, Essam Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue benchmarks are crucial in training and evaluating chatbots engaging
in domain-specific conversations. Knowledge graphs (KGs) represent semantically
rich and well-organized data spanning various domains, such as DBLP, DBpedia,
and YAGO. Traditionally, dialogue benchmarks have been manually created from
documents, neglecting the potential of KGs in automating this process. Some
question-answering benchmarks are automatically generated using extensive
preprocessing from KGs, but they do not support dialogue generation. This paper
introduces Chatty-Gen, a novel multi-stage retrieval-augmented generation
platform for automatically generating high-quality dialogue benchmarks tailored
to a specific domain using a KG. Chatty-Gen decomposes the generation process
into manageable stages and uses assertion rules for automatic validation
between stages. Our approach enables control over intermediate results to
prevent time-consuming restarts due to hallucinations. It also reduces reliance
on costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront
processing of the entire KG using efficient query-based retrieval to find
representative subgraphs based on the dialogue context. Our experiments with
several real and large KGs demonstrate that Chatty-Gen significantly
outperforms state-of-the-art systems and ensures consistent model and system
performance across multiple LLMs of diverse capabilities, such as GPT-4o,
Gemini 1.5, Llama 3, and Mistral.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is publsihed in SIGMOD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Automatic Speech Recognition And Structure Learning For Better
  Speech Understanding <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiliang Hu, Zuchao Li, Mengjia Shen, Haojun Ai, Sheng Li, Jun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken language understanding (SLU) is a structure prediction task in the
field of speech. Recently, many works on SLU that treat it as a
sequence-to-sequence task have achieved great success. However, This method is
not suitable for simultaneous speech recognition and understanding. In this
paper, we propose a joint speech recognition and structure learning framework
(JSRSL), an end-to-end SLU model based on span, which can accurately transcribe
speech and extract structured content simultaneously. We conduct experiments on
name entity recognition and intent classification using the Chinese dataset
AISHELL-NER and the English dataset SLURP. The results show that our proposed
method not only outperforms the traditional sequence-to-sequence method in both
transcription and extraction capabilities but also achieves state-of-the-art
performance on the two datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual
  Information in Long-form Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03545v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03545v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Samarinas, Alexander Krubner, Alireza Salemi, Youngwoo Kim, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ICAT, an evaluation framework for measuring coverage of
diverse factual information in long-form text generation. ICAT breaks down a
long output text into a list of atomic claims and not only verifies each claim
through retrieval from a (reliable) knowledge source, but also computes the
alignment between the atomic factual claims and various aspects expected to be
presented in the output. We study three implementations of the ICAT framework,
each with a different assumption on the availability of aspects and alignment
method. By adopting data from the diversification task in the TREC Web Track
and the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong
correlation with human judgments and provide comprehensive evaluation across
multiple state-of-the-art LLMs. Our framework further offers interpretable and
fine-grained analysis of diversity and coverage. Its modular design allows for
easy adaptation to different domains and datasets, making it a valuable tool
for evaluating the qualitative aspects of long-form responses produced by LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuradicon: operational representation learning of neuroimaging reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.10021v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.10021v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Watkins, Robert Gray, Adam Julius, Yee-Haur Mah, Walter H. L. Pinaya, Paul Wright, Ashwani Jha, Holger Engleitner, Jorge Cardoso, Sebastien Ourselin, Geraint Rees, Rolf Jaeger, Parashkev Nachev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiological reports typically summarize the content and interpretation of
imaging studies in unstructured form that precludes quantitative analysis. This
limits the monitoring of radiological services to throughput undifferentiated
by content, impeding specific, targeted operational optimization. Here we
present Neuradicon, a natural language processing (NLP) framework for
quantitative analysis of neuroradiological reports. Our framework is a hybrid
of rule-based and artificial intelligence models to represent neurological
reports in succinct, quantitative form optimally suited to operational
guidance. We demonstrate the application of Neuradicon to operational
phenotyping of a corpus of 336,569 reports, and report excellent
generalizability across time and two independent healthcare institutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Paraphrase Generation via Controllable Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08938v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08938v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zou, Ziyuan Zhuang, Xiang Geng, Shujian Huang, Jia Liu, Jiajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Paraphrase generation strives to generate high-quality and diverse
expressions of a given text, a domain where diffusion models excel. Though SOTA
diffusion generation reconciles generation quality and diversity, textual
diffusion suffers from a truncation issue that hinders efficiency and quality
control. In this work, we propose \textit{L}atent \textit{D}iffusion
\textit{P}araphraser~(LDP), a novel paraphrase generation by modeling a
controllable diffusion process given a learned latent space. LDP achieves
superior generation efficiency compared to its diffusion counterparts. It can
facilitate only input segments to ensure paraphrase semantics, improving the
results without external features. Experiments show that LDP better reconciles
paraphrase generation quality and diversity than baselines. Further analysis
shows that our method is also helpful to other similar text generations and
domain adaptations
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article has been accepted by Frontiers of Computer Science (FCS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Large Reasoning Models: A <span class="highlight-title">Survey</span> on Scaling LLM Reasoning
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language has long been conceived as an essential tool for human reasoning.
The breakthrough of Large Language Models (LLMs) has sparked significant
research interest in leveraging these models to tackle complex reasoning tasks.
Researchers have moved beyond simple autoregressive token generation by
introducing the concept of "thought" -- a sequence of tokens representing
intermediate steps in the reasoning process. This innovative paradigm enables
LLMs' to mimic complex human reasoning processes, such as tree search and
reflective thinking. Recently, an emerging trend of learning to reason has
applied reinforcement learning (RL) to train LLMs to master reasoning
processes. This approach enables the automatic generation of high-quality
reasoning trajectories through trial-and-error search algorithms, significantly
expanding LLMs' reasoning capacity by providing substantially more training
data. Furthermore, recent studies demonstrate that encouraging LLMs to "think"
with more tokens during test-time inference can further significantly boost
reasoning accuracy. Therefore, the train-time and test-time scaling combined to
show a new research frontier -- a path toward Large Reasoning Model. The
introduction of OpenAI's o1 series marks a significant milestone in this
research direction. In this survey, we present a comprehensive review of recent
progress in LLM reasoning. We begin by introducing the foundational background
of LLMs and then explore the key technical components driving the development
of large reasoning models, with a focus on automated data construction,
learning-to-reason techniques, and test-time scaling. We also analyze popular
open-source projects at building large reasoning models, and conclude with open
challenges and future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Zero-Shot Chinese-English <span class="highlight-title">Code</span>-Switching ASR with kNN-CTC and
  Gated Monolingual Datastores <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03814v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03814v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Zhou, Shiwan Zhao, Hui Wang, Tian-Hao Zhang, Haoqin Sun, Xuechen Wang, Yong Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The kNN-CTC model has proven to be effective for monolingual automatic speech
recognition (ASR). However, its direct application to multilingual scenarios
like code-switching, presents challenges. Although there is potential for
performance improvement, a kNN-CTC model utilizing a single bilingual datastore
can inadvertently introduce undesirable noise from the alternative language. To
address this, we propose a novel kNN-CTC-based code-switching ASR (CS-ASR)
framework that employs dual monolingual datastores and a gated datastore
selection mechanism to reduce noise interference. Our method selects the
appropriate datastore for decoding each frame, ensuring the injection of
language-specific information into the ASR process. We apply this framework to
cutting-edge CTC-based models, developing an advanced CS-ASR system. Extensive
experiments demonstrate the remarkable effectiveness of our gated datastore
mechanism in enhancing the performance of zero-shot Chinese-English CS-ASR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Quantization for Matrix Multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Ordentlich, Yury Polyanskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in machine learning community proposed multiple methods for
performing lossy compression (quantization) of large matrices. This
quantization is important for accelerating matrix multiplication (main
component of large language models), which is often bottlenecked by the speed
of loading these matrices from memory. Unlike classical vector quantization and
rate-distortion theory, the goal of these new compression algorithms is to be
able to approximate not the matrices themselves, but their matrix product.
Specifically, given a pair of real matrices $A,B$ an encoder (compressor) is
applied to each of them independently producing descriptions with $R$ bits per
entry. These representations subsequently are used by the decoder to estimate
matrix product $A^\top B$. In this work, we provide a non-asymptotic lower
bound on the mean squared error of this approximation (as a function of rate
$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,
we construct a universal quantizer based on nested lattices with an explicit
guarantee of approximation error for any (non-random) pair of matrices $A$, $B$
in terms of only Frobenius norms $\|\bar{A}\|_F, \|\bar{B}\|_F$ and
$\|\bar{A}^\top \bar{B}\|_F$, where $\bar{A},\bar{B}$ are versions of $A,B$
with zero-centered columns, respectively. For iid Gaussian matrices our
quantizer achieves the lower bound and is, thus, asymptotically optimal. A
practical low-complexity version of our quantizer achieves performance quite
close to optimal. In addition, we derive rate-distortion function for matrix
multiplication of iid Gaussian matrices, which exhibits an interesting
phase-transition at $R\approx 0.906$ bit/entry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPCL-Diff: The Temporal <span class="highlight-title">Knowledge Graph</span> Reasoning Based on Graph Node
  Diffusion Model with Dual-Domain Periodic Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Cao, Lisheng Wang, Luobin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal knowledge graph (TKG) reasoning that infers future missing facts is
an essential and challenging task. Predicting future events typically relies on
closely related historical facts, yielding more accurate results for repetitive
or periodic events. However, for future events with sparse historical
interactions, the effectiveness of this method, which focuses on leveraging
high-frequency historical information, diminishes. Recently, the capabilities
of diffusion models in image generation have opened new opportunities for TKG
reasoning. Therefore, we propose a graph node diffusion model with dual-domain
periodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)
introduces noise into sparsely related events to simulate new events,
generating high-quality data that better conforms to the actual distribution.
This generative mechanism significantly enhances the model's ability to reason
about new events. Additionally, the dual-domain periodic contrastive learning
(DPCL) maps periodic and non-periodic event entities to Poincar\'e and
Euclidean spaces, leveraging their characteristics to distinguish similar
periodic events effectively. Experimental results on four public datasets
demonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG
models in event prediction, demonstrating our approach's effectiveness. This
study also investigates the combined effectiveness of GNDiff and DPCL in TKG
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking as a Reward Misspecification Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14393v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14393v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihui Xie, Jiahui Gao, Lei Li, Zhenguo Li, Qi Liu, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of large language models (LLMs) has raised concerns
about their safety and reliability, particularly regarding their vulnerability
to adversarial attacks. In this paper, we propose a novel perspective that
attributes this vulnerability to reward misspecification during the alignment
process. This misspecification occurs when the reward function fails to
accurately capture the intended behavior, leading to misaligned model outputs.
We introduce a metric ReGap to quantify the extent of reward misspecification
and demonstrate its effectiveness and robustness in detecting harmful backdoor
prompts. Building upon these insights, we present ReMiss, a system for
automated red teaming that generates adversarial prompts in a
reward-misspecified space. ReMiss achieves state-of-the-art attack success
rates on the AdvBench benchmark against various target aligned LLMs while
preserving the human readability of the generated prompts. Furthermore, these
attacks on open-source models demonstrate high transferability to closed-source
models like GPT-4o and out-of-distribution tasks from HarmBench. Detailed
analysis highlights the unique advantages of the proposed reward
misspecification objective compared to previous methods, offering new insights
for improving LLM safety and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bandit on the Hunt: Dynamic Crawling for Cyber Threat Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Kuehn, Dilara Nadermahmoodi, Markus Bayer, Christian Reuter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public information contains valuable Cyber Threat Intelligence (CTI) that is
used to prevent attacks in the future. Ideally, the learnings from previous
attacks help to mitigate all those that follow. While there are standards for
sharing this information, much of it is shared in non-standardized news
articles or blog posts. It is a time-consuming task to monitor online sources
for threats and even then, one can never be sure, to use the right sources.
Current research propose extractors of Indicators of Compromise from known
sources, while the identification of new sources is rarely considered. This
paper proposes a focused crawler focused on the CTI domain based on multi-armed
bandit ( MAB) and different crawling strategies. It uses SBERT to identify
relevant documents, while dynamically adapt its crawling path. We propose a
system called ThreatCrawl, which achieve a harvest rate of over 25% and is able
to expand its used seed by over 300%, while retaining focus on the topic at
hand. In addition, this crawler identified previously unknown but highly
relevant overview pages, datasets, and domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Redundant Is the <span class="highlight-title">Transformer</span> Stack in Speech Representation Models? <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teresa Dorszewski, Albert Kjøller Jacobsen, Lenka Tětková, Lars Kai Hansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech representation models, particularly those leveraging
transformer architectures, have demonstrated remarkable performance across
various tasks such as speech recognition, speaker identification, and emotion
detection. Recent studies on transformer models revealed a high redundancy
between layers and the potential for significant pruning, which we will
investigate here for transformer-based speech representation models. We perform
a detailed analysis of layer similarity in speech representation models using
three similarity metrics: cosine similarity, centered kernel alignment, and
mutual nearest-neighbor alignment. Our findings reveal a block-like structure
of high similarity, suggesting two main processing steps and significant
redundancy of layers. We demonstrate the effectiveness of pruning
transformer-based speech representation models without the need for
post-training, achieving up to 40% reduction in transformer layers while
maintaining over 95% of the model's predictive capacity. Furthermore, we employ
a knowledge distillation method to substitute the entire transformer stack with
mimicking layers, reducing the network size 95-98% and the inference time by up
to 94%. This substantial decrease in computational load occurs without
considerable performance loss, suggesting that the transformer stack is almost
completely redundant for downstream applications of speech representation
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICASSP 2025 (excluding appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Piece of Table: A Divide-and-Conquer Approach for Selecting Sub-Tables
  in Table Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjin Lee, Kyumin Kim, Sungjae Lee, Jihun Lee, Kwang In Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying language models (LMs) to tables is challenging due to the inherent
structural differences between two-dimensional tables and one-dimensional text
for which the LMs were originally designed. Furthermore, when applying
linearized tables to LMs, the maximum token lengths often imposed in
self-attention calculations make it difficult to comprehensively understand the
context spread across large tables. To address these challenges, we present
PieTa (Piece of Table), a new framework for sub-table-based question answering
(QA). PieTa operates through an iterative process of dividing tables into
smaller windows, using LMs to select relevant cells within each window, and
merging these cells into a sub-table. This multi-resolution approach captures
dependencies across multiple rows and columns while avoiding the limitations
caused by long context inputs. Instantiated as a simple iterative sub-table
union algorithm, PieTa demonstrates improved performance over previous
sub-table-based QA approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Low-Resource Language Retrieval: Establishing Baselines for
  Urdu MS MARCO <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umer Butt, Stalin Veranasi, Günter Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the Information Retrieval (IR) field increasingly recognizes the
importance of inclusivity, addressing the needs of low-resource languages
remains a significant challenge. This paper introduces the first large-scale
Urdu IR dataset, created by translating the MS MARCO dataset through machine
translation. We establish baseline results through zero-shot learning for IR in
Urdu and subsequently apply the mMARCO multilingual IR methodology to this
newly translated dataset. Our findings demonstrate that the fine-tuned model
(Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a
Recall@10 of 0.439, representing significant improvements over zero-shot
results and showing the potential for expanding IR access for Urdu speakers. By
bridging access gaps for speakers of low-resource languages, this work not only
advances multilingual IR research but also emphasizes the ethical and societal
importance of inclusive IR technologies. This work provides valuable insights
into the challenges and solutions for improving language representation and
lays the groundwork for future research, especially in South Asian languages,
which can benefit from the adaptable methods used in this study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, ECIR 2025, conference camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLSBench: Unveiling Visual Leakage in Multimodal Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhao Hu, Dongrui Liu, Hao Li, Xuanjing Huang, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety concerns of Multimodal large language models (MLLMs) have gradually
become an important problem in various applications. Surprisingly, previous
works indicate a counter-intuitive phenomenon that using textual unlearning to
align MLLMs achieves comparable safety performances with MLLMs trained with
image-text pairs. To explain such a counter-intuitive phenomenon, we discover a
visual safety information leakage (VSIL) problem in existing multimodal safety
benchmarks, i.e., the potentially risky and sensitive content in the image has
been revealed in the textual query. In this way, MLLMs can easily refuse these
sensitive text-image queries according to textual queries. However, image-text
pairs without VSIL are common in real-world scenarios and are overlooked by
existing multimodal safety benchmarks. To this end, we construct multimodal
visual leakless safety benchmark (VLSBench) preventing visual safety leakage
from image to textual query with 2.4k image-text pairs. Experimental results
indicate that VLSBench poses a significant challenge to both open-source and
close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.
This study demonstrates that textual alignment is enough for multimodal safety
scenarios with VSIL, while multimodal alignment is a more promising solution
for multimodal safety scenarios without VSIL. Please see our code and data at:
https://hxhcreate.github.io/vlsbench.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix
  Sharing and Throughput-oriented Token Batching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) increasingly play an important role in a wide
range of information processing and management tasks. Many of these tasks are
performed in large batches or even offline, and the performance indictor for
which is throughput. These tasks usually show the characteristic of prefix
sharing, where different prompt input can partially show the common prefix.
However, the existing LLM inference engines tend to optimize the streaming
requests and show limitations of supporting the large batched tasks with the
prefix sharing characteristic. The existing solutions use the LRU-based cache
to reuse the KV context of common prefix between requests. The KV context that
are about to be reused may prematurely evicted with the implicit cache
management. Besides, the streaming oriented systems do not leverage the
request-batch information and can not mix the decoding tokens with the prefill
chunks to the best for the batched scenarios, and thus fails to saturate the
GPU. We propose BatchLLM to address the above problems. BatchLLM explicitly
identifies the common prefixes globally. The requests sharing the same prefix
will be scheduled together to reuse the KV context the best. BatchLLM reorders
the requests and schedules the requests with larger ratio of decoding first to
better mix the decoding tokens with the latter prefill chunks, and applies
memory-centric token batching to enlarge the token-batch sizes, which helps to
increase the GPU utilization. Finally, BatchLLM optimizes the prefix-shared
Attention kernel with horizontal fusion to reduce tail effect and kernel launch
overhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang
by 1.3$\times$ to 10.8$\times$ on a set of microbenchmarks and a typical
industry workload under different hardware environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language
  Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaut Thonet, Jos Rozen, Laurent Besacier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on Large Language Models (LLMs) has recently witnessed an increasing
interest in extending the models' context size to better capture dependencies
within long documents. While benchmarks have been proposed to assess long-range
abilities, existing efforts primarily considered generic tasks that are not
necessarily aligned with real-world applications. In contrast, we propose a new
benchmark for long-context LLMs focused on a practical meeting assistant
scenario in which the long contexts consist of transcripts obtained by
automatic speech recognition, presenting unique challenges for LLMs due to the
inherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench,
augments the existing ELITR corpus by adding 271 manually crafted questions
with their ground-truth answers, as well as noisy versions of meeting
transcripts altered to target different Word Error Rate levels. Our experiments
with 12 long-context LLMs on ELITR-Bench confirm the progress made across
successive generations of both proprietary and open models, and point out their
discrepancies in terms of robustness to transcript noise. We also provide a
thorough analysis of our GPT-4-based evaluation, including insights from a
crowdsourcing study. Our findings indicate that while GPT-4's scores align with
human judges, its ability to distinguish beyond three score levels may be
limited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Packing in LLM Training Improves Long Context Utilization <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17296v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17296v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konrad Staniszewski, Szymon Tworkowski, Sebastian Jaszczur, Yu Zhao, Henryk Michalewski, Łukasz Kuciński, Piotr Miłoś
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in long-context large language models have attracted
significant attention, yet their practical applications often suffer from
suboptimal context utilization. This study investigates structuring training
data to enhance semantic interdependence, demonstrating that this approach
effectively improves context utilization. To this end, we introduce the
Structured Packing for Long Context (SPLiCe) method, which utilizes retrieval
to collate mutually relevant documents into long and coherent training
examples. We validate SPLiCe empirically across models of varying sizes -- 3B,
7B, and 13B -- achieving improved performance in long-context tasks, such as
Qasper and HotpotQA. Remarkably, even brief fine-tuning with SPLiCe is
sufficient to realize these benefits. Additionally, SPLiCe effectively
mitigates the lost-in-middle phenomenon often observed in large models. Our
comprehensive analysis of SPLiCe explores its design choices and reveals
intriguing transfer effects; for instance, training on programming code
enhances performance on natural language tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning from Human Feedback: Whose Culture, Whose Values,
  Whose Perspectives? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristian González Barman, Simon Lohse, Henk de Regt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We argue for the epistemic and ethical advantages of pluralism in
Reinforcement Learning from Human Feedback (RLHF) in the context of Large
Language Models (LLM). Drawing on social epistemology and pluralist philosophy
of science, we suggest ways in which RHLF can be made more responsive to human
needs and how we can address challenges along the way. The paper concludes with
an agenda for change, i.e. concrete, actionable steps to improve LLM
development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can linguists better understand DNA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07678v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07678v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual transfer ability, which reflects how well models fine-tuned on
one source language can be applied to other languages, has been well studied in
multilingual pre-trained models. However, the existence of such capability
transfer between natural language and gene sequences/languages remains under
explored.This study addresses this gap by drawing inspiration from the
sentence-pair classification task used for evaluating sentence similarity in
natural language. We constructed two analogous tasks: DNA-pair
classification(DNA sequence similarity) and DNA-protein-pair
classification(gene coding determination). These tasks were designed to
validate the transferability of capabilities from natural language to gene
sequences. Even a small-scale pre-trained model like GPT-2-small, which was
pre-trained on English, achieved an accuracy of 78% on the DNA-pair
classification task after being fine-tuned on English sentence-pair
classification data(XTREME PAWS-X). While training a BERT model on multilingual
text, the precision reached 89%. On the more complex DNA-protein-pair
classification task, however, the model's output was barely distinguishable
from random output.Experimental validation has confirmed that the transfer of
capabilities from natural language to biological language is unequivocally
present. Building on this foundation, we have also investigated the impact of
model parameter scale and pre-training on this capability transfer. We provide
recommendations for facilitating the transfer of capabilities from natural
language to genetic language,as well as new approaches for conducting
biological research based on this capability.This study offers an intriguing
new perspective on exploring the relationship between natural language and
genetic language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages,8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-guided Image Restoration and Semantic Enhancement for Text-to-Image
  Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09059v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09059v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific
person images according to the given textual descriptions. A primary challenge
in this task is bridging the substantial representational gap between visual
and textual modalities. The prevailing methods map texts and images into
unified embedding space for matching, while the intricate semantic
correspondences between texts and images are still not effectively constructed.
To address this issue, we propose a novel TIPR framework to build fine-grained
interactions and alignment between person images and the corresponding texts.
Specifically, via fine-tuning the Contrastive Language-Image Pre-training
(CLIP) model, a visual-textual dual encoder is firstly constructed, to
preliminarily align the image and text features. Secondly, a Text-guided Image
Restoration (TIR) auxiliary task is proposed to map abstract textual entities
to specific image regions, improving the alignment between local textual and
visual embeddings. Additionally, a cross-modal triplet loss is presented to
handle hard samples, and further enhance the model's discriminability for minor
differences. Moreover, a pruning-based text data augmentation approach is
proposed to enhance focus on essential elements in descriptions, thereby
avoiding excessive model attention to less significant information. The
experimental results show our proposed method outperforms state-of-the-art
methods on three popular benchmark datasets, and the code will be made publicly
available at https://github.com/Delong-liu-bupt/SEN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was withdrawn due to a dispute among the authors regarding
  the content of the article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce AceMath, a suite of frontier math models that
excel in solving complex math problems, along with highly effective reward
models capable of evaluating generated solutions and reliably identifying the
correct ones. To develop the instruction-tuned math models, we propose a
supervised fine-tuning (SFT) process that first achieves competitive
performance across general domains, followed by targeted fine-tuning for the
math domain using a carefully curated set of prompts and synthetically
generated responses. The resulting model, AceMath-72B-Instruct greatly
outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop
math-specialized reward model, we first construct AceMath-RewardBench, a
comprehensive and robust benchmark for evaluating math reward models across
diverse problems and difficulty levels. After that, we present a systematic
approach to build our math reward models. The resulting model, AceMath-72B-RM,
consistently outperforms state-of-the-art reward models. Furthermore, when
combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest
average rm@8 score across the math reasoning benchmarks. We release model
weights, training data, and evaluation benchmarks at:
https://research.nvidia.com/labs/adlr/acemath
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation
  for Design Space Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, Kevin Zhou, Jianliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GraphRAG integrates (knowledge) graphs with large language models (LLMs) to
improve reasoning accuracy and contextual relevance. Despite its promising
applications and strong relevance to multiple research communities, such as
databases and natural language processing, GraphRAG currently lacks modular
workflow analysis, systematic solution frameworks, and insightful empirical
studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework
that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)
systematic classification of existing techniques and implemented GraphRAG
instances, and 3) creation of new GraphRAG instances. Our framework facilitates
comprehensive empirical studies of GraphRAG on large-scale real-world graphs
and diverse query sets, revealing insights into balancing reasoning quality,
runtime efficiency, and token or GPU cost, that are essential for building
advanced GraphRAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering <span class="highlight-title">Large Language Model</span> for Continual Video Question Answering
  with Collaborative <span class="highlight-title">Prompt</span>ing <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Cai, Zheng Wang, Jianjun Gao, Wenyang Liu, Ye Lu, Runzhong Zhang, Kim-Hui Yap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the rapid increase in online video content has underscored
the limitations of static Video Question Answering (VideoQA) models trained on
fixed datasets, as they struggle to adapt to new questions or tasks posed by
newly available content. In this paper, we explore the novel challenge of
VideoQA within a continual learning framework, and empirically identify a
critical issue: fine-tuning a large language model (LLM) for a sequence of
tasks often results in catastrophic forgetting. To address this, we propose
Collaborative Prompting (ColPro), which integrates specific question constraint
prompting, knowledge acquisition prompting, and visual temporal awareness
prompting. These prompts aim to capture textual question context, visual
content, and video temporal dynamics in VideoQA, a perspective underexplored in
prior research. Experimental results on the NExT-QA and DramaQA datasets show
that ColPro achieves superior performance compared to existing approaches,
achieving 55.14\% accuracy on NExT-QA and 71.24\% accuracy on DramaQA,
highlighting its practical relevance and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by main EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Iterative Enhancement for Improving Learnersourced
  Multiple-Choice Question Explanations with <span class="highlight-title">Large Language Model</span>s <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10444v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10444v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Bao, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Gaël Gendron, Timothy Pistotti, Alice Huang, Paul Denny, Michael Witbrock, Jiamou Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models exhibit superior capabilities in processing and
understanding language, yet their applications in educational contexts remain
underexplored. Learnersourcing enhances learning by engaging students in
creating their own educational content. When learnersourcing multiple-choice
questions, creating explanations for the solution of a question is a crucial
step; it helps other students understand the solution and promotes a deeper
understanding of related concepts. However, it is often difficult for students
to craft effective solution explanations, due to limited subject understanding.
To help scaffold the task of automated explanation generation, we present and
evaluate a framework called "ILearner-LLM", that iteratively enhances the
generated explanations for the given questions with large language models.
Comprising an explanation generation model and an explanation evaluation model,
the framework generates high-quality student-aligned explanations by
iteratively feeding the quality rating score from the evaluation model back
into the instruction prompt of the explanation generation model. Experimental
results demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and
GPT-4 to generate higher quality explanations that are closer to those written
by students on five PeerWise datasets. Our findings represent a promising path
to enrich the learnersourcing experience for students and to enhance the
capabilities of large language models for educational applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The short version (v4) has been accepted as a non-archival workshop
  paper at AGI@ICLR 2024, and the full version has been accepted by the main
  track of AAAI/EAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing and Enhancing the Robustness of <span class="highlight-title">Large Language Model</span>s with
  Task Structure Variations for Logical Reasoning <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09430v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09430v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Bao, Gael Gendron, Alex Yuxuan Peng, Wanjun Zhong, Neset Tan, Yang Chen, Michael Witbrock, Jiamou Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and
GPT-4, have advanced the performance of AI systems on various natural language
processing tasks to human-like levels. However, their generalisation and
robustness when performing logical reasoning has not been sufficiently
assessed. To comprehensively evaluate this ability, we develop three new
logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and
"LogiQAv2-plus" that extend standard logical reasoning datasets to evaluate the
robustness of the LLM's reasoning. For each, we create three subsets: the first
with randomly shuffled options, the second with the correct choices replaced by
"none of the other options is correct", and the third with a combination of
shuffling and substitution. Experiments on these datasets show that these
simple augmentations greatly hinder the models' performance. Despite their high
performance on the original publicly available datasets, we find that all
models perform poorly on these newly constructed datasets. We also demonstrate
that introducing task variations into the training set can markedly improve the
model's performance on both the original and our developed datasets. Finally,
we show that applying logic-driven data augmentation for fine-tuning and
prompting can enhance generalisation in both discriminative and generative
models, offering a path to improving their robustness for tasks involving
logical reasoning. Source code and data are made publicly available at
https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The short version (v3) was accepted for oral presentation at the
  first LLM@IJCAI 2023 non-archival symposium, and the full version was
  accepted by ICONIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhisheng Tang, Mayank Kejriwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial reasoning, an important faculty of human cognition with many
practical applications, is one of the core commonsense skills that is not
purely language-based and, for satisfying (as opposed to optimal) solutions,
requires some minimum degree of planning. Existing benchmarks of Commonsense
Spatial Reasoning (CSR) tend to evaluate how Large Language Models (LLMs)
interpret text-based spatial $\textit{descriptions}$ rather than directly
evaluate a plan produced by the LLM in response to a $\textit{specific}$
spatial reasoning problem. In this paper, we construct a large-scale benchmark
called GRASP, which consists of 16,000 grid-based environments where the agent
is tasked with an energy collection problem. These environments include 100
grid instances instantiated using each of the 160 different grid settings,
involving five different energy distributions, two modes of agent starting
position, and two distinct obstacle configurations, as well as three kinds of
agent constraints. Using GRASP, we compare classic baseline approaches, such as
random walk and greedy search methods, with advanced LLMs like GPT-3.5-Turbo,
GPT-4o, and GPT-o1-mini. The experimental results indicate that even these
advanced LLMs struggle to consistently achieve satisfactory solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training
  on an Assistant Task for a Target Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xindi Tong, Yujin Zhu, Shijian Fan, Liang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long text summarization, gradually being essential for efficiently processing
large volumes of information, stays challenging for Large Language Models
(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced
training datasets and the high requirement of contextual details dealing. To
address the issue, we design a novel zero-shot transfer learning framework,
abbreviated as T3, to iteratively training a baseline LLM on an assistant task
for the target task, where the former should own richer data resources and
share structural or semantic similarity with the latter. In practice, T3 is
approached to deal with the long text summarization task by utilizing question
answering as the assistant task, and further validated its effectiveness on the
BBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%
improvement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore
compared to three baseline LLMs, demonstrating its potential for more
assistant-target task combinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can AI-Generated Text be Reliably Detected? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11156v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11156v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) perform impressively well in various
applications. However, the potential for misuse of these models in activities
such as plagiarism, generating fake news, and spamming has raised concern about
their responsible use. Consequently, the reliable detection of AI-generated
text has become a critical area of research. AI text detectors have shown to be
effective under their specific settings. In this paper, we stress-test the
robustness of these AI text detectors in the presence of an attacker. We
introduce recursive paraphrasing attack to stress test a wide range of
detection schemes, including the ones using the watermarking as well as neural
network-based detectors, zero shot classifiers, and retrieval-based detectors.
Our experiments conducted on passages, each approximately 300 tokens long,
reveal the varying sensitivities of these detectors to our attacks. Our
findings indicate that while our recursive paraphrasing method can
significantly reduce detection rates, it only slightly degrades text quality in
many cases, highlighting potential vulnerabilities in current detection systems
in the presence of an attacker. Additionally, we investigate the susceptibility
of watermarked LLMs to spoofing attacks aimed at misclassifying human-written
text as AI-generated. We demonstrate that an attacker can infer hidden AI text
signatures without white-box access to the detection method, potentially
leading to reputational risks for LLM developers. Finally, we provide a
theoretical framework connecting the AUROC of the best possible detector to the
Total Variation distance between human and AI text distributions. This analysis
offers insights into the fundamental challenges of reliable detection as
language models continue to advance. Our code is publicly available at
https://github.com/vinusankars/Reliability-of-AI-text-detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning with Human Judgement: The Role of Pairwise Preference in Large
  Language Model Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16950v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16950v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated promising capabilities as
automatic evaluators in assessing the quality of generated natural language.
However, LLMs still exhibit biases in evaluation and often struggle to generate
coherent evaluations that align with human assessments. In this work, we first
conduct a systematic study of the misalignment between LLM evaluators and human
evaluation, revealing that existing calibration methods aimed at mitigating
biases of LLMs are insufficient for effectively aligning LLM evaluators.
Inspired by the use of preference data in RLHF, we formulate the evaluation as
a ranking problem and introduce Pairwise-preference Search (PAIRS), an
uncertainty-guided search-based rank aggregation method that employs LLMs to
conduct pairwise comparisons locally and efficiently ranks candidate texts
globally. PAIRS achieves state-of-the-art performance on representative
evaluation tasks in long-form generations and demonstrates significant
improvements over direct scoring. Furthermore, we provide insights into the
role of pairwise preference in quantifying the transitivity of LLMs and
demonstrate how PAIRS benefits from calibration using debiased pairwise
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL2KQL: From Natural Language to Kusto Query 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02933v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02933v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinye Tang, Amir H. Abdi, Jeremias Eichelbaum, Mahan Das, Alex Klein, Nihal Irmak Pakis, William Blum, Daniel L Mace, Tanvi Raja, Namrata Padmanabhan, Ye Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is growing rapidly in volume and complexity. Proficiency in database
query languages is pivotal for crafting effective queries. As coding assistants
become more prevalent, there is significant opportunity to enhance database
query languages. The Kusto Query Language (KQL) is a widely used query language
for large semi-structured data such as logs, telemetries, and time-series for
big data analytics platforms. This paper introduces NL2KQL an innovative
framework that uses large language models (LLMs) to convert natural language
queries (NLQs) to KQL queries. The proposed NL2KQL framework includes several
key components: Schema Refiner which narrows down the schema to its most
pertinent elements; the Few-shot Selector which dynamically selects relevant
examples from a few-shot dataset; and the Query Refiner which repairs syntactic
and semantic errors in KQL queries. Additionally, this study outlines a method
for generating large datasets of synthetic NLQ-KQL pairs which are valid within
a specific database contexts. To validate NL2KQL's performance, we utilize an
array of online (based on query execution) and offline (based on query parsing)
metrics. Through ablation studies, the significance of each framework component
is examined, and the datasets used for benchmarking are made publicly
available. This work is the first of its kind and is compared with available
baselines to demonstrate its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Matrix Multiplications for Lookup Table-Quantized LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10960v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10960v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley, Eric P. Xing, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of large language models (LLMs) is often constrained by memory
bandwidth, where the primary bottleneck is the cost of transferring model
parameters from the GPU's global memory to its registers. When coupled with
custom kernels that fuse the dequantization and matmul operations, weight-only
quantization can thus enable faster inference by reducing the amount of memory
movement. However, developing high-performance kernels for weight-quantized
LLMs presents substantial challenges, especially when the weights are
compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,
lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup
table engine for LUT-quantized LLMs, which uses offline restructuring of the
quantized weight matrix to minimize bit manipulations associated with
unpacking, and vectorization and duplication of the lookup table to mitigate
shared memory bandwidth constraints. At batch sizes < 32 and quantization group
size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster
than existing GEMM kernels. As an application of FLUTE, we explore a simple
extension to lookup table-based NormalFloat quantization and apply it to
quantize LLaMA3 to various configurations, obtaining competitive quantization
performance against strong baselines while obtaining an end-to-end throughput
increase of 1.5 to 2 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Continuous Semantic Shifts with Diachronic Word Similarity
  Matrices <span class="chip">COLING2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hajime Kiyama, Taichi Aida, Mamoru Komachi, Toshinobu Ogiso, Hiroya Takamura, Daichi Mochihashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The meanings and relationships of words shift over time. This phenomenon is
referred to as semantic shift. Research focused on understanding how semantic
shifts occur over multiple time periods is essential for gaining a detailed
understanding of semantic shifts. However, detecting change points only between
adjacent time periods is insufficient for analyzing detailed semantic shifts,
and using BERT-based methods to examine word sense proportions incurs a high
computational cost. To address those issues, we propose a simple yet intuitive
framework for how semantic shifts occur over multiple time periods by
leveraging a similarity matrix between the embeddings of the same word through
time. We compute a diachronic word similarity matrix using fast and lightweight
word embeddings across arbitrary time periods, making it deeper to analyze
continuous semantic shifts. Additionally, by clustering the similarity matrices
for different words, we can categorize words that exhibit similar behavior of
semantic shift in an unsupervised manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Language Driven Slice Discovery and Error Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07832v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07832v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error slice discovery is crucial to diagnose and mitigate model errors.
Current clustering or discrete attribute-based slice discovery methods face key
limitations: 1) clustering results in incoherent slices, while assigning
discrete attributes to slices leads to incomplete coverage of error patterns
due to missing or insufficient attributes; 2) these methods lack complex
reasoning, preventing them from fully explaining model biases; 3) they fail to
integrate \textit{domain knowledge}, limiting their usage in specialized fields
\eg radiology. We propose\ladder (\underline{La}nguage-\underline{D}riven
\underline{D}iscovery and \underline{E}rror \underline{R}ectification), to
address the limitations by: (1) leveraging the flexibility of natural language
to address incompleteness, (2) employing LLM's latent \textit{domain knowledge}
and advanced reasoning to analyze sentences and derive testable hypotheses
directly, identifying biased attributes, and form coherent error slices without
clustering. Existing mitigation methods typically address only the
worst-performing group, often amplifying errors in other subgroups. In
contrast,\ladder generates pseudo attributes from the discovered hypotheses to
mitigate errors across all biases without explicit attribute annotations or
prior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural
and medical images -- comparing 200+ classifiers with diverse architectures,
pretraining strategies, and LLMs -- show that\ladder consistently outperforms
existing baselines in discovering and mitigating biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Hallucinations in Practical <span class="highlight-title">Code</span> Generation: Phenomena, Mechanism,
  and Mitigation <span class="chip">ISSTA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Zhang, Yanlin Wang, Chong Wang, Jiachi Chen, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code generation aims to automatically generate code from input requirements,
significantly enhancing development efficiency. Recent large language models
(LLMs) based approaches have shown promising results and revolutionized code
generation task. Despite the promising performance, LLMs often generate
contents with hallucinations, especially for the code generation scenario
requiring the handling of complex contextual dependencies in practical
development process. Although previous study has analyzed hallucinations in
LLM-powered code generation, the study is limited to standalone function
generation. In this paper, we conduct an empirical study to study the
phenomena, mechanism, and mitigation of LLM hallucinations within more
practical and complex development contexts in repository-level generation
scenario. First, we manually examine the code generation results from six
mainstream LLMs to establish a hallucination taxonomy of LLM-generated code.
Next, we elaborate on the phenomenon of hallucinations, analyze their
distribution across different models. We then analyze causes of hallucinations
and identify four potential factors contributing to hallucinations. Finally, we
propose an RAG-based mitigation method, which demonstrates consistent
effectiveness in all studied LLMs. The replication package including code,
data, and experimental results is available at
https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ISSTA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keeping LLMs Aligned After <span class="highlight-title">Fine-tuning</span>: The Crucial Role of <span class="highlight-title">Prompt</span>
  Templates <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public LLMs such as the Llama 2-Chat underwent alignment training and were
considered safe. Recently Qi et al. [2024] reported that even benign
fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the
models. The current paper is about methods and best practices to mitigate such
loss of alignment. We focus on the setting where a public model is fine-tuned
before serving users for specific usage, where the model should improve on the
downstream task while maintaining alignment. Through extensive experiments on
several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct
v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt
templates used during fine-tuning and inference play a crucial role in
preserving safety alignment, and proposes the ``Pure Tuning, Safe Testing''
(PTST) strategy -- fine-tune models without a safety prompt, but include it at
test time. This seemingly counterintuitive strategy incorporates an intended
distribution shift to encourage alignment preservation. Fine-tuning experiments
on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the
rise of unsafe behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLPF: Reinforcement Learning from Prediction Feedback for User
  Summarization with LLMs <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxing Wu, Lin Ning, Luyang Liu, Harrison Lee, Neo Wu, Chao Wang, Sushant Prakash, Shawn O'Banion, Bradley Green, Jun Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-powered personalization agent systems employ Large Language Models (LLMs)
to predict users' behavior from their past activities. However, their
effectiveness often hinges on the ability to effectively leverage extensive,
long user historical data due to its inherent noise and length of such data.
Existing pretrained LLMs may generate summaries that are concise but lack the
necessary context for downstream tasks, hindering their utility in
personalization systems. To address these challenges, we introduce
Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to
generate concise, human-readable user summaries that are optimized for
downstream task performance. By maximizing the usefulness of the generated
summaries, RLPF effectively distills extensive user history data while
preserving essential information for downstream tasks. Our empirical evaluation
demonstrates significant improvements in both extrinsic downstream task utility
and intrinsic summary quality, surpassing baseline methods by up to 22% on
downstream task performance and achieving an up to 84.59% win rate on
Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable
74% reduction in context length while improving performance on 16 out of 19
unseen tasks and/or datasets, showcasing its generalizability. This approach
offers a promising solution for enhancing LLM personalization by effectively
transforming long, noisy user histories into informative and human-readable
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">114</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3rd Workshop on Maritime Computer Vision (MaCVi) 2025: Challenge Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Kiefer, Lojze Žust, Jon Muhovič, Matej Kristan, Janez Perš, Matija Teršek, Uma Mudenagudi Chaitra Desai, Arnold Wiliem, Marten Kreis, Nikhil Akalwadi, Yitong Quan, Zhiqiang Zhong, Zhe Zhang, Sujie Liu, Xuran Chen, Yang Yang, Matej Fabijanić, Fausto Ferreira, Seongju Lee, Junseok Lee, Kyoobin Lee, Shanliang Yao, Runwei Guan, Xiaoyu Huang, Yi Ni, Himanshu Kumar, Yuan Feng, Yi-Ching Cheng, Tzu-Yu Lin, Chia-Ming Lee, Chih-Chung Hsu, Jannik Sheikh, Andreas Michel, Wolfgang Gross, Martin Weinmann, Josip Šarić, Yipeng Lin, Xiang Yang, Nan Jiang, Yutang Lu, Fei Feng, Ali Awad, Evan Lucas, Ashraf Saleem, Ching-Heng Cheng, Yu-Fan Lin, Tzu-Yu Lin, Chih-Chung Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 3rd Workshop on Maritime Computer Vision (MaCVi) 2025 addresses maritime
computer vision for Unmanned Surface Vehicles (USV) and underwater. This report
offers a comprehensive overview of the findings from the challenges. We provide
both statistical and qualitative analyses, evaluating trends from over 700
submissions. All datasets, evaluation code, and the leaderboard are available
to the public at https://macvi.org/workshop/macvi25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Part of the MaCVi 2025 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent4Edu: Generating Learner Response Data by Generative Agents for
  Intelligent Education Systems <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weibo Gao, Qi Liu, Linan Yue, Fangzhou Yao, Rui Lv, Zheng Zhang, Hao Wang, Zhenya Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized learning represents a promising educational strategy within
intelligent educational systems, aiming to enhance learners' practice
efficiency. However, the discrepancy between offline metrics and online
performance significantly impedes their progress. To address this challenge, we
introduce Agent4Edu, a novel personalized learning simulator leveraging recent
advancements in human intelligence through large language models (LLMs).
Agent4Edu features LLM-powered generative agents equipped with learner profile,
memory, and action modules tailored to personalized learning algorithms. The
learner profiles are initialized using real-world response data, capturing
practice styles and cognitive factors. Inspired by human psychology theory, the
memory module records practice facts and high-level summaries, integrating
reflection mechanisms. The action module supports various behaviors, including
exercise understanding, analysis, and response generation. Each agent can
interact with personalized learning algorithms, such as computerized adaptive
testing, enabling a multifaceted evaluation and enhancement of customized
services. Through a comprehensive assessment, we explore the strengths and
weaknesses of Agent4Edu, emphasizing the consistency and discrepancies in
responses between agents and human learners. The code, data, and appendix are
publicly available at https://github.com/bigdata-ustc/Agent4Edu.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Large language model</span>s for automated scholarly paper <span class="highlight-title">review</span>: A <span class="highlight-title">survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenzhen Zhuang, Jiandong Chen, Hongfeng Xu, Yuwen Jiang, Jialiang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly impacted human society,
influencing various domains. Among them, academia is not simply a domain
affected by LLMs, but it is also the pivotal force in the development of LLMs.
In academic publications, this phenomenon is represented during the
incorporation of LLMs into the peer review mechanism for reviewing manuscripts.
We proposed the concept of automated scholarly paper review (ASPR) in our
previous paper. As the incorporation grows, it now enters the coexistence phase
of ASPR and peer review, which is described in that paper. LLMs hold
transformative potential for the full-scale implementation of ASPR, but they
also pose new issues and challenges that need to be addressed. In this survey
paper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin
with a survey to find out which LLMs are used to conduct ASPR. Then, we review
what ASPR-related technological bottlenecks have been solved with the
incorporation of LLM technology. After that, we move on to explore new methods,
new datasets, new source code, and new online systems that come with LLMs for
ASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and
investigate the attitudes and reactions of publishers and academia to ASPR.
Lastly, we discuss the challenges associated with the development of LLMs for
ASPR. We hope this survey can serve as an inspirational reference for the
researchers and promote the progress of ASPR for its actual implementation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Autoregressive <span class="highlight-title">Transformer</span>s: Combining Byte-~and Word-Level
  Processing for Robust, Adaptable Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pit Neitemeier, Björn Deiseroth, Constantin Eichenberg, Lukas Balles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is a fundamental step in natural language processing, breaking
text into units that computational models can process. While learned subword
tokenizers have become the de-facto standard, they present challenges such as
large vocabularies, limited adaptability to new domains or languages, and
sensitivity to spelling errors and variations. To overcome these limitations,
we investigate a hierarchical architecture for autoregressive language
modelling that combines character-level and word-level processing. It employs a
lightweight character-level encoder to convert character sequences into word
embeddings, which are then processed by a word-level backbone model and decoded
back into characters via a compact character-level decoder. This method retains
the sequence compression benefits of word-level tokenization without relying on
a rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion
parameters, that hierarchical transformers match the downstream task
performance of subword-tokenizer-based models while exhibiting significantly
greater robustness to input perturbations. Additionally, during continued
pretraining on an out-of-domain language, our model trains almost twice as
fast, achieves superior performance on the target language, and retains more of
its previously learned knowledge. Hierarchical transformers pave the way for
NLP systems that are more robust, flexible, and generalizable across languages
and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Ontology for Social Determinants of Education (SDoEd) based on
  Human-AI Collaborative Approach <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navya Martin Kollapally, James Geller, Patricia Morreale, Daehan Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of computational ontologies is well-established in the field of
Medical Informatics. The topic of Social Determinants of Health (SDoH) has also
received extensive attention. Work at the intersection of ontologies and SDoH
has been published. However, a standardized framework for Social Determinants
of Education (SDoEd) is lacking. In this paper, we are closing the gap by
introducing an SDoEd ontology for creating a precise conceptualization of the
interplay between life circumstances of students and their possible educational
achievements. The ontology was developed utilizing suggestions from
ChatGPT-3.5-010422 and validated using peer-reviewed research articles. The
first version of developed ontology was evaluated by human experts in the field
of education and validated using standard ontology evaluation software. This
version of the SDoEd ontology contains 231 domain concepts, 10 object
properties, and 24 data properties
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CONSORTIUM FOR COMPUTING SCIENCES IN COLLEGES</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEANN: A Domain-Informed Neural Network for Epidemiological Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Baptiste Guimbaud, Marc Plantevit, Léa Maître, Rémy Cazabet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In epidemiology, traditional statistical methods such as logistic regression,
linear regression, and other parametric models are commonly employed to
investigate associations between predictors and health outcomes. However,
non-parametric machine learning techniques, such as deep neural networks
(DNNs), coupled with explainable AI (XAI) tools, offer new opportunities for
this task. Despite their potential, these methods face challenges due to the
limited availability of high-quality, high-quantity data in this field. To
address these challenges, we introduce SEANN, a novel approach for informed
DNNs that leverages a prevalent form of domain-specific knowledge: Pooled
Effect Sizes (PES). PESs are commonly found in published Meta-Analysis studies,
in different forms, and represent a quantitative form of a scientific
consensus. By direct integration within the learning procedure using a custom
loss, we experimentally demonstrate significant improvements in the
generalizability of predictive performances and the scientific plausibility of
extracted relationships compared to a domain-knowledge agnostic neural network
in a scarce and noisy data setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Rhythm and Voice Conversion of Dysarthric to Healthy Speech
  for ASR <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl El Hajal, Enno Hermann, Ajinkya Kulkarni, Mathew Magimai. -Doss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) systems are well known to perform poorly
on dysarthric speech. Previous works have addressed this by speaking rate
modification to reduce the mismatch with typical speech. Unfortunately, these
approaches rely on transcribed speech data to estimate speaking rates and
phoneme durations, which might not be available for unseen speakers. Therefore,
we combine unsupervised rhythm and voice conversion methods based on
self-supervised speech representations to map dysarthric to typical speech. We
evaluate the outputs with a large ASR model pre-trained on healthy speech
without further fine-tuning and find that the proposed rhythm conversion
especially improves performance for speakers of the Torgo corpus with more
severe cases of dysarthria. Code and audio samples are available at
https://idiap.github.io/RnV .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025 Satellite Workshop: Workshop on Speech
  Pathology Analysis and DEtection (SPADE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random-Key Algorithms for Optimizing Integrated Operating Room
  Scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Salezze Vieira, Eduardo Machado Silva, Antonio Augusto Chaves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient surgery room scheduling is essential for hospital efficiency,
patient satisfaction, and resource utilization. This study addresses this
challenge by introducing a novel concept of Random-Key Optimizer (RKO),
rigorously tested on literature and new, real-world inspired instances. Our
combinatorial optimization problem incorporates multi-room scheduling,
equipment scheduling, and complex availability constraints for rooms, patients,
and surgeons, facilitating rescheduling and enhancing operational flexibility.
The RKO approach represents solutions as points in a continuous space, which
are then mapped in the problem solution space via a deterministic function
known as a decoder. The core idea is to operate metaheuristics and heuristics
in the random-key space, unaware of the original solution space. We design the
Biased Random-Key Genetic Algorithm with $Q$-Learning, Simulated Annealing, and
Iterated Local Search for use within an RKO framework, employing a single
decoder function. The proposed metaheuristics are complemented by lower-bound
formulations, providing optimal gaps for evaluating the effectiveness of the
heuristic results. Our results demonstrate significant lower and upper bounds
improvements for the literature instances, notably proving one optimal result.
Furthermore, the best-proposed metaheuristic efficiently generates schedules
for the newly introduced instances, even in highly constrained scenarios. This
research offers valuable insights and practical solutions for improving surgery
scheduling processes, offering tangible benefits to hospitals by optimising
resource allocation, reducing patient wait times, and enhancing overall
operational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, Preprint submitted to Applied Soft Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Challenges and recommendations for Electronic Health Records data
  extraction and preparation for dynamic prediction modelling in hospitalized
  patients -- a practical guide 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Albu, Shan Gao, Pieter Stijnen, Frank E. Rademakers, Bas C T van Bussel, Taya Collyer, Tina Hernandez-Boussard, Laure Wynants, Ben Van Calster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic predictive modeling using electronic health record (EHR) data has
gained significant attention in recent years. The reliability and
trustworthiness of such models depend heavily on the quality of the underlying
data, which is largely determined by the stages preceding the model
development: data extraction from EHR systems and data preparation. We list
over forty challenges encountered during these stages and provide actionable
recommendations for addressing them. These challenges are organized into four
categories: cohort definition, outcome definition, feature engineering, and
data cleaning. This list is designed to serve as a practical guide for data
extraction engineers and researchers, supporting better practices and improving
the quality and real-world applicability of dynamic prediction models in
clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Causal Reasoning with (Non-Recursive) Structural Equation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Gladyshev, Natasha Alechina, Mehdi Dastani, Dragan Doder, Brian Logan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural Equation Models (SEM) are the standard approach to representing
causal dependencies between variables in causal models. In this paper we
propose a new interpretation of SEMs when reasoning about Actual Causality, in
which SEMs are viewed as mechanisms transforming the dynamics of exogenous
variables into the dynamics of endogenous variables. This allows us to combine
counterfactual causal reasoning with existing temporal logic formalisms, and to
introduce a temporal logic, CPLTL, for causal reasoning about such structures.
We show that the standard restriction to so-called \textit{recursive} models
(with no cycles in the dependency graph) is not necessary in our approach,
allowing us to reason about mutually dependent processes and feedback loops.
Finally, we introduce new notions of model equivalence for temporal causal
models, and show that CPLTL has an efficient model-checking procedure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Good things come in small packages: Should we adopt Lite-GPUs in AI
  infrastructure? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burcu Canakci, Junyi Liu, Xingbo Wu, Nathanaël Cheriere, Paolo Costa, Sergey Legtchenko, Dushyanth Narayanan, Ant Rowstron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To match the blooming demand of generative AI workloads, GPU designers have
so far been trying to pack more and more compute and memory into single complex
and expensive packages. However, there is growing uncertainty about the
scalability of individual GPUs and thus AI clusters, as state-of-the-art GPUs
are already displaying packaging, yield, and cooling limitations. We propose to
rethink the design and scaling of AI clusters through efficiently-connected
large clusters of Lite-GPUs, GPUs with single, small dies and a fraction of the
capabilities of larger GPUs. We think recent advances in co-packaged optics can
be key in overcoming the communication challenges of distributing AI workloads
onto more Lite-GPUs. In this paper, we present the key benefits of Lite-GPUs on
manufacturing cost, blast radius, yield, and power efficiency; and discuss
systems opportunities and challenges around resource, workload, memory, and
network management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5+ pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Artificial Intelligence: Implications for Biomedical and
  Health Professions Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Hersh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has had a profound impact on biomedicine and health, both in
professional work and in education. Based on large language models (LLMs),
generative AI has been found to perform as well as humans in simulated
situations taking medical board exams, answering clinical questions, solving
clinical cases, applying clinical reasoning, and summarizing information.
Generative AI is also being used widely in education, performing well in
academic courses and their assessments. This review summarizes the successes of
LLMs and highlights some of their challenges in the context of education, most
notably aspects that may undermines the acquisition of knowledge and skills for
professional work. It then provides recommendations for best practices
overcoming shortcomings for LLM use in education. Although there are challenges
for use of generative AI in education, all students and faculty, in biomedicine
and health and beyond, must have understanding and be competent in its use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple but Effective Closed-form Solution for Extreme Multi-label
  Learning <span class="chip">ECIR25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuma Onishi, Katsuhiko Hayashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extreme multi-label learning (XML) is a task of assigning multiple labels
from an extremely large set of labels to each data instance. Many current
high-performance XML models are composed of a lot of hyperparameters, which
complicates the tuning process. Additionally, the models themselves are adapted
specifically to XML, which complicates their reimplementation. To remedy this
problem, we propose a simple method based on ridge regression for XML. The
proposed method not only has a closed-form solution but also is composed of a
single hyperparameter. Since there are no precedents on applying ridge
regression to XML, this paper verified the performance of the method by using
various XML benchmark datasets. Furthermore, we enhanced the prediction of
low-frequency labels in XML, which hold informative content. This prediction is
essential yet challenging because of the limited amount of data. Here, we
employed a simple frequency-based weighting. This approach greatly simplifies
the process compared with existing techniques. Experimental results revealed
that it can achieve levels of performance comparable to, or even exceeding,
those of models with numerous hyperparameters. Additionally, we found that the
frequency-based weighting significantly improved the predictive performance for
low-frequency labels, while requiring almost no changes in implementation. The
source code for the proposed method is available on github at
https://github.com/cars1015/XML-ridge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages, Accepted at ECIR25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSSDM Ontology to Enable Continuity of Care Data Interoperability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhashis Das, Debashis Naskar, Sara Rodriguez Gonzalez, Pamela Hussey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of digital technologies and recent global pandemic
scenarios have led to a growing focus on how these technologies can enhance
healthcare service delivery and workflow to address crises. Action plans that
consolidate existing digital transformation programs are being reviewed to
establish core infrastructure and foundations for sustainable healthcare
solutions. Reforming health and social care to personalize home care, for
example, can help avoid treatment in overcrowded acute hospital settings and
improve the experiences and outcomes for both healthcare professionals and
service users. In this information-intensive domain, addressing the
interoperability challenge through standards-based roadmaps is crucial for
enabling effective connections between health and social care services. This
approach facilitates safe and trustworthy data workflows between different
healthcare system providers. In this paper, we present a methodology for
extracting, transforming, and loading data through a semi-automated process
using a Common Semantic Standardized Data Model (CSSDM) to create personalized
healthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology
of ISO 13940 ContSys and incorporates FHIR-based specifications to support
structural attributes for generating KGs. We propose that the CSSDM facilitates
data harmonization and linking, offering an alternative approach to
interoperability. This approach promotes a novel form of collaboration between
companies developing health information systems and cloud-enabled health
services. Consequently, it provides multiple stakeholders with access to
high-quality data and information sharing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, Published in: 2024 IEEE International Conference
  on Bioinformatics and Biomedicine (BIBM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Region-wise stacking ensembles for estimating brain-age using MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Antonopoulos, Shammi More, Simon B. Eickhoff, Federico Raimondo, Kaustubh R. Patil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive modeling using structural magnetic resonance imaging (MRI) data is
a prominent approach to study brain-aging. Machine learning algorithms and
feature extraction methods have been employed to improve predictions and
explore healthy and accelerated aging e.g. neurodegenerative and psychiatric
disorders. The high-dimensional MRI data pose challenges to building
generalizable and interpretable models as well as for data privacy. Common
practices are resampling or averaging voxels within predefined parcels, which
reduces anatomical specificity and biological interpretability as voxels within
a region may differently relate to aging. Effectively, naive fusion by
averaging can result in information loss and reduced accuracy. We present a
conceptually novel two-level stacking ensemble (SE) approach. The first level
comprises regional models for predicting individuals' age based on voxel-wise
information, fused by a second-level model yielding final predictions. Eight
data fusion scenarios were explored using as input Gray matter volume (GMV)
estimates from four datasets covering the adult lifespan. Performance, measured
using mean absolute error (MAE), R2, correlation and prediction bias, showed
that SE outperformed the region-wise averages. The best performance was
obtained when first-level regional predictions were obtained as out-of-sample
predictions on the application site with second-level models trained on
independent and site-specific data (MAE=4.75 vs baseline regional mean GMV
MAE=5.68). Performance improved as more datasets were used for training.
First-level predictions showed improved and more robust aging signal providing
new biological insights and enhanced data privacy. Overall, the SE improves
accuracy compared to the baseline while preserving or enhancing data privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>version1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Topology-Driven Attribute Recovery for Attribute Missing Graph Learning
  in Social Internet of Things 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengran Li, Junzhou Chen, Chenyun Yu, Guanying Jiang, Ronghui Zhang, Yanming Shen, Houbing Herbert Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of information technology, the Social Internet of Things
(SIoT) has fostered the integration of physical devices and social networks,
deepening the study of complex interaction patterns. Text Attribute Graphs
(TAGs) capture both topological structures and semantic attributes, enhancing
the analysis of complex interactions within the SIoT. However, existing graph
learning methods are typically designed for complete attributed graphs, and the
common issue of missing attributes in Attribute Missing Graphs (AMGs) increases
the difficulty of analysis tasks. To address this, we propose the
Topology-Driven Attribute Recovery (TDAR) framework, which leverages
topological data for AMG learning. TDAR introduces an improved pre-filling
method for initial attribute recovery using native graph topology.
Additionally, it dynamically adjusts propagation weights and incorporates
homogeneity strategies within the embedding space to suit AMGs' unique
topological structures, effectively reducing noise during information
propagation. Extensive experiments on public datasets demonstrate that TDAR
significantly outperforms state-of-the-art methods in attribute reconstruction
and downstream tasks, offering a robust solution to the challenges posed by
AMGs. The code is available at https://github.com/limengran98/TDAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Internet of Things Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Debiasing: Remove Stereotypes and Keep Factual Gender for Fair
  Language Modeling and Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomasz Limisiewicz, David Mareček, Tomáš Musil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigation of biases, such as language models' reliance on gender
stereotypes, is a crucial endeavor required for the creation of reliable and
useful language technology. The crucial aspect of debiasing is to ensure that
the models preserve their versatile capabilities, including their ability to
solve language tasks and equitably represent various genders. To address this
issue, we introduce a streamlined Dual Dabiasing Algorithm through Model
Adaptation (2DAMA). Novel Dual Debiasing enables robust reduction of
stereotypical bias while preserving desired factual gender information encoded
by language models. We show that 2DAMA effectively reduces gender bias in
English and is one of the first approaches facilitating the mitigation of
stereotypical tendencies in translation. The proposed method's key advantage is
the preservation of factual gender cues, which are useful in a wide range of
natural language processing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing UAV Path Planning Efficiency Through Accelerated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseanne Viana, Boris Galkin, Lester Ho, Holger Claussen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs) are increasingly essential in various fields
such as surveillance, reconnaissance, and telecommunications. This study aims
to develop a learning algorithm for the path planning of UAV wireless
communication relays, which can reduce storage requirements and accelerate Deep
Reinforcement Learning (DRL) convergence. Assuming the system possesses terrain
maps of the area and can estimate user locations using localization algorithms
or direct GPS reporting, it can input these parameters into the learning
algorithms to achieve optimized path planning performance. However, higher
resolution terrain maps are necessary to extract topological information such
as terrain height, object distances, and signal blockages. This requirement
increases memory and storage demands on UAVs while also lengthening convergence
times in DRL algorithms. Similarly, defining the telecommunication coverage map
in UAV wireless communication relays using these terrain maps and user position
estimations demands higher memory and storage utilization for the learning path
planning algorithms. Our approach reduces path planning training time by
applying a dimensionality reduction technique based on Principal Component
Analysis (PCA), sample combination, Prioritized Experience Replay (PER), and
the combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE) loss
calculations in the coverage map estimates, thereby enhancing a Twin Delayed
Deep Deterministic Policy Gradient (TD3) algorithm. The proposed solution
reduces the convergence episodes needed for basic training by approximately
four times compared to the traditional TD3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted in https://camad2024.ieee-camad.org/
  conference but it is not available from the conference yet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Prediction Sets with Improved Conditional Coverage using Trust
  Scores 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jivat Neet Kaur, Michael I. Jordan, Ahmed Alaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard conformal prediction offers a marginal guarantee on coverage, but
for prediction sets to be truly useful, they should ideally ensure coverage
conditional on each test point. Unfortunately, it is impossible to achieve
exact, distribution-free conditional coverage in finite samples. In this work,
we propose an alternative conformal prediction algorithm that targets coverage
where it matters most--in instances where a classifier is overconfident in its
incorrect predictions. We start by dissecting miscoverage events in
marginally-valid conformal prediction, and show that miscoverage rates vary
based on the classifier's confidence and its deviation from the Bayes optimal
classifier. Motivated by this insight, we develop a variant of conformal
prediction that targets coverage conditional on a reduced set of two variables:
the classifier's confidence in a prediction and a nonparametric trust score
that measures its deviation from the Bayes classifier. Empirical evaluation on
multiple image datasets shows that our method generally improves conditional
coverage properties compared to standard conformal prediction, including
class-conditional coverage, coverage over arbitrary subgroups, and coverage
over demographic groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Impact of Generative Artificial Intelligence in Education:
  A Thematic Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Kaushik, Sargam Yadav, Andrew Browne, David Lillis, David Williams, Jack Mc Donnell, Peadar Grant, Siobhan Connolly Kernan, Shubham Sharma, Mansi Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in Generative Artificial intelligence (GenAI)
technology have been transformative for the field of education. Large Language
Models (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate
tasks, create content for personalised teaching, and handle repetitive tasks to
allow more time for creative thinking. However, it is important to develop
guidelines, policies, and assessment methods in the education sector to ensure
the responsible integration of these tools. In this article, thematic analysis
has been performed on seven essays obtained from professionals in the education
sector to understand the advantages and pitfalls of using GenAI models such as
ChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been
performed on the essays to extract further insights from the text. The study
found several themes which highlight benefits and drawbacks of GenAI tools, as
well as suggestions to overcome these limitations and ensure that students are
using these tools in a responsible and ethical manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatio-temporal Graph Learning on Adaptive Mined Key Frames for
  High-performance Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Futian Wang, Fengxiang Liu, Xiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of multi-object tracking, the challenge of accurately capturing
the spatial and temporal relationships between objects in video sequences
remains a significant hurdle. This is further complicated by frequent
occurrences of mutual occlusions among objects, which can lead to tracking
errors and reduced performance in existing methods. Motivated by these
challenges, we propose a novel adaptive key frame mining strategy that
addresses the limitations of current tracking approaches. Specifically, we
introduce a Key Frame Extraction (KFE) module that leverages reinforcement
learning to adaptively segment videos, thereby guiding the tracker to exploit
the intrinsic logic of the video content. This approach allows us to capture
structured spatial relationships between different objects as well as the
temporal relationships of objects across frames. To tackle the issue of object
occlusions, we have developed an Intra-Frame Feature Fusion (IFF) module.
Unlike traditional graph-based methods that primarily focus on inter-frame
feature fusion, our IFF module uses a Graph Convolutional Network (GCN) to
facilitate information exchange between the target and surrounding objects
within a frame. This innovation significantly enhances target
distinguishability and mitigates tracking loss and appearance similarity due to
occlusions. By combining the strengths of both long and short trajectories and
considering the spatial relationships between objects, our proposed tracker
achieves impressive results on the MOT17 dataset, i.e., 68.6 HOTA, 81.0 IDF1,
66.6 AssA, and 893 IDS, proving its effectiveness and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Infrastructure for AI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Chan, Kevin Wei, Sihao Huang, Nitarshan Rajkumar, Elija Perrier, Seth Lazar, Gillian K. Hadfield, Markus Anderljung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Increasingly many AI systems can plan and execute interactions in open-ended
environments, such as making phone calls or buying online goods. As developers
grow the space of tasks that such AI agents can accomplish, we will need tools
both to unlock their benefits and manage their risks. Current tools are largely
insufficient because they are not designed to shape how agents interact with
existing institutions (e.g., legal and economic systems) or actors (e.g.,
digital service providers, humans, other AI agents). For example, alignment
techniques by nature do not assure counterparties that some human will be held
accountable when a user instructs an agent to perform an illegal action. To
fill this gap, we propose the concept of agent infrastructure: technical
systems and shared protocols external to agents that are designed to mediate
and influence their interactions with and impacts on their environments. Agent
infrastructure comprises both new tools and reconfigurations or extensions of
existing tools. For example, to facilitate accountability, protocols that tie
users to agents could build upon existing systems for user authentication, such
as OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue
that agent infrastructure will be similarly indispensable to ecosystems of
agents. We identify three functions for agent infrastructure: 1) attributing
actions, properties, and other information to specific agents, their users, or
other actors; 2) shaping agents' interactions; and 3) detecting and remedying
harmful actions from agents. We propose infrastructure that could help achieve
each function, explaining use cases, adoption, limitations, and open questions.
Making progress on agent infrastructure can prepare society for the adoption of
more advanced agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BBPOS: <span class="highlight-title">BERT</span>-based Part-of-Speech Tagging for Uzbek 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Latofat Bobojonova, Arofat Akhundjanova, Phil Ostheimer, Sophie Fellenz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper advances NLP research for the low-resource Uzbek language by
evaluating two previously untested monolingual Uzbek BERT models on the
part-of-speech (POS) tagging task and introducing the first publicly available
UPOS-tagged benchmark dataset for Uzbek. Our fine-tuned models achieve 91%
average accuracy, outperforming the baseline multi-lingual BERT as well as the
rule-based tagger. Notably, these models capture intermediate POS changes
through affixes and demonstrate context sensitivity, unlike existing rule-based
taggers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Reasoner and Automated Planner: A new NPC approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Israel Puerta-Merino, Jordi Sabater-Mir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In domains requiring intelligent agents to emulate plausible human-like
behaviour, such as formative simulations, traditional techniques like behaviour
trees encounter significant challenges. Large Language Models (LLMs), despite
not always yielding optimal solutions, usually offer plausible and human-like
responses to a given problem. In this paper, we exploit this capability and
propose a novel architecture that integrates an LLM for decision-making with a
classical automated planner that can generate sound plans for that decision.
The combination aims to equip an agent with the ability to make decisions in
various situations, even if they were not anticipated during the design phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, extended version of the homonymous paper
  submitted to the Catalan Conference on Artificial Intelligent (CCIA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Actions for Enhanced Embodied Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training on diverse, internet-scale data is a key factor in the success of
recent large foundation models. Yet, using the same recipe for building
embodied agents has faced noticeable difficulties. Despite the availability of
many crowd-sourced embodied datasets, their action spaces often exhibit
significant heterogeneity due to distinct physical embodiment and control
interfaces for different robots, causing substantial challenges in developing
embodied foundation models using cross-domain data. In this paper, we introduce
UniAct, a new embodied foundation modeling framework operating in a tokenized
Universal Action Space. Our learned universal actions capture the generic
atomic behaviors across diverse robots by exploiting their shared structural
features, and enable enhanced cross-domain data utilization and
cross-embodiment generalizations by eliminating the notorious heterogeneity.
The universal actions can be efficiently translated back to heterogeneous
actionable commands by simply adding embodiment-specific details, from which
fast adaptation to new robots becomes simple and straightforward. Our 0.5B
instantiation of UniAct outperforms 14X larger SOTA embodied foundation models
in extensive evaluations on various real-world and simulation robots,
showcasing exceptional cross-embodiment control and adaptation capability,
highlighting the crucial benefit of adopting universal actions. Project page:
https://github.com/2toinf/UniAct
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotic World Model: A Neural Network Simulator for Robust Policy
  Optimization in Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Li, Andreas Krause, Marco Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning robust and generalizable world models is crucial for enabling
efficient and scalable robotic control in real-world environments. In this
work, we introduce a novel framework for learning world models that accurately
capture complex, partially observable, and stochastic dynamics. The proposed
method employs a dual-autoregressive mechanism and self-supervised training to
achieve reliable long-horizon predictions without relying on domain-specific
inductive biases, ensuring adaptability across diverse robotic tasks. We
further propose a policy optimization framework that leverages world models for
efficient training in imagined environments and seamless deployment in
real-world systems. Through extensive experiments, our approach consistently
outperforms state-of-the-art methods, demonstrating superior autoregressive
prediction accuracy, robustness to noise, and generalization across
manipulation and locomotion tasks. Notably, policies trained with our method
are successfully deployed on ANYmal D hardware in a zero-shot transfer,
achieving robust performance with minimal sim-to-real performance loss. This
work advances model-based reinforcement learning by addressing the challenges
of long-horizon prediction, error accumulation, and sim-to-real transfer. By
providing a scalable and robust framework, the introduced methods pave the way
for adaptive and efficient robotic systems in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jef Jonkers, Luc Duchateau, Glenn Van Wallendael, Sofie Van Hoecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical landmark localization in 2D/3D images is a critical task in
medical imaging. Although many general-purpose tools exist for landmark
localization in classical computer vision tasks, such as pose estimation, they
lack the specialized features and modularity necessary for anatomical landmark
localization applications in the medical domain. Therefore, we introduce
landmarker, a Python package built on PyTorch. The package provides a
comprehensive, flexible toolkit for developing and evaluating landmark
localization algorithms, supporting a range of methodologies, including static
and adaptive heatmap regression. landmarker enhances the accuracy of landmark
identification, streamlines research and development processes, and supports
various image formats and preprocessing pipelines. Its modular design allows
users to customize and extend the toolkit for specific datasets and
applications, accelerating innovation in medical imaging. landmarker addresses
a critical need for precision and customization in landmark localization tasks
not adequately met by existing general-purpose pose estimation tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do Programming Students Use Generative AI? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Rahe, Walid Maalej
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programming students have a widespread access to powerful Generative AI tools
like ChatGPT. While this can help understand the learning material and assist
with exercises, educators are voicing more and more concerns about an
over-reliance on generated outputs and lack of critical thinking skills. It is
thus important to understand how students actually use generative AI and what
impact this could have on their learning behavior. To this end, we conducted a
study including an exploratory experiment with 37 programming students, giving
them monitored access to ChatGPT while solving a code understanding and
improving exercise. While only 23 of the students actually opted to use the
chatbot, the majority of those eventually prompted it to simply generate a full
solution. We observed two prevalent usage strategies: to seek knowledge about
general concepts and to directly generate solutions. Instead of using the bot
to comprehend the code and their own mistakes, students often got trapped in a
vicious cycle of submitting wrong generated code and then asking the bot for a
fix. Those who self-reported using generative AI regularly were more likely to
prompt the bot to generate a solution. Our findings indicate that concerns
about potential decrease in programmers' agency and productivity with
Generative AI are justified. We discuss how researchers and educators can
respond to the potential risk of students uncritically over-relying on
generative AI. We also discuss potential modifications to our study design for
large-scale replications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint; accepted to ACM International Conference on the Foundations
  of Software Engineering (FSE) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Change Captioning in Remote Sensing: SECOND-CC Dataset and
  MModalCC Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Can Karaca, M. Enes Ozelbas, Saadettin Berber, Orkhan Karimli, Turabi Yildirim, M. Fatih Amasyali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing change captioning (RSICC) aims to describe changes between
bitemporal images in natural language. Existing methods often fail under
challenges like illumination differences, viewpoint changes, blur effects,
leading to inaccuracies, especially in no-change regions. Moreover, the images
acquired at different spatial resolutions and have registration errors tend to
affect the captions. To address these issues, we introduce SECOND-CC, a novel
RSICC dataset featuring high-resolution RGB image pairs, semantic segmentation
maps, and diverse real-world scenarios. SECOND-CC which contains 6,041 pairs of
bitemporal RS images and 30,205 sentences describing the differences between
images. Additionally, we propose MModalCC, a multimodal framework that
integrates semantic and visual data using advanced attention mechanisms,
including Cross-Modal Cross Attention (CMCA) and Multimodal Gated Cross
Attention (MGCA). Detailed ablation studies and attention visualizations
further demonstrate its effectiveness and ability to address RSICC challenges.
Comprehensive experiments show that MModalCC outperforms state-of-the-art RSICC
methods, including RSICCformer, Chg2Cap, and PSNet with +4.6% improvement on
BLEU4 score and +9.6% improvement on CIDEr score. We will make our dataset and
codebase publicly available to facilitate future research at
https://github.com/ChangeCapsInRS/SecondCC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE Transactions on Geoscience
  and Remote Sensing journal for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and
  Chain-of-Thought for Embodied Task Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, Guangjian Tian, Xingyue Quan, Jianye Hao, Yuzheng Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial reasoning is an essential problem in embodied AI research. Efforts to
enhance spatial reasoning abilities through supplementary spatial data and
fine-tuning have proven limited and ineffective when addressing complex
embodied tasks, largely due to their dependence on language-based outputs.
While some approaches have introduced a point-based action space to mitigate
this issue, they fall short in managing more intricate tasks within complex
environments. This deficiency arises from their failure to fully exploit the
inherent thinking and reasoning capabilities that are fundamental strengths of
Vision-Language Models (VLMs). To address these limitations, we propose a novel
approach named SpatialCoT, specifically designed to bolster the spatial
reasoning capabilities of VLMs. Our approach comprises two stages: spatial
coordinate bi-directional alignment, which aligns vision-language inputs with
spatial coordinates, and chain-of-thought spatial grounding, which harnesses
the reasoning capabilities of language models for advanced spatial reasoning.
We evaluate SpatialCoT on challenging navigation and manipulation tasks, both
in simulation and real-world settings. Experimental results demonstrate that
our method significantly outperforms previous state-of-the-art approaches in
both tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on LLM Test-Time Compute via Search: Tasks, LLM Profiling,
  Search Algorithms, and Relevant Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhe Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM test-time compute (or LLM inference) via search has emerged as a
promising research area with rapid developments. However, current frameworks
often adopt distinct perspectives on three key aspects (task definition, LLM
profiling, and search procedures), making direct comparisons challenging.
Moreover, the search algorithms employed often diverge from standard
implementations, and their specific characteristics are not thoroughly
specified. In this survey, we provide a comprehensive technical review that
unifies task definitions and provides modular definitions of LLM profiling and
search procedures. The definitions enable precise comparisons of various LLM
inference frameworks while highlighting their departures from conventional
search algorithms. We also discuss the applicability, performance, and
efficiency of these methods. For further details and ongoing updates, please
refer to our GitHub repository:
https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating <span class="highlight-title">Large Language Model</span>s through Partially Linear Feed-Forward
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gansen Hu, Zhaoguo Wang, Jinglin Wei, Wei Huang, Haibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate remarkable capabilities but face
deployment challenges due to their massive parameter counts. While existing
compression techniques like pruning can reduce model size, it leads to
significant accuracy degradation under high compression ratios. We present a
novel perspective inspired by constant folding in compiler optimization. Our
approach enables parameter reduction by treating activation functions in LLMs
as linear functions.
  However, recent LLMs use complex non-linear activations like GELU that
prevent direct application of this technique. We propose TARDIS, which enables
optimization of LLMs with non-linear activations by partially approximating
them with linear functions in frequently occurring input ranges. For outlier
inputs, TARDIS employs an online predictor to dynamically fall back to original
computations.
  Our experiments demonstrate that TARDIS achieves 80% parameter reduction in
feed-forward networks, while significantly outperforming state-of-the-art
pruning methods Wanda and RIA with up to 65% higher accuracy. In practical
deployments for a 7B model, TARDIS achieves 1.6x end-to-end inference speedup
when integrated with the vLLM serving system, and 1.4x speedup with the widely
adopted HuggingFace implementation, while incurring only a 10.9% accuracy
trade-off.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented
  Generation via Tree-based Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenfeng Feng, Chuzhan Hao, Yuewei Zhang, Jingyi Song, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging the autonomous decision-making capabilities of large language
models (LLMs) demonstrates superior performance in reasoning tasks. Despite the
successes of iterative or recursive retrieval-augmented generation (RAG), they
often are trapped in a single solution space when confronted with complex
tasks. In this paper, we propose a novel thinking pattern in RAG which
integrates system analysis with efficient reasoning actions, significantly
activating intrinsic reasoning capabilities and expanding the solution space of
specific tasks via Monte Carlo Tree Search (MCTS), dubbed AirRAG. Specifically,
our approach designs five fundamental reasoning actions that are expanded to a
wide tree-based reasoning spaces using MCTS. The extension also uses
self-consistency verification to explore potential reasoning paths and
implement inference scaling. In addition, computationally optimal strategies
are used to apply more inference computation to key actions to achieve further
performance improvements. Experimental results demonstrate the effectiveness of
AirRAG through considerable performance gains over complex QA datasets.
Furthermore, AirRAG is flexible and lightweight, making it easy to integrate
with other advanced technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Virtual Nodes Improve Long-term Traffic Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Cao, Dingyi Zhuang, Jinhua Zhao, Shenhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective traffic prediction is a cornerstone of intelligent transportation
systems, enabling precise forecasts of traffic flow, speed, and congestion.
While traditional spatio-temporal graph neural networks (ST-GNNs) have achieved
notable success in short-term traffic forecasting, their performance in
long-term predictions remains limited. This challenge arises from
over-squashing problem, where bottlenecks and limited receptive fields restrict
information flow and hinder the modeling of global dependencies. To address
these challenges, this study introduces a novel framework that incorporates
virtual nodes, which are additional nodes added to the graph and connected to
existing nodes, in order to aggregate information across the entire graph
within a single GNN layer. Our proposed model incorporates virtual nodes by
constructing a semi-adaptive adjacency matrix. This matrix integrates
distance-based and adaptive adjacency matrices, allowing the model to leverage
geographical information while also learning task-specific features from data.
Experimental results demonstrate that the inclusion of virtual nodes
significantly enhances long-term prediction accuracy while also improving
layer-wise sensitivity to mitigate the over-squashing problem. Virtual nodes
also offer enhanced explainability by focusing on key intersections and
high-traffic areas, as shown by the visualization of their adjacency matrix
weights on road network heat maps. Our advanced approach enhances the
understanding and management of urban traffic systems, making it particularly
well-suited for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatiotemporal Prediction of Secondary Crashes by Rebalancing Dynamic
  and Static Data with Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlan Chen, Yiqun Li, Chenyu Ling, Ziyuan Pu, Xiucheng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data imbalance is a common issue in analyzing and predicting sudden traffic
events. Secondary crashes constitute only a small proportion of all crashes.
These secondary crashes, triggered by primary crashes, significantly exacerbate
traffic congestion and increase the severity of incidents. However, the severe
imbalance of secondary crash data poses significant challenges for prediction
models, affecting their generalization ability and prediction accuracy.
Existing methods fail to fully address the complexity of traffic crash data,
particularly the coexistence of dynamic and static features, and often struggle
to effectively handle data samples of varying lengths. Furthermore, most
current studies predict the occurrence probability and spatiotemporal
distribution of secondary crashes separately, lacking an integrated solution.
To address these challenges, this study proposes a hybrid model named
VarFusiGAN-Transformer, aimed at improving the fidelity of secondary crash data
generation and jointly predicting the occurrence and spatiotemporal
distribution of secondary crashes. The VarFusiGAN-Transformer model employs
Long Short-Term Memory (LSTM) networks to enhance the generation of
multivariate long-time series data, incorporating a static data generator and
an auxiliary discriminator to model the joint distribution of dynamic and
static features. In addition, the model's prediction module achieves
simultaneous prediction of both the occurrence and spatiotemporal distribution
of secondary crashes. Compared to existing methods, the proposed model
demonstrates superior performance in generating high-fidelity data and
improving prediction accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Speech Recognition for Sanskrit with Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bidit Sadhukhan, Swami Punyeshwarananda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sanskrit, one of humanity's most ancient languages, has a vast collection of
books and manuscripts on diverse topics that have been accumulated over
millennia. However, its digital content (audio and text), which is vital for
the training of AI systems, is profoundly limited. Furthermore, its intricate
linguistics make it hard to develop robust NLP tools for wider accessibility.
Given these constraints, we have developed an automatic speech recognition
model for Sanskrit by employing transfer learning mechanism on OpenAI's Whisper
model. After carefully optimising the hyper-parameters, we obtained promising
results with our transfer-learned model achieving a word error rate of 15.42%
on Vaksancayah dataset. An online demo of our model is made available for the
use of public and to evaluate its performance firsthand thereby paving the way
for improved accessibility and technological support for Sanskrit learning in
the modern era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper has been accepted at the 4th International Conference on
  Computer, Communication, Control & Information Technology (C3IT), Hooghly,
  India, 2024, pp. 1-5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Crash Frequency Modeling Based on Augmented Multi-Type Data by
  Hybrid VAE-Diffusion-Based Generative Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlan Chen, Qijie He, Pei Liu, Wei Ma, Ziyuan Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crash frequency modelling analyzes the impact of factors like traffic volume,
road geometry, and environmental conditions on crash occurrences. Inaccurate
predictions can distort our understanding of these factors, leading to
misguided policies and wasted resources, which jeopardize traffic safety. A key
challenge in crash frequency modelling is the prevalence of excessive zero
observations, caused by underreporting, the low probability of crashes, and
high data collection costs. These zero observations often reduce model accuracy
and introduce bias, complicating safety decision making. While existing
approaches, such as statistical methods, data aggregation, and resampling,
attempt to address this issue, they either rely on restrictive assumptions or
result in significant information loss, distorting crash data. To overcome
these limitations, we propose a hybrid VAE-Diffusion neural network, designed
to reduce zero observations and handle the complexities of multi-type tabular
crash data (count, ordinal, nominal, and real-valued variables). We assess the
synthetic data quality generated by this model through metrics like similarity,
accuracy, diversity, and structural consistency, and compare its predictive
performance against traditional statistical models. Our findings demonstrate
that the hybrid VAE-Diffusion model outperforms baseline models across all
metrics, offering a more effective approach to augmenting crash data and
improving the accuracy of crash frequency predictions. This study highlights
the potential of synthetic data to enhance traffic safety by improving crash
frequency modelling and informing better policy decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations on Object Attributes using Multiview Images
  and Negative Instructions <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Tan, Yuzhi Li, Shengwei Meng, Xiang Yuan, Weiping Li, Tong Mo, Bingce Wang, Xu Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current popular Large Vision-Language Models (LVLMs) are suffering from
Hallucinations on Object Attributes (HoOA), leading to incorrect determination
of fine-grained attributes in the input images. Leveraging significant
advancements in 3D generation from a single image, this paper proposes a novel
method to mitigate HoOA in LVLMs. This method utilizes multiview images sampled
from generated 3D representations as visual prompts for LVLMs, thereby
providing more visual information from other viewpoints. Furthermore, we
observe the input order of multiple multiview images significantly affects the
performance of LVLMs. Consequently, we have devised Multiview Image Augmented
VLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule
capable of simultaneously eliminating the influence of input image order and
aligning visual information from multiview images with Large Language Models
(LLMs). Besides, we designed and employed negative instructions to mitigate
LVLMs' bias towards ``Yes" responses. Comprehensive experiments demonstrate the
effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Spatiotemporal Augmentation for Improving Dynamic Graph
  Learning <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Chu, Hanlin Xue, Bingce Wang, Xiaoyang Liu, Weiping Li, Tong Mo, Tuoyu Feng, Zhijie Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graph augmentation is used to improve the performance of dynamic
GNNs. Most methods assume temporal locality, meaning that recent edges are more
influential than earlier edges. However, for temporal changes in edges caused
by random noise, overemphasizing recent edges while neglecting earlier ones may
lead to the model capturing noise. To address this issue, we propose STAA
(SpatioTemporal Activity-Aware Random Walk Diffusion). STAA identifies nodes
likely to have noisy edges in spatiotemporal dimensions. Spatially, it analyzes
critical topological positions through graph wavelet coefficients. Temporally,
it analyzes edge evolution through graph wavelet coefficient change rates.
Then, random walks are used to reduce the weights of noisy edges, deriving a
diffusion matrix containing spatiotemporal information as an augmented
adjacency matrix for dynamic GNN learning. Experiments on multiple datasets
show that STAA outperforms other dynamic graph augmentation methods in node
classification and link prediction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Early Alzheimer Disease Detection with MRI Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Rafsan, Tamer Oraby, Upal Roy, Sanjeev Kumar, Hansapani Rodrigo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's Disease is a neurodegenerative condition characterized by
dementia and impairment in neurological function. The study primarily focuses
on the individuals above age 40, affecting their memory, behavior, and
cognitive processes of the brain. Alzheimer's disease requires diagnosis by a
detailed assessment of MRI scans and neuropsychological tests of the patients.
This project compares existing deep learning models in the pursuit of enhancing
the accuracy and efficiency of AD diagnosis, specifically focusing on the
Convolutional Neural Network, Bayesian Convolutional Neural Network, and the
U-net model with the Open Access Series of Imaging Studies brain MRI dataset.
Besides, to ensure robustness and reliability in the model evaluations, we
address the challenge of imbalance in data. We then perform rigorous evaluation
to determine strengths and weaknesses for each model by considering
sensitivity, specificity, and computational efficiency. This comparative
analysis would shed light on the future role of AI in revolutionizing AD
diagnostics but also paved ways for future innovation in medical imaging and
the management of neurodegenerative diseases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-guided Self-reflection for Zero-shot Hallucination Detection
  in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Liu, Xinlong Chen, Yue Ding, Shizhen Xu, Shu Wu, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination has emerged as a significant barrier to the effective
application of Large Language Models (LLMs). In this work, we introduce a novel
Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination
detection in LLMs. The AGSER method utilizes attention contributions to
categorize the input query into attentive and non-attentive queries. Each query
is then processed separately through the LLMs, allowing us to compute
consistency scores between the generated responses and the original answer. The
difference between the two consistency scores serves as a hallucination
estimator. In addition to its efficacy in detecting hallucinations, AGSER
notably reduces computational complexity, requiring only three passes through
the LLM and utilizing two sets of tokens. We have conducted extensive
experiments with four widely-used LLMs across three different hallucination
benchmarks, demonstrating that our approach significantly outperforms existing
methods in zero-shot hallucination detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast energy-aware OLSR routing in VANETs by means of a parallel
  evolutionary algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamal Toutouh, Sergio Nesmachnow, Enrique Alba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work tackles the problem of reducing the power consumption of the OLSR
routing protocol in vehicular networks. Nowadays, energy-aware and green
communication protocols are important research topics, specially when deploying
wireless mobile networks. This article introduces a fast automatic methodology
to search for energy-efficient OLSR configurations by using a parallel
evolutionary algorithm. The experimental analysis demonstrates that significant
improvements over the standard configuration can be attained in terms of power
consumption, with no noteworthy loss in the QoS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Attention Networks for Enhanced Segmentation and Depth
  Estimation of Subsurface Defects in Pulse Thermography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Salah, Naoufel Werghi, Davor Svetinovic, Yusra Abdulrahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-driven pulse thermography (PT) has become a crucial tool in
non-destructive testing (NDT), enabling automatic detection of hidden anomalies
in various industrial components. Current state-of-the-art techniques feed
segmentation and depth estimation networks compressed PT sequences using either
Principal Component Analysis (PCA) or Thermographic Signal Reconstruction
(TSR). However, treating these two modalities independently constrains the
performance of PT inspection models as these representations possess
complementary semantic features. To address this limitation, this work proposes
PT-Fusion, a multi-modal attention-based fusion network that fuses both PCA and
TSR modalities for defect segmentation and depth estimation of subsurface
defects in PT setups. PT-Fusion introduces novel feature fusion modules,
Encoder Attention Fusion Gate (EAFG) and Attention Enhanced Decoding Block
(AEDB), to fuse PCA and TSR features for enhanced segmentation and depth
estimation of subsurface defects. In addition, a novel data augmentation
technique is proposed based on random data sampling from thermographic
sequences to alleviate the scarcity of PT datasets. The proposed method is
benchmarked against state-of-the-art PT inspection models, including U-Net,
attention U-Net, and 3D-CNN on the Universit\'e Laval IRT-PVC dataset. The
results demonstrate that PT-Fusion outperforms the aforementioned models in
defect segmentation and depth estimation accuracies with a margin of 10%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pulse thermography, infrared thermography, defect segmentation,
  multi-modal networks, attention mechanism</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RichSpace: Enriching Text-to-Video <span class="highlight-title">Prompt</span> Space via Text Embedding
  Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuefan Cao, Chengyue Gong, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video generation models have made impressive progress, but they still
struggle with generating videos with complex features. This limitation often
arises from the inability of the text encoder to produce accurate embeddings,
which hinders the video generation model. In this work, we propose a novel
approach to overcome this challenge by selecting the optimal text embedding
through interpolation in the embedding space. We demonstrate that this method
enables the video generation model to produce the desired videos. Additionally,
we introduce a simple algorithm using perpendicular foot embeddings and cosine
similarity to identify the optimal interpolation embedding. Our findings
highlight the importance of accurate text embeddings and offer a pathway for
improving text-to-video generation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aneumo: A Large-Scale Comprehensive Synthetic Dataset of Aneurysm
  Hemodynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xigui Li, Yuanye Zhou, Feiyang Xiao, Xin Guo, Yichi Zhang, Chen Jiang, Jianchao Ge, Xiansheng Wang, Qimeng Wang, Taiwei Zhang, Chensen Lin, Yuan Cheng, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intracranial aneurysm (IA) is a common cerebrovascular disease that is
usually asymptomatic but may cause severe subarachnoid hemorrhage (SAH) if
ruptured. Although clinical practice is usually based on individual factors and
morphological features of the aneurysm, its pathophysiology and hemodynamic
mechanisms remain controversial. To address the limitations of current
research, this study constructed a comprehensive hemodynamic dataset of
intracranial aneurysms. The dataset is based on 466 real aneurysm models, and
10,000 synthetic models were generated by resection and deformation operations,
including 466 aneurysm-free models and 9,534 deformed aneurysm models. The
dataset also provides medical image-like segmentation mask files to support
insightful analysis. In addition, the dataset contains hemodynamic data
measured at eight steady-state flow rates (0.001 to 0.004 kg/s), including
critical parameters such as flow velocity, pressure, and wall shear stress,
providing a valuable resource for investigating aneurysm pathogenesis and
clinical prediction. This dataset will help advance the understanding of the
pathologic features and hemodynamic mechanisms of intracranial aneurysms and
support in-depth research in related fields. Dataset hosted at
https://github.com/Xigui-Li/Aneumo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GVMGen: A General Video-to-Music Generation Model with Hierarchical
  Attentions <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heda Zuo, Weitao You, Junxian Wu, Shihong Ren, Pei Chen, Mingxu Zhou, Yujia Lu, Lingyun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composing music for video is essential yet challenging, leading to a growing
interest in automating music generation for video applications. Existing
approaches often struggle to achieve robust music-video correspondence and
generative diversity, primarily due to inadequate feature alignment methods and
insufficient datasets. In this study, we present General Video-to-Music
Generation model (GVMGen), designed for generating high-related music to the
video input. Our model employs hierarchical attentions to extract and align
video features with music in both spatial and temporal dimensions, ensuring the
preservation of pertinent features while minimizing redundancy. Remarkably, our
method is versatile, capable of generating multi-style music from different
video inputs, even in zero-shot scenarios. We also propose an evaluation model
along with two novel objective metrics for assessing video-music alignment.
Additionally, we have compiled a large-scale dataset comprising diverse types
of video-music pairs. Experimental results demonstrate that GVMGen surpasses
previous models in terms of music-video correspondence, generative diversity,
and application universality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 39th AAAI Conference on Artificial Intelligence
  (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable artificial intelligence (XAI): from inherent explainability
  to <span class="highlight-title">large language model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuseini Mumuni, Alhassan Mumuni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has continued to achieve tremendous success in
recent times. However, the decision logic of these frameworks is often not
transparent, making it difficult for stakeholders to understand, interpret or
explain their behavior. This limitation hinders trust in machine learning
systems and causes a general reluctance towards their adoption in practical
applications, particularly in mission-critical domains like healthcare and
autonomous driving. Explainable AI (XAI) techniques facilitate the
explainability or interpretability of machine learning models, enabling users
to discern the basis of the decision and possibly avert undesirable behavior.
This comprehensive survey details the advancements of explainable AI methods,
from inherently interpretable models to modern approaches for achieving
interpretability of various black box models, including large language models
(LLMs). Additionally, we review explainable AI techniques that leverage LLM and
vision-language model (VLM) frameworks to automate or improve the
explainability of other machine learning models. The use of LLM and VLM as
interpretability methods particularly enables high-level, semantically
meaningful explanations of model decisions and behavior. Throughout the paper,
we highlight the scientific principles, strengths and weaknesses of
state-of-the-art methods and outline different areas of improvement. Where
appropriate, we also present qualitative and quantitative comparison results of
various methods to show how they compare. Finally, we discuss the key
challenges of XAI and directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIRCHITECT v2: Learning the Hardware Accelerator Design Space through
  Unified Representations <span class="chip">DATE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamin Seo, Akshat Ramachandran, Yu-Chuan Chuang, Anirudh Itagi, Tushar Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Design space exploration (DSE) plays a crucial role in enabling custom
hardware architectures, particularly for emerging applications like AI, where
optimized and specialized designs are essential. With the growing complexity of
deep neural networks (DNNs) and the introduction of advanced foundational
models (FMs), the design space for DNN accelerators is expanding at an
exponential rate. Additionally, this space is highly non-uniform and
non-convex, making it increasingly difficult to navigate and optimize.
Traditional DSE techniques rely on search-based methods, which involve
iterative sampling of the design space to find the optimal solution. However,
this process is both time-consuming and often fails to converge to the global
optima for such design spaces. Recently, AIrchitect v1, the first attempt to
address the limitations of search-based techniques, transformed DSE into a
constant-time classification problem using recommendation networks. In this
work, we propose AIrchitect v2, a more accurate and generalizable
learning-based DSE technique applicable to large-scale design spaces that
overcomes the shortcomings of earlier approaches. Specifically, we devise an
encoder-decoder transformer model that (a) encodes the complex design space
into a uniform intermediate representation using contrastive learning and (b)
leverages a novel unified representation blending the advantages of
classification and regression to effectively explore the large DSE space
without sacrificing accuracy. Experimental results evaluated on 10^5 real DNN
workloads demonstrate that, on average, AIrchitect v2 outperforms existing
techniques by 15% in identifying optimal design points. Furthermore, to
demonstrate the generalizability of our method, we evaluate performance on
unseen model workloads (LLMs) and attain a 1.7x improvement in inference
latency on the identified hardware architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to DATE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiPruner: Balanced Structure Removal in Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Pablo Muñoz, Jinjie Yuan, Nilesh Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, state-of-the-art approaches for pruning large pre-trained models
(LPMs) have demonstrated that the training-free removal of non-critical
residual blocks in Transformers is viable for reducing model size, achieving
results that outperform previous training-free pruning approaches. Motivated by
these findings, we extend BlockPruner (Zhong et al., 2024) and propose
MultiPruner, a pruning approach that surpasses recent training-free pruning
methods by adopting a multidimensional, iterative, fine-grained pruning
strategy. In MultiPruner, multidimensional pruning reinstates the structural
balance in block-pruned models by sequentially compressing along three
dimensions: i) residual blocks, ii) channels of multilayer perceptrons (MLP),
and iii) attention heads. This solution enhances zero-shot accuracy on
downstream tasks compared to other techniques while improving model compression
ratios, producing compressed models with fewer computing and memory
requirements. Extensive experiments demonstrate the advantages of the proposed
method across various large pre-trained models. The code and pruning
configurations are available at
https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Explainability for Power Electronics: From a Lipschitz Continuity
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinze Li, Fanfan Lin, Homer Alan Mantooth, Juan José Rodríguez-Andina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifecycle management of power converters continues to thrive with emerging
artificial intelligence (AI) solutions, yet AI mathematical explainability
remains unexplored in power electronics (PE) community. The lack of theoretical
rigor challenges adoption in mission-critical applications. Therefore, this
letter proposes a generic framework to evaluate mathematical explainability,
highlighting inference stability and training convergence from a Lipschitz
continuity perspective. Inference stability governs consistent outputs under
input perturbations, essential for robust real-time control and fault
diagnosis. Training convergence guarantees stable learning dynamics,
facilitating accurate modeling in PE contexts. Additionally, a Lipschitz-aware
learning rate selection strategy is introduced to accelerate convergence while
mitigating overshoots and oscillations. The feasibility of the proposed
Lipschitz-oriented framework is demonstrated by validating the mathematical
explainability of a state-of-the-art physics-in-architecture neural network,
and substantiated through empirical case studies on dual-active-bridge
converters. This letter serves as a clarion call for the PE community to
embrace mathematical explainability, heralding a transformative era of
trustworthy and explainable AI solutions that potentially redefine the future
of power electronics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Client-Centric Federated Adaptive Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhui Sun, Xidong Wu, Heng Huang, Aidong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed learning paradigm where clients
collaboratively train a model while keeping their own data private. With an
increasing scale of clients and models, FL encounters two key challenges,
client drift due to a high degree of statistical/system heterogeneity, and lack
of adaptivity. However, most existing FL research is based on unrealistic
assumptions that virtually ignore system heterogeneity. In this paper, we
propose Client-Centric Federated Adaptive Optimization, which is a class of
novel federated adaptive optimization approaches. We enable several features in
this framework such as arbitrary client participation, asynchronous server
aggregation, and heterogeneous local computing, which are ubiquitous in
real-world FL systems but are missed in most existing works. We provide a
rigorous convergence analysis of our proposed framework for general nonconvex
objectives, which is shown to converge with the best-known rate. Extensive
experiments show that our approaches consistently outperform the baseline by a
large margin across benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HEART: Achieving Timely Multi-Model Training for
  Vehicle-Edge-Cloud-Integrated Hierarchical Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohong Yang, Minghui Liwang, Xianbin Wang, Zhipeng Cheng, Seyyedali Hosseinalipour, Huaiyu Dai, Zhenzhen Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of AI-enabled Internet of Vehicles (IoV) calls for efficient
machine learning (ML) solutions that can handle high vehicular mobility and
decentralized data. This has motivated the emergence of Hierarchical Federated
Learning over vehicle-edge-cloud architectures (VEC-HFL). Nevertheless, one
aspect which is underexplored in the literature on VEC-HFL is that vehicles
often need to execute multiple ML tasks simultaneously, where this multi-model
training environment introduces crucial challenges. First, improper aggregation
rules can lead to model obsolescence and prolonged training times. Second,
vehicular mobility may result in inefficient data utilization by preventing the
vehicles from returning their models to the network edge. Third, achieving a
balanced resource allocation across diverse tasks becomes of paramount
importance as it majorly affects the effectiveness of collaborative training.
We take one of the first steps towards addressing these challenges via
proposing a framework for multi-model training in dynamic VEC-HFL with the goal
of minimizing global training latency while ensuring balanced training across
various tasks-a problem that turns out to be NP-hard. To facilitate timely
model training, we introduce a hybrid synchronous-asynchronous aggregation
rule. Building on this, we present a novel method called Hybrid Evolutionary
And gReedy allocaTion (HEART). The framework operates in two stages: first, it
achieves balanced task scheduling through a hybrid heuristic approach that
combines improved Particle Swarm Optimization (PSO) and Genetic Algorithms
(GA); second, it employs a low-complexity greedy algorithm to determine the
training priority of assigned tasks on vehicles. Experiments on real-world
datasets demonstrate the superiority of HEART over existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering <span class="highlight-title">Large Language Model</span>s with Feature Guided Activation Additions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Soo, Wesley Teng, Chandrasekaran Balaganesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective and reliable control over large language model (LLM) behavior is a
significant challenge. While activation steering methods, which add steering
vectors to a model's hidden states, are a promising approach, existing
techniques often lack precision and interpretability in how they influence
model outputs. We introduce Feature Guided Activation Additions (FGAA), a novel
activation steering method that leverages insights from Contrastive Activation
Addition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating
in the latent space of a Sparse Autoencoder (SAE) and employing optimization
techniques to select desired SAE features, FGAA constructs precise steering
vectors that provide better steering effects while maintaining coherence of
steered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B
models across various steering tasks demonstrate that FGAA outperforms existing
steering methods of CAA, SAE decoder steering, and SAE-TS. Our results also
highlight important trade-offs between steering scale and general model
capabilities that are consistent across all tested steering methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 maintext pages, 14 appendix pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dialogue Benchmark Generation from <span class="highlight-title">Knowledge Graph</span>s with Cost-Effective
  Retrieval-Augmented LLMs <span class="chip">SIGMOD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reham Omar, Omij Mangukiya, Essam Mansour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue benchmarks are crucial in training and evaluating chatbots engaging
in domain-specific conversations. Knowledge graphs (KGs) represent semantically
rich and well-organized data spanning various domains, such as DBLP, DBpedia,
and YAGO. Traditionally, dialogue benchmarks have been manually created from
documents, neglecting the potential of KGs in automating this process. Some
question-answering benchmarks are automatically generated using extensive
preprocessing from KGs, but they do not support dialogue generation. This paper
introduces Chatty-Gen, a novel multi-stage retrieval-augmented generation
platform for automatically generating high-quality dialogue benchmarks tailored
to a specific domain using a KG. Chatty-Gen decomposes the generation process
into manageable stages and uses assertion rules for automatic validation
between stages. Our approach enables control over intermediate results to
prevent time-consuming restarts due to hallucinations. It also reduces reliance
on costly and more powerful commercial LLMs. Chatty-Gen eliminates upfront
processing of the entire KG using efficient query-based retrieval to find
representative subgraphs based on the dialogue context. Our experiments with
several real and large KGs demonstrate that Chatty-Gen significantly
outperforms state-of-the-art systems and ensures consistent model and system
performance across multiple LLMs of diverse capabilities, such as GPT-4o,
Gemini 1.5, Llama 3, and Mistral.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is publsihed in SIGMOD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IE-Bench: Advancing the Measurement of Text-Driven Image Editing for
  Human Perception Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangkun Sun, Bowen Qu, Xiaoyu Liang, Songlin Fan, Wei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-driven image editing have been significant, yet the
task of accurately evaluating these edited images continues to pose a
considerable challenge. Different from the assessment of text-driven image
generation, text-driven image editing is characterized by simultaneously
conditioning on both text and a source image. The edited images often retain an
intrinsic connection to the original image, which dynamically change with the
semantics of the text. However, previous methods tend to solely focus on
text-image alignment or have not aligned with human perception. In this work,
we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to
enhance the assessment of text-driven edited images. IE-Bench includes a
database contains diverse source images, various editing prompts and the
corresponding results different editing methods, and total 3,010 Mean Opinion
Scores (MOS) provided by 25 human subjects. Furthermore, we introduce IE-QA, a
multi-modality source-aware quality assessment method for text-driven image
editing. To the best of our knowledge, IE-Bench offers the first IQA dataset
and model tailored for text-driven image editing. Extensive experiments
demonstrate IE-QA's superior subjective-alignments on the text-driven image
editing task compared with previous metrics. We will make all related data and
code available to the public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ForestProtector: An IoT Architecture Integrating Machine Vision and Deep
  Reinforcement Learning for Efficient Wildfire Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Bonilla-Ormachea, Horacio Cuizaga, Edwin Salcedo, Sebastian Castro, Sergio Fernandez-Testa, Misael Mamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection of forest fires is crucial to minimizing the environmental
and socioeconomic damage they cause. Indeed, a fire's duration directly
correlates with the difficulty and cost of extinguishing it. For instance, a
fire burning for 1 minute might require 1 liter of water to extinguish, while a
2-minute fire could demand 100 liters, and a 10-minute fire might necessitate
1,000 liters. On the other hand, existing fire detection systems based on novel
technologies (e.g., remote sensing, PTZ cameras, UAVs) are often expensive and
require human intervention, making continuous monitoring of large areas
impractical. To address this challenge, this work proposes a low-cost forest
fire detection system that utilizes a central gateway device with computer
vision capabilities to monitor a 360{\deg} field of view for smoke at long
distances. A deep reinforcement learning agent enhances surveillance by
dynamically controlling the camera's orientation, leveraging real-time sensor
data (smoke levels, ambient temperature, and humidity) from distributed IoT
devices. This approach enables automated wildfire monitoring across expansive
areas while reducing false positives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the proceedings of the 11th International
  Conference on Automation, Robotics, and Applications (ICARA 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Study on a Fast Solver for Combined Field Integral Equations of 3D
  Conducting Bodies Based on Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Shan, Xin Zhang, Di Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a graph neural networks (GNNs)-based fast solver
(GraphSolver) for solving combined field integral equations (CFIEs) of 3D
conducting bodies. Rao-Wilton-Glisson (RWG) basis functions are employed to
discretely and accurately represent the geometry of 3D conducting bodies. A
concise and informative graph representation is then constructed by treating
each RWG function as a node in the graph, enabling the flow of current between
nodes. With the transformed graphs, GraphSolver is developed to directly
predict real and imaginary parts of the x, y and z components of the surface
current densities at each node (RWG function). Numerical results demonstrate
the efficacy of GraphSolver in solving CFIEs for 3D conducting bodies with
varying levels of geometric complexity, including basic 3D targets,
missile-shaped targets, and airplane-shaped targets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenSC-6G: A Prototype Testbed for Integrated Generative AI, Quantum, and
  Semantic Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian E. Arfeto, Shehbaz Tariq, Uman Khalid, Trung Q. Duong, Hyundong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a prototyping testbed, GenSC-6G, developed to generate a
comprehensive dataset that supports the integration of generative artificial
intelligence (AI), quantum computing, and semantic communication for emerging
sixth-generation (6G) applications. The GenSC-6G dataset is designed with
noise-augmented synthetic data optimized for semantic decoding, classification,
and localization tasks, significantly enhancing flexibility for diverse
AI-driven communication applications. This adaptable prototype supports
seamless modifications across baseline models, communication modules, and
goal-oriented decoders. Case studies demonstrate its application in lightweight
classification, semantic upsampling, and edge-based language inference under
noise conditions. The GenSC-6G dataset serves as a scalable and robust resource
for developing goal-oriented communication systems tailored to the growing
demands of 6G networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SUBMITTED FOR PUBLICATION IN IEEE COMMUNICATIONS MAGAZINE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards A Litmus Test for Common Sense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Latapie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is the second in a planned series aimed at envisioning a path to
safe and beneficial artificial intelligence. Building on the conceptual
insights of "Common Sense Is All You Need," we propose a more formal litmus
test for common sense, adopting an axiomatic approach that combines minimal
prior knowledge (MPK) constraints with diagonal or Godel-style arguments to
create tasks beyond the agent's known concept set. We discuss how this approach
applies to the Abstraction and Reasoning Corpus (ARC), acknowledging
training/test data constraints, physical or virtual embodiment, and large
language models (LLMs). We also integrate observations regarding emergent
deceptive hallucinations, in which more capable AI systems may intentionally
fabricate plausible yet misleading outputs to disguise knowledge gaps. The
overarching theme is that scaling AI without ensuring common sense risks
intensifying such deceptive tendencies, thereby undermining safety and trust.
Aligning with the broader goal of developing beneficial AI without causing
harm, our axiomatic litmus test not only diagnoses whether an AI can handle
truly novel concepts but also provides a stepping stone toward an ethical,
reliable foundation for future safe, beneficial, and aligned artificial
intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon
  Visuomotor Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Yiqing Yang, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a low-cost quadruped manipulation system that solves long-horizon
real-world tasks, trained by reinforcement learning purely in simulation. The
system comprises 1) a hierarchical design of a high-level policy for
visual-mobile manipulation following instructions, and a low-level policy for
quadruped movement and limb-control, 2) a progressive policy expansion approach
for solving the long-horizon task together with a teacher-student framework for
efficient high-level training of the high-level visuomotor policy, and 3) a
suite of techniques for minimizing sim-to-real gaps.
  With budget-friendly but limited reliability and performance hardware, and
just one wrist-mounted RGB camera, the entire system fully trained in
simulation achieves high success rates for long horizon tasks involving search,
move, grasp, and drop-into, with fluid sim-to-real transfer in a wide variety
of indoor and outdoor scenes and lighting conditions.Extensive real-world
evaluations show that on the long horizon mobile manipulation tasks, our system
achieves good performance when transferred to real both in terms of task
success rate and execution efficiency. Finally, we discuss the necessity of our
sim-to-real techniques for legged mobile manipulation, and show their ablation
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evolving Deeper LLM Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, Xinyun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore an evolutionary search strategy for scaling inference time compute
in Large Language Models. The proposed approach, Mind Evolution, uses a
language model to generate, recombine and refine candidate responses. The
proposed approach avoids the need to formalize the underlying inference problem
whenever a solution evaluator is available. Controlling for inference cost, we
find that Mind Evolution significantly outperforms other inference strategies
such as Best-of-N and Sequential Revision in natural language planning tasks.
In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more
than 98% of the problem instances using Gemini 1.5 Pro without the use of a
formal solver.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Implementation of AI in Early Onset Interviews to Help
  Mitigate Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishka Lal, Omar Benkraouda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of artificial intelligence (AI) in
early-stage recruitment interviews in order to reduce inherent bias,
specifically sentiment bias. Traditional interviewers are often subject to
several biases, including interviewer bias, social desirability effects, and
even confirmation bias. In turn, this leads to non-inclusive hiring practices,
and a less diverse workforce. This study further analyzes various AI
interventions that are present in the marketplace today such as multimodal
platforms and interactive candidate assessment tools in order to gauge the
current market usage of AI in early-stage recruitment. However, this paper aims
to use a unique AI system that was developed to transcribe and analyze
interview dynamics, which emphasize skill and knowledge over emotional
sentiments. Results indicate that AI effectively minimizes sentiment-driven
biases by 41.2%, suggesting its revolutionizing power in companies' recruitment
processes for improved equity and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Learning Informative Trajectory Embeddings for Imitation,
  Classification and Regression <span class="chip">AAMAS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichang Ge, Changyu Chen, Arunesh Sinha, Pradeep Varakantham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world sequential decision making tasks like autonomous driving,
robotics, and healthcare, learning from observed state-action trajectories is
critical for tasks like imitation, classification, and clustering. For example,
self-driving cars must replicate human driving behaviors, while robots and
healthcare systems benefit from modeling decision sequences, whether or not
they come from expert data. Existing trajectory encoding methods often focus on
specific tasks or rely on reward signals, limiting their ability to generalize
across domains and tasks. Inspired by the success of embedding models like CLIP
and BERT in static domains, we propose a novel method for embedding
state-action trajectories into a latent space that captures the skills and
competencies in the dynamic underlying decision-making processes. This method
operates without the need for reward labels, enabling better generalization
across diverse domains and tasks. Our contributions are threefold: (1) We
introduce a trajectory embedding approach that captures multiple abilities from
state-action data. (2) The learned embeddings exhibit strong representational
power across downstream tasks, including imitation, classification, clustering,
and regression. (3) The embeddings demonstrate unique properties, such as
controlling agent behaviors in IQ-Learn and an additive structure in the latent
space. Experimental results confirm that our method outperforms traditional
approaches, offering more flexible and powerful trajectory representations for
various applications. Our code is available at
https://github.com/Erasmo1015/vte.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAMAS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuradicon: operational representation learning of neuroimaging reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.10021v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.10021v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henry Watkins, Robert Gray, Adam Julius, Yee-Haur Mah, Walter H. L. Pinaya, Paul Wright, Ashwani Jha, Holger Engleitner, Jorge Cardoso, Sebastien Ourselin, Geraint Rees, Rolf Jaeger, Parashkev Nachev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiological reports typically summarize the content and interpretation of
imaging studies in unstructured form that precludes quantitative analysis. This
limits the monitoring of radiological services to throughput undifferentiated
by content, impeding specific, targeted operational optimization. Here we
present Neuradicon, a natural language processing (NLP) framework for
quantitative analysis of neuroradiological reports. Our framework is a hybrid
of rule-based and artificial intelligence models to represent neurological
reports in succinct, quantitative form optimally suited to operational
guidance. We demonstrate the application of Neuradicon to operational
phenotyping of a corpus of 336,569 reports, and report excellent
generalizability across time and two independent healthcare institutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Moonshine: Distilling Game Content Generators into Steerable Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhe Nie, Michael Middleton, Tim Merino, Nidhushan Kanagaraja, Ashutosh Kumar, Zhan Zhuang, Julian Togelius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural Content Generation via Machine Learning (PCGML) has enhanced game
content creation, yet challenges in controllability and limited training data
persist. This study addresses these issues by distilling a constructive PCG
algorithm into a controllable PCGML model. We first generate a large amount of
content with a constructive algorithm and label it using a Large Language Model
(LLM). We use these synthetic labels to condition two PCGML models for
content-specific generation, a diffusion model and the five-dollar model. This
neural network distillation process ensures that the generation aligns with the
original algorithm while introducing controllability through plain text. We
define this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering
an alternative to prevalent text-to-image multi-modal tasks. We compare our
distilled models with the baseline constructive algorithm. Our analysis of the
variety, accuracy, and quality of our generation demonstrates the efficacy of
distilling constructive methods into controllable text-conditioned PCGML
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Types of AI Existential Risk: Decisive and Accumulative 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07836v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07836v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atoosa Kasirzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional discourse on existential risks (x-risks) from AI typically
focuses on abrupt, dire events caused by advanced AI systems, particularly
those that might achieve or surpass human-level intelligence. These events have
severe consequences that either lead to human extinction or irreversibly
cripple human civilization to a point beyond recovery. This discourse, however,
often neglects the serious possibility of AI x-risks manifesting incrementally
through a series of smaller yet interconnected disruptions, gradually crossing
critical thresholds over time. This paper contrasts the conventional "decisive
AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the
former envisions an overt AI takeover pathway, characterized by scenarios like
uncontrollable superintelligence, the latter suggests a different causal
pathway to existential catastrophes. This involves a gradual accumulation of
critical AI-induced threats such as severe vulnerabilities and systemic erosion
of economic and political structures. The accumulative hypothesis suggests a
boiling frog scenario where incremental AI risks slowly converge, undermining
societal resilience until a triggering event results in irreversible collapse.
Through systems analysis, this paper examines the distinct assumptions
differentiating these two hypotheses. It is then argued that the accumulative
view can reconcile seemingly incompatible perspectives on AI risks. The
implications of differentiating between these causal pathways -- the decisive
and the accumulative -- for the governance of AI as well as long-term AI safety
are discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal article for Philosophical Studies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Machine Learning for Remaining Useful Life Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc-André Zöller, Fabian Mauthe, Peter Zeiler, Marius Lindauer, Marco F. Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Being able to predict the remaining useful life (RUL) of an engineering
system is an important task in prognostics and health management. Recently,
data-driven approaches to RUL predictions are becoming prevalent over
model-based approaches since no underlying physical knowledge of the
engineering system is required. Yet, this just replaces required expertise of
the underlying physics with machine learning (ML) expertise, which is often
also not available. Automated machine learning (AutoML) promises to build
end-to-end ML pipelines automatically enabling domain experts without ML
expertise to create their own models. This paper introduces AutoRUL, an
AutoML-driven end-to-end approach for automatic RUL predictions. AutoRUL
combines fine-tuned standard regression methods to an ensemble with high
predictive power. By evaluating the proposed method on eight real-world and
synthetic datasets against state-of-the-art hand-crafted models, we show that
AutoML provides a viable alternative to hand-crafted data-driven RUL
predictions. Consequently, creating RUL predictions can be made more accessible
for domain experts using AutoML by eliminating ML expertise from data-driven
model construction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript accepted at IEEE SMC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Large Reasoning Models: A <span class="highlight-title">Survey</span> on Scaling LLM Reasoning
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language has long been conceived as an essential tool for human reasoning.
The breakthrough of Large Language Models (LLMs) has sparked significant
research interest in leveraging these models to tackle complex reasoning tasks.
Researchers have moved beyond simple autoregressive token generation by
introducing the concept of "thought" -- a sequence of tokens representing
intermediate steps in the reasoning process. This innovative paradigm enables
LLMs' to mimic complex human reasoning processes, such as tree search and
reflective thinking. Recently, an emerging trend of learning to reason has
applied reinforcement learning (RL) to train LLMs to master reasoning
processes. This approach enables the automatic generation of high-quality
reasoning trajectories through trial-and-error search algorithms, significantly
expanding LLMs' reasoning capacity by providing substantially more training
data. Furthermore, recent studies demonstrate that encouraging LLMs to "think"
with more tokens during test-time inference can further significantly boost
reasoning accuracy. Therefore, the train-time and test-time scaling combined to
show a new research frontier -- a path toward Large Reasoning Model. The
introduction of OpenAI's o1 series marks a significant milestone in this
research direction. In this survey, we present a comprehensive review of recent
progress in LLM reasoning. We begin by introducing the foundational background
of LLMs and then explore the key technical components driving the development
of large reasoning models, with a focus on automated data construction,
learning-to-reason techniques, and test-time scaling. We also analyze popular
open-source projects at building large reasoning models, and conclude with open
challenges and future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Model</span> is Secretly a Protein Sequence Optimizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinkai Wang, Jiaxing He, Yuanqi Du, Xiaohui Chen, Jianan Canal Li, Li-Ping Liu, Xiaolin Xu, Soha Hassoun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the protein sequence engineering problem, which aims to find
protein sequences with high fitness levels, starting from a given wild-type
sequence. Directed evolution has been a dominating paradigm in this field which
has an iterative process to generate variants and select via experimental
feedback. We demonstrate large language models (LLMs), despite being trained on
massive texts, are secretly protein sequence optimizers. With a directed
evolutionary method, LLM can perform protein engineering through Pareto and
experiment-budget constrained optimization, demonstrating success on both
synthetic and experimental fitness landscapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Diversity and Uncertainty in Active learning with
  <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Doucet, Benjamin Estermann, Till Aczel, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the integration of diversity-based and uncertainty-based
sampling strategies in active learning, particularly within the context of
self-supervised pre-trained models. We introduce a straightforward heuristic
called TCM that mitigates the cold start problem while maintaining strong
performance across various data levels. By initially applying TypiClust for
diversity sampling and subsequently transitioning to uncertainty sampling with
Margin, our approach effectively combines the strengths of both strategies. Our
experiments demonstrate that TCM consistently outperforms existing methods
across various datasets in both low and high data regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024 Workshop on Practical Machine Learning for Low
  Resource Settings (PML4LRS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Animal-AI Environment: A Virtual Laboratory For Comparative
  Cognition and Artificial Intelligence Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11414v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11414v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Voudouris, Ibrahim Alhas, Wout Schellaert, Matteo G. Mecattaf, Ben Slater, Matthew Crosby, Joel Holmes, John Burden, Niharika Chaubey, Niall Donnelly, Matishalin Patel, Marta Halina, José Hernández-Orallo, Lucy G. Cheke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Animal-AI Environment is a unique game-based research platform designed
to facilitate collaboration between the artificial intelligence and comparative
cognition research communities. In this paper, we present the latest version of
the Animal-AI Environment, outlining several major features that make the game
more engaging for humans and more complex for AI systems. These features
include interactive buttons, reward dispensers, and player notifications, as
well as an overhaul of the environment's graphics and processing for
significant improvements in agent training time and quality of the human player
experience. We provide detailed guidance on how to build computational and
behavioural experiments with the Animal-AI Environment. We present results from
a series of agents, including the state-of-the-art deep reinforcement learning
agent Dreamer-v3, on newly designed tests and the Animal-AI Testbed of 900
tasks inspired by research in the field of comparative cognition. The Animal-AI
Environment offers a new approach for modelling cognition in humans and
non-human animals, and for building biologically inspired artificial
intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 16 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Quantization for Matrix Multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Ordentlich, Yury Polyanskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in machine learning community proposed multiple methods for
performing lossy compression (quantization) of large matrices. This
quantization is important for accelerating matrix multiplication (main
component of large language models), which is often bottlenecked by the speed
of loading these matrices from memory. Unlike classical vector quantization and
rate-distortion theory, the goal of these new compression algorithms is to be
able to approximate not the matrices themselves, but their matrix product.
Specifically, given a pair of real matrices $A,B$ an encoder (compressor) is
applied to each of them independently producing descriptions with $R$ bits per
entry. These representations subsequently are used by the decoder to estimate
matrix product $A^\top B$. In this work, we provide a non-asymptotic lower
bound on the mean squared error of this approximation (as a function of rate
$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,
we construct a universal quantizer based on nested lattices with an explicit
guarantee of approximation error for any (non-random) pair of matrices $A$, $B$
in terms of only Frobenius norms $\|\bar{A}\|_F, \|\bar{B}\|_F$ and
$\|\bar{A}^\top \bar{B}\|_F$, where $\bar{A},\bar{B}$ are versions of $A,B$
with zero-centered columns, respectively. For iid Gaussian matrices our
quantizer achieves the lower bound and is, thus, asymptotically optimal. A
practical low-complexity version of our quantizer achieves performance quite
close to optimal. In addition, we derive rate-distortion function for matrix
multiplication of iid Gaussian matrices, which exhibits an interesting
phase-transition at $R\approx 0.906$ bit/entry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Compression Autoen<span class="highlight-title">code</span>r for Efficient High-Resolution Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10733v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10733v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder
models for accelerating high-resolution diffusion models. Existing autoencoder
models have demonstrated impressive results at a moderate spatial compression
ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for
high spatial compression ratios (e.g., 64x). We address this challenge by
introducing two key techniques: (1) Residual Autoencoding, where we design our
models to learn residuals based on the space-to-channel transformed features to
alleviate the optimization difficulty of high spatial-compression autoencoders;
(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases
training strategy for mitigating the generalization penalty of high
spatial-compression autoencoders. With these designs, we improve the
autoencoder's spatial compression ratio up to 128 while maintaining the
reconstruction quality. Applying our DC-AE to latent diffusion models, we
achieve significant speedup without accuracy drop. For example, on ImageNet
512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup
on H100 GPU for UViT-H while achieving a better FID, compared with the widely
used SD-VAE-f8 autoencoder. Our code is available at
https://github.com/mit-han-lab/efficientvit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. First two authors contributed equally to this work. Update:
  fix typo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generate E-commerce Product Background by Integrating Category
  Commonality and Personalized Style <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohan Wang, Wei Feng, Yaoyu Li, Zheng Zhang, Jingjing Lv, Junjie Shen, Zhangang Lin, Jingping Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state-of-the-art methods for e-commerce product background generation
suffer from the inefficiency of designing product-wise prompts when scaling up
the production, as well as the ineffectiveness of describing fine-grained
styles when customizing personalized backgrounds for some specific brands. To
address these obstacles, we integrate the category commonality and personalized
style into diffusion models. Concretely, we propose a Category-Wise Generator
to enable large-scale background generation with only one model for the first
time. A unique identifier in the prompt is assigned to each category, whose
attention is located on the background by a mask-guided cross attention layer
to learn the category-wise style. Furthermore, for products with specific and
fine-grained requirements in layout, elements, etc, a Personality-Wise
Generator is devised to learn such personalized style directly from a reference
image to resolve textual ambiguities, and is trained in a self-supervised
manner for more efficient training data usage. To advance research in this
field, the first large-scale e-commerce product background generation dataset
BG60k is constructed, which covers more than 60k product images from over 2k
categories. Experiments demonstrate that our method could generate high-quality
backgrounds for different categories, and maintain the personalized background
style of reference images. BG60k will be available at
\url{https://github.com/Whileherham/BG60k}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ News Without Borders: Domain Adaptation of Multilingual Sentence
  Embeddings for Cross-lingual News Recommendation <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreea Iana, Fabian David Schmidt, Goran Glavaš, Heiko Paulheim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapidly growing numbers of multilingual news consumers pose an increasing
challenge to news recommender systems in terms of providing customized
recommendations. First, existing neural news recommenders, even when powered by
multilingual language models (LMs), suffer substantial performance losses in
zero-shot cross-lingual transfer (ZS-XLT). Second, the current paradigm of
fine-tuning the backbone LM of a neural recommender on task-specific data is
computationally expensive and infeasible in few-shot recommendation and
cold-start setups, where data is scarce or completely unavailable. In this
work, we propose a news-adapted sentence encoder (NaSE), domain-specialized
from a pretrained massively multilingual sentence encoder (SE). To this end, we
construct and leverage PolyNews and PolyNewsParallel, two multilingual
news-specific corpora. With the news-adapted multilingual SE in place, we test
the effectiveness of (i.e., question the need for) supervised fine-tuning for
news recommendation, and propose a simple and strong baseline based on (i)
frozen NaSE embeddings and (ii) late click-behavior fusion. We show that NaSE
achieves state-of-the-art performance in ZS-XLT in true cold-start and few-shot
news recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 47th European Conference on Information Retrieval
  (ECIR 2025) Appendix A is provided only in the arXiv version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by
  Real-time MRI <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil Shah, Ayan Kashyap, Shirish Karande, Vineet Gandhi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous real-time MRI (rtMRI)-based speech synthesis models depend heavily
on noisy ground-truth speech. Applying loss directly over ground truth
mel-spectrograms entangles speech content with MRI noise, resulting in poor
intelligibility. We introduce a novel approach that adapts the multi-modal
self-supervised AV-HuBERT model for text prediction from rtMRI and incorporates
a new flow-based duration predictor for speaker-specific alignment. The
predicted text and durations are then used by a speech decoder to synthesize
aligned speech in any novel voice. We conduct thorough experiments on two
datasets and demonstrate our method's generalization ability to unseen
speakers. We assess our framework's performance by masking parts of the rtMRI
video to evaluate the impact of different articulators on text prediction. Our
method achieves a $15.18\%$ Word Error Rate (WER) on the USC-TIMIT MRI corpus,
marking a huge improvement over the current state-of-the-art. Speech samples
are available at https://mri2speech.github.io/MRI2Speech/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Driven Reinforcement Learning for Head-Orientation in Naturalistic
  Environments <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wessel Ledder, Yuzhen Qin, Kiki van der Heijden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep reinforcement learning (DRL) approaches in audio signal
processing have seen substantial progress in recent years, audio-driven DRL for
tasks such as navigation, gaze control and head-orientation control in the
context of human-robot interaction have received little attention. Here, we
propose an audio-driven DRL framework in which we utilise deep Q-learning to
develop an autonomous agent that orients towards a talker in the acoustic
environment based on stereo speech recordings. Our results show that the agent
learned to perform the task at a near perfect level when trained on speech
segments in anechoic environments (that is, without reverberation). The
presence of reverberation in naturalistic acoustic environments affected the
agent's performance, although the agent still substantially outperformed a
baseline, randomly acting agent. Finally, we quantified the degree of
generalization of the proposed DRL approach across naturalistic acoustic
environments. Our experiments revealed that policies learned by agents trained
on medium or high reverb environments generalized to low reverb environments,
but policies learned by agents trained on anechoic or low reverb environments
did not generalize to medium or high reverb environments. Taken together, this
study demonstrates the potential of audio-driven DRL for tasks such as
head-orientation control and highlights the need for training strategies that
enable robust generalization across environments for real-world audio-driven
DRL applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02957v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02957v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Li, Yunghwei Lai, Weitao Li, Jingyi Ren, Meng Zhang, Xinhui Kang, Siyu Wang, Peng Li, Ya-Qin Zhang, Weizhi Ma, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent rapid development of large language models (LLMs) has sparked a
new wave of technological revolution in medical artificial intelligence (AI).
While LLMs are designed to understand and generate text like a human,
autonomous agents that utilize LLMs as their "brain" have exhibited
capabilities beyond text processing such as planning, reflection, and using
tools by enabling their "bodies" to interact with the environment. We introduce
a simulacrum of hospital called Agent Hospital that simulates the entire
process of treating illness, in which all patients, nurses, and doctors are
LLM-powered autonomous agents. Within the simulacrum, doctor agents are able to
evolve by treating a large number of patient agents without the need to label
training data manually. After treating tens of thousands of patient agents in
the simulacrum (human doctors may take several years in the real world), the
evolved doctor agents outperform state-of-the-art medical agent methods on the
MedQA benchmark comprising US Medical Licensing Examination (USMLE) test
questions. Our methods of simulacrum construction and agent evolution have the
potential in benefiting a broad range of applications beyond medical AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Piece of Table: A Divide-and-Conquer Approach for Selecting Sub-Tables
  in Table Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjin Lee, Kyumin Kim, Sungjae Lee, Jihun Lee, Kwang In Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying language models (LMs) to tables is challenging due to the inherent
structural differences between two-dimensional tables and one-dimensional text
for which the LMs were originally designed. Furthermore, when applying
linearized tables to LMs, the maximum token lengths often imposed in
self-attention calculations make it difficult to comprehensively understand the
context spread across large tables. To address these challenges, we present
PieTa (Piece of Table), a new framework for sub-table-based question answering
(QA). PieTa operates through an iterative process of dividing tables into
smaller windows, using LMs to select relevant cells within each window, and
merging these cells into a sub-table. This multi-resolution approach captures
dependencies across multiple rows and columns while avoiding the limitations
caused by long context inputs. Instantiated as a simple iterative sub-table
union algorithm, PieTa demonstrates improved performance over previous
sub-table-based QA approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Process Models: A Vision for Business Process Management in the
  Age of Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00900v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00900v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timotheus Kampik, Christian Warmuth, Adrian Rebmann, Ron Agam, Lukas N. P. Egger, Andreas Gerber, Johannes Hoffart, Jonas Kolk, Philipp Herzig, Gero Decker, Han van der Aa, Artem Polyvyanyy, Stefanie Rinderle-Ma, Ingo Weber, Matthias Weidlich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The continued success of Large Language Models (LLMs) and other generative
artificial intelligence approaches highlights the advantages that large
information corpora can have over rigidly defined symbolic models, but also
serves as a proof-point of the challenges that purely statistics-based
approaches have in terms of safety and trustworthiness. As a framework for
contextualizing the potential, as well as the limitations of LLMs and other
foundation model-based technologies, we propose the concept of a Large Process
Model (LPM) that combines the correlation power of LLMs with the analytical
precision and reliability of knowledge-based systems and automated reasoning
approaches. LPMs are envisioned to directly utilize the wealth of process
management experience that experts have accumulated, as well as process
performance data of organizations with diverse characteristics, e.g.,\
regarding size, region, or industry. In this vision, the proposed LPM would
allow organizations to receive context-specific (tailored) process and other
business models, analytical deep-dives, and improvement recommendations. As
such, they would allow to substantially decrease the time and effort required
for business transformation, while also allowing for deeper, more impactful,
and more actionable insights than previously possible. We argue that
implementing an LPM is feasible, but also highlight limitations and research
challenges that need to be solved to implement particular aspects of the LPM
vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative AI in Cybersecurity: A Comprehensive <span class="highlight-title">Review</span> of LLM
  Applications and Vulnerabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.12750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.12750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Amine Ferrag, Fatima Alwahedi, Ammar Battah, Bilel Cherif, Abdechakour Mechri, Norbert Tihanyi, Tamas Bisztray, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a comprehensive review of the future of cybersecurity
through Generative AI and Large Language Models (LLMs). We explore LLM
applications across various domains, including hardware design security,
intrusion detection, software engineering, design verification, cyber threat
intelligence, malware detection, and phishing detection. We present an overview
of LLM evolution and its current state, focusing on advancements in models such
as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends
to LLM vulnerabilities, such as prompt injection, insecure output handling,
data poisoning, DDoS attacks, and adversarial instructions. We delve into
mitigation strategies to protect these models, providing a comprehensive look
at potential attack scenarios and prevention techniques. Furthermore, we
evaluate the performance of 42 LLM models in cybersecurity knowledge and
hardware security, highlighting their strengths and weaknesses. We thoroughly
evaluate cybersecurity datasets for LLM training and testing, covering the
lifecycle from data creation to usage and identifying gaps for future research.
In addition, we review new strategies for leveraging LLMs, including techniques
like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human
Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank
Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim
to enhance real-time cybersecurity defenses and improve the sophistication of
LLM applications in threat detection and response. Our paper provides a
foundational understanding and strategic direction for integrating LLMs into
future cybersecurity frameworks, emphasizing innovation and robust model
deployment to safeguard against evolving cyber threats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tarsier2: Advancing Large Vision-Language Models from Detailed Video
  Description to Comprehensive Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, Yuan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM)
designed for generating detailed and accurate video descriptions, while also
exhibiting superior general video understanding capabilities. Tarsier2 achieves
significant advancements through three key upgrades: (1) Scaling pre-training
data from 11M to 40M video-text pairs, enriching both volume and diversity; (2)
Performing fine-grained temporal alignment during supervised fine-tuning; (3)
Using model-based sampling to automatically construct preference data and
applying DPO training for optimization. Extensive experiments show that
Tarsier2-7B consistently outperforms leading proprietary models, including
GPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K
benchmark, Tarsier2-7B improves F1 by 2.8\% over GPT-4o and 5.8\% over
Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\%
performance advantage over GPT-4o and +24.9\% over Gemini-1.5-Pro. Tarsier2-7B
also sets new state-of-the-art results across 15 public benchmarks, spanning
tasks such as video question-answering, video grounding, hallucination test,
and embodied question-answering, demonstrating its versatility as a robust
generalist vision-language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XEQ Scale for Evaluating XAI Experience Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10662v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10662v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjana Wijekoon, Nirmalie Wiratunga, David Corsar, Kyle Martin, Ikechukwu Nkisi-Orji, Belen Díaz-Agudo, Derek Bridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) aims to improve the transparency of
autonomous decision-making through explanations. Recent literature has
emphasised users' need for holistic "multi-shot" explanations and personalised
engagement with XAI systems. We refer to this user-centred interaction as an
XAI Experience. Despite advances in creating XAI experiences, evaluating them
in a user-centred manner has remained challenging. In response, we developed
the XAI Experience Quality (XEQ) Scale. XEQ quantifies the quality of
experiences across four dimensions: learning, utility, fulfilment and
engagement. These contributions extend the state-of-the-art of XAI evaluation,
moving beyond the one-dimensional metrics frequently developed to assess
single-shot explanations. This paper presents the XEQ scale development and
validation process, including content validation with XAI experts, and
discriminant and construct validation through a large-scale pilot study. Our
pilot study results offer strong evidence that establishes the XEQ Scale as a
comprehensive framework for evaluating user-centred XAI experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Low-Resource Language Retrieval: Establishing Baselines for
  Urdu MS MARCO <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umer Butt, Stalin Veranasi, Günter Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the Information Retrieval (IR) field increasingly recognizes the
importance of inclusivity, addressing the needs of low-resource languages
remains a significant challenge. This paper introduces the first large-scale
Urdu IR dataset, created by translating the MS MARCO dataset through machine
translation. We establish baseline results through zero-shot learning for IR in
Urdu and subsequently apply the mMARCO multilingual IR methodology to this
newly translated dataset. Our findings demonstrate that the fine-tuned model
(Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a
Recall@10 of 0.439, representing significant improvements over zero-shot
results and showing the potential for expanding IR access for Urdu speakers. By
bridging access gaps for speakers of low-resource languages, this work not only
advances multilingual IR research but also emphasizes the ethical and societal
importance of inclusive IR technologies. This work provides valuable insights
into the challenges and solutions for improving language representation and
lays the groundwork for future research, especially in South Asian languages,
which can benefit from the adaptable methods used in this study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, ECIR 2025, conference camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLSBench: Unveiling Visual Leakage in Multimodal Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhao Hu, Dongrui Liu, Hao Li, Xuanjing Huang, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety concerns of Multimodal large language models (MLLMs) have gradually
become an important problem in various applications. Surprisingly, previous
works indicate a counter-intuitive phenomenon that using textual unlearning to
align MLLMs achieves comparable safety performances with MLLMs trained with
image-text pairs. To explain such a counter-intuitive phenomenon, we discover a
visual safety information leakage (VSIL) problem in existing multimodal safety
benchmarks, i.e., the potentially risky and sensitive content in the image has
been revealed in the textual query. In this way, MLLMs can easily refuse these
sensitive text-image queries according to textual queries. However, image-text
pairs without VSIL are common in real-world scenarios and are overlooked by
existing multimodal safety benchmarks. To this end, we construct multimodal
visual leakless safety benchmark (VLSBench) preventing visual safety leakage
from image to textual query with 2.4k image-text pairs. Experimental results
indicate that VLSBench poses a significant challenge to both open-source and
close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.
This study demonstrates that textual alignment is enough for multimodal safety
scenarios with VSIL, while multimodal alignment is a more promising solution
for multimodal safety scenarios without VSIL. Please see our code and data at:
https://hxhcreate.github.io/vlsbench.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Sycophancy in De<span class="highlight-title">code</span>r-Only <span class="highlight-title">Transformer</span> Architectures:
  Synthetic Data Intervention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10156v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10156v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the sycophancy problem caused by reinforcement learning from human
feedback in large language models, this research applies synthetic data
intervention technology to the decoder-only transformer architecture. Based on
the research gaps in the existing literature, the researcher designed an
experimental process to reduce the tendency of models to cater by generating
diversified data, and used GPT4o as an experimental tool for verification. The
experiment used 100 true and false questions, and compared the performance of
the model trained with synthetic data intervention and the original untrained
model on multiple indicators. The results show that the SDI training model
supports the technology in terms of accuracy rate and sycophancy rate and has
significant effectiveness in reducing sycophancy phenomena. Notably, the data
set, experimental process, code and data results have been uploaded to Github,
the link is https://github.com/brucewang123456789/GeniusTrail.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This research is also submitted to OpenReview. The main text is 9
  pages (excluding citations), 7 figures, and 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix
  Sharing and Throughput-oriented Token Batching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) increasingly play an important role in a wide
range of information processing and management tasks. Many of these tasks are
performed in large batches or even offline, and the performance indictor for
which is throughput. These tasks usually show the characteristic of prefix
sharing, where different prompt input can partially show the common prefix.
However, the existing LLM inference engines tend to optimize the streaming
requests and show limitations of supporting the large batched tasks with the
prefix sharing characteristic. The existing solutions use the LRU-based cache
to reuse the KV context of common prefix between requests. The KV context that
are about to be reused may prematurely evicted with the implicit cache
management. Besides, the streaming oriented systems do not leverage the
request-batch information and can not mix the decoding tokens with the prefill
chunks to the best for the batched scenarios, and thus fails to saturate the
GPU. We propose BatchLLM to address the above problems. BatchLLM explicitly
identifies the common prefixes globally. The requests sharing the same prefix
will be scheduled together to reuse the KV context the best. BatchLLM reorders
the requests and schedules the requests with larger ratio of decoding first to
better mix the decoding tokens with the latter prefill chunks, and applies
memory-centric token batching to enlarge the token-batch sizes, which helps to
increase the GPU utilization. Finally, BatchLLM optimizes the prefix-shared
Attention kernel with horizontal fusion to reduce tail effect and kernel launch
overhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang
by 1.3$\times$ to 10.8$\times$ on a set of microbenchmarks and a typical
industry workload under different hardware environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language
  Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaut Thonet, Jos Rozen, Laurent Besacier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on Large Language Models (LLMs) has recently witnessed an increasing
interest in extending the models' context size to better capture dependencies
within long documents. While benchmarks have been proposed to assess long-range
abilities, existing efforts primarily considered generic tasks that are not
necessarily aligned with real-world applications. In contrast, we propose a new
benchmark for long-context LLMs focused on a practical meeting assistant
scenario in which the long contexts consist of transcripts obtained by
automatic speech recognition, presenting unique challenges for LLMs due to the
inherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench,
augments the existing ELITR corpus by adding 271 manually crafted questions
with their ground-truth answers, as well as noisy versions of meeting
transcripts altered to target different Word Error Rate levels. Our experiments
with 12 long-context LLMs on ELITR-Bench confirm the progress made across
successive generations of both proprietary and open models, and point out their
discrepancies in terms of robustness to transcript noise. We also provide a
thorough analysis of our GPT-4-based evaluation, including insights from a
crowdsourcing study. Our findings indicate that while GPT-4's scores align with
human judges, its ability to distinguish beyond three score levels may be
limited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning from Human Feedback: Whose Culture, Whose Values,
  Whose Perspectives? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristian González Barman, Simon Lohse, Henk de Regt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We argue for the epistemic and ethical advantages of pluralism in
Reinforcement Learning from Human Feedback (RLHF) in the context of Large
Language Models (LLM). Drawing on social epistemology and pluralist philosophy
of science, we suggest ways in which RHLF can be made more responsive to human
needs and how we can address challenges along the way. The paper concludes with
an agenda for change, i.e. concrete, actionable steps to improve LLM
development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating analytical variability in fMRI results with style transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03703v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03703v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elodie Germani, Camille Maumet, Elisa Fromont
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to improve the reproducibility of neuroimaging
results by converting statistic maps across different functional MRI pipelines.
We make the assumption that pipelines used to compute fMRI statistic maps can
be considered as a style component and we propose to use different generative
models, among which, Generative Adversarial Networks (GAN) and Diffusion Models
(DM) to convert statistic maps across different pipelines. We explore the
performance of multiple GAN frameworks, and design a new DM framework for
unsupervised multi-domain styletransfer. We constrain the generation of 3D fMRI
statistic maps using the latent space of an auxiliary classifier that
distinguishes statistic maps from different pipelines and extend traditional
sampling techniques used in DM to improve the transition performance. Our
experiments demonstrate that our proposed methods aresuccessful: pipelines can
indeed be transferred as a style component, providing animportant source of
data augmentation for future medical studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Median (GM) Matching for Robust Data Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Acharya, Inderjit S Dhillon, Sujay Sanghavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale data collections in the wild, are invariably noisy. Thus
developing data pruning strategies that remain robust even in the presence of
corruption is critical in practice. In this work, we propose Geometric Median
($\gm$) Matching -- a herding style greedy algorithm that yields a $k$-subset
such that the mean of the subset approximates the geometric median of the
(potentially) noisy dataset. Theoretically, we show that $\gm$ Matching enjoys
an improved $\gO(1/k)$ scaling over $\gO(1/\sqrt{k})$ scaling of uniform
sampling; while achieving {\bf optimal breakdown point} of {\bf 1/2} even under
{\bf arbitrary} corruption. Extensive experiments across several popular deep
learning benchmarks indicate that $\gm$ Matching consistently improves over
prior state-of-the-art; the gains become more profound at high rates of
corruption and aggressive pruning rates; making $\gm$ Matching a strong
baseline for future research in robust data pruning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-guided Image Restoration and Semantic Enhancement for Text-to-Image
  Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09059v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09059v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific
person images according to the given textual descriptions. A primary challenge
in this task is bridging the substantial representational gap between visual
and textual modalities. The prevailing methods map texts and images into
unified embedding space for matching, while the intricate semantic
correspondences between texts and images are still not effectively constructed.
To address this issue, we propose a novel TIPR framework to build fine-grained
interactions and alignment between person images and the corresponding texts.
Specifically, via fine-tuning the Contrastive Language-Image Pre-training
(CLIP) model, a visual-textual dual encoder is firstly constructed, to
preliminarily align the image and text features. Secondly, a Text-guided Image
Restoration (TIR) auxiliary task is proposed to map abstract textual entities
to specific image regions, improving the alignment between local textual and
visual embeddings. Additionally, a cross-modal triplet loss is presented to
handle hard samples, and further enhance the model's discriminability for minor
differences. Moreover, a pruning-based text data augmentation approach is
proposed to enhance focus on essential elements in descriptions, thereby
avoiding excessive model attention to less significant information. The
experimental results show our proposed method outperforms state-of-the-art
methods on three popular benchmark datasets, and the code will be made publicly
available at https://github.com/Delong-liu-bupt/SEN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was withdrawn due to a dispute among the authors regarding
  the content of the article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Instruction Tuning with <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Liang, Tianyu Zheng, Xinrun Du, Ge Zhang, Xingwei Qu, Xiang Yue, Chujie Zheng, Jiaheng Liu, Lei Ma, Wenhu Chen, Guoyin Wang, Zhaoxiang Zhang, Wenhao Huang, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning enhances large language models (LLMs) to follow human
instructions across diverse tasks, relying on high-quality datasets to guide
behavior. However, these datasets, whether manually curated or synthetically
generated, are often narrowly focused and misaligned with the broad
distributions captured during pre-training, limiting LLM generalization and
effective use of pre-trained knowledge. We propose *Aligning Instruction Tuning
with Pre-training* (AITP), a method that bridges this gap by identifying
coverage shortfalls in instruction-tuning datasets and rewriting
underrepresented pre-training data into high-quality instruction-response
pairs. This approach enriches dataset diversity while preserving task-specific
objectives. Evaluations on three fully open LLMs across eight benchmarks
demonstrate consistent performance improvements with AITP. Ablations highlight
the benefits of adaptive data selection, controlled rewriting, and balanced
integration, emphasizing the importance of aligning instruction tuning with
pre-training distributions to unlock the full potential of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Elucidating the Design Space of Dataset Condensation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13733v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13733v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shitong Shao, Zikai Zhou, Huanran Chen, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset condensation, a concept within data-centric learning, efficiently
transfers critical attributes from an original dataset to a synthetic version,
maintaining both diversity and realism. This approach significantly improves
model training efficiency and is adaptable across multiple application areas.
Previous methods in dataset condensation have faced challenges: some incur high
computational costs which limit scalability to larger datasets (e.g., MTT,
DREAM, and TESLA), while others are restricted to less optimal design spaces,
which could hinder potential improvements, especially in smaller datasets
(e.g., SRe2L, G-VBSM, and RDED). To address these limitations, we propose a
comprehensive design framework that includes specific, effective strategies
like implementing soft category-aware matching and adjusting the learning rate
schedule. These strategies are grounded in empirical evidence and theoretical
backing. Our resulting approach, Elucidate Dataset Condensation (EDC),
establishes a benchmark for both small and large-scale dataset condensation. In
our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on
ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a
compression ratio of 0.78%. This performance exceeds those of SRe2L, G-VBSM,
and RDED by margins of 27.3%, 17.2%, and 6.6%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce AceMath, a suite of frontier math models that
excel in solving complex math problems, along with highly effective reward
models capable of evaluating generated solutions and reliably identifying the
correct ones. To develop the instruction-tuned math models, we propose a
supervised fine-tuning (SFT) process that first achieves competitive
performance across general domains, followed by targeted fine-tuning for the
math domain using a carefully curated set of prompts and synthetically
generated responses. The resulting model, AceMath-72B-Instruct greatly
outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop
math-specialized reward model, we first construct AceMath-RewardBench, a
comprehensive and robust benchmark for evaluating math reward models across
diverse problems and difficulty levels. After that, we present a systematic
approach to build our math reward models. The resulting model, AceMath-72B-RM,
consistently outperforms state-of-the-art reward models. Furthermore, when
combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest
average rm@8 score across the math reasoning benchmarks. We release model
weights, training data, and evaluation benchmarks at:
https://research.nvidia.com/labs/adlr/acemath
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing small projectors and multiple views for efficient vision
  <span class="highlight-title">pretrain</span>ing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Krishna Agrawal, Arna Ghosh, Shagun Sodhani, Adam Oberman, Blake Richards
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in self-supervised (SSL) visual representation learning has
led to the development of several different proposed frameworks that rely on
augmentations of images but use different loss functions. However, there are
few theoretically grounded principles to guide practice, so practical
implementation of each SSL framework requires several heuristics to achieve
competitive performance. In this work, we build on recent analytical results to
design practical recommendations for competitive and efficient SSL that are
grounded in theory. Specifically, recent theory tells us that existing SSL
frameworks are minimizing the same idealized loss, which is to learn features
that best match the data similarity kernel defined by the augmentations used.
We show how this idealized loss can be reformulated to a functionally
equivalent loss that is more efficient to compute. We study the implicit bias
of using gradient descent to minimize our reformulated loss function and find
that using a stronger orthogonalization constraint with a reduced projector
dimensionality should yield good representations. Furthermore, the theory tells
us that approximating the reformulated loss should be improved by increasing
the number of augmentations, and as such using multiple augmentations should
lead to improved convergence. We empirically verify our findings on CIFAR, STL
and Imagenet datasets, wherein we demonstrate an improved linear readout
performance when training a ResNet-backbone using our theoretically grounded
recommendations. Remarkably, we also demonstrate that by leveraging these
insights, we can reduce the pretraining dataset size by up to 2$\times$ while
maintaining downstream accuracy simply by using more data augmentations. Taken
together, our work provides theoretically grounded recommendations that can be
used to improve SSL convergence and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against
  Model Extraction Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Xu, Binxing Fang, Rui Wang, Yinghai Zhou, Shouling Ji, Yuan Liu, Mohan Li, Zhihong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing high-performance deep learning models is resource-intensive,
leading model owners to utilize Machine Learning as a Service (MLaaS) platforms
instead of publicly releasing their models. However, malicious users may
exploit query interfaces to execute model extraction attacks, reconstructing
the target model's functionality locally. While prior research has investigated
triggerable watermarking techniques for asserting ownership, existing methods
face significant challenges: (1) most approaches require additional training,
resulting in high overhead and limited flexibility, and (2) they often fail to
account for advanced attackers, leaving them vulnerable to adaptive attacks.
  In this paper, we propose Neural Honeytrace, a robust plug-and-play
watermarking framework against model extraction attacks. We first formulate a
watermark transmission model from an information-theoretic perspective,
providing an interpretable account of the principles and limitations of
existing triggerable watermarking. Guided by the model, we further introduce:
(1) a similarity-based training-free watermarking method for plug-and-play and
flexible watermarking, and (2) a distribution-based multi-step watermark
information transmission strategy for robust watermarking. Comprehensive
experiments on four datasets demonstrate that Neural Honeytrace outperforms
previous methods in efficiency and resisting adaptive attacks. Neural
Honeytrace reduces the average number of samples required for a worst-case
t-Test-based copyright claim from $12,000$ to $200$ with zero training cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TraceFL: Interpretability-Driven Debugging in Federated Learning via
  Neuron Provenance <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13632v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13632v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waris Gill, Ali Anwar, Muhammad Ali Gulzar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Federated Learning, clients train models on local data and send updates to
a central server, which aggregates them into a global model using a fusion
algorithm. This collaborative yet privacy-preserving training comes at a cost.
FL developers face significant challenges in attributing global model
predictions to specific clients. Localizing responsible clients is a crucial
step towards (a) excluding clients primarily responsible for incorrect
predictions and (b) encouraging clients who contributed high-quality models to
continue participating in the future. Existing ML debugging approaches are
inherently inapplicable as they are designed for single-model, centralized
training.
  We introduce TraceFL, a fine-grained neuron provenance capturing mechanism
that identifies clients responsible for a global model's prediction by tracking
the flow of information from individual clients to the global model. Since
inference on different inputs activates a different set of neurons of the
global model, TraceFL dynamically quantifies the significance of the global
model's neurons in a given prediction, identifying the most crucial neurons in
the global model. It then maps them to the corresponding neurons in every
participating client to determine each client's contribution, ultimately
localizing the responsible client. We evaluate TraceFL on six datasets,
including two real-world medical imaging datasets and four neural networks,
including advanced models such as GPT. TraceFL achieves 99% accuracy in
localizing the responsible client in FL tasks spanning both image and text
classification tasks. At a time when state-of-the-artML debugging approaches
are mostly domain-specific (e.g., image classification only), TraceFL is the
first technique to enable highly accurate automated reasoning across a wide
range of FL applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2025 IEEE/ACM 47th International Conference on Software
  Engineering (ICSE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation
  for Design Space Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, Kevin Zhou, Jianliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  GraphRAG integrates (knowledge) graphs with large language models (LLMs) to
improve reasoning accuracy and contextual relevance. Despite its promising
applications and strong relevance to multiple research communities, such as
databases and natural language processing, GraphRAG currently lacks modular
workflow analysis, systematic solution frameworks, and insightful empirical
studies. To bridge these gaps, we propose LEGO-GraphRAG, a modular framework
that enables: 1) fine-grained decomposition of the GraphRAG workflow, 2)
systematic classification of existing techniques and implemented GraphRAG
instances, and 3) creation of new GraphRAG instances. Our framework facilitates
comprehensive empirical studies of GraphRAG on large-scale real-world graphs
and diverse query sets, revealing insights into balancing reasoning quality,
runtime efficiency, and token or GPU cost, that are essential for building
advanced GraphRAG systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEAL: Entangled White-box Watermarks on Low-Rank Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giyeong Oh, Saejin Kim, Woohyun Cho, Sangkyu Lee, Jiwan Chung, Dokyung Song, Youngjae Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, LoRA and its variants have become the de facto strategy for
training and sharing task-specific versions of large pretrained models, thanks
to their efficiency and simplicity. However, the issue of copyright protection
for LoRA weights, especially through watermark-based techniques, remains
underexplored. To address this gap, we propose SEAL (SEcure wAtermarking on
LoRA weights), the universal whitebox watermarking for LoRA. SEAL embeds a
secret, non-trainable matrix between trainable LoRA weights, serving as a
passport to claim ownership. SEAL then entangles the passport with the LoRA
weights through training, without extra loss for entanglement, and distributes
the finetuned weights after hiding the passport. When applying SEAL, we
observed no performance degradation across commonsense reasoning,
textual/visual instruction tuning, and text-to-image synthesis tasks. We
demonstrate that SEAL is robust against a variety of known attacks: removal,
obfuscation, and ambiguity attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Author name corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Iterative Enhancement for Improving Learnersourced
  Multiple-Choice Question Explanations with <span class="highlight-title">Large Language Model</span>s <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10444v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10444v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Bao, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Gaël Gendron, Timothy Pistotti, Alice Huang, Paul Denny, Michael Witbrock, Jiamou Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models exhibit superior capabilities in processing and
understanding language, yet their applications in educational contexts remain
underexplored. Learnersourcing enhances learning by engaging students in
creating their own educational content. When learnersourcing multiple-choice
questions, creating explanations for the solution of a question is a crucial
step; it helps other students understand the solution and promotes a deeper
understanding of related concepts. However, it is often difficult for students
to craft effective solution explanations, due to limited subject understanding.
To help scaffold the task of automated explanation generation, we present and
evaluate a framework called "ILearner-LLM", that iteratively enhances the
generated explanations for the given questions with large language models.
Comprising an explanation generation model and an explanation evaluation model,
the framework generates high-quality student-aligned explanations by
iteratively feeding the quality rating score from the evaluation model back
into the instruction prompt of the explanation generation model. Experimental
results demonstrate the effectiveness of our ILearner-LLM on LLaMA2-13B and
GPT-4 to generate higher quality explanations that are closer to those written
by students on five PeerWise datasets. Our findings represent a promising path
to enrich the learnersourcing experience for students and to enhance the
capabilities of large language models for educational applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The short version (v4) has been accepted as a non-archival workshop
  paper at AGI@ICLR 2024, and the full version has been accepted by the main
  track of AAAI/EAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing and Enhancing the Robustness of <span class="highlight-title">Large Language Model</span>s with
  Task Structure Variations for Logical Reasoning <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09430v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09430v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiming Bao, Gael Gendron, Alex Yuxuan Peng, Wanjun Zhong, Neset Tan, Yang Chen, Michael Witbrock, Jiamou Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and
GPT-4, have advanced the performance of AI systems on various natural language
processing tasks to human-like levels. However, their generalisation and
robustness when performing logical reasoning has not been sufficiently
assessed. To comprehensively evaluate this ability, we develop three new
logical reasoning datasets named "ReClor-plus", "LogiQA-plus" and
"LogiQAv2-plus" that extend standard logical reasoning datasets to evaluate the
robustness of the LLM's reasoning. For each, we create three subsets: the first
with randomly shuffled options, the second with the correct choices replaced by
"none of the other options is correct", and the third with a combination of
shuffling and substitution. Experiments on these datasets show that these
simple augmentations greatly hinder the models' performance. Despite their high
performance on the original publicly available datasets, we find that all
models perform poorly on these newly constructed datasets. We also demonstrate
that introducing task variations into the training set can markedly improve the
model's performance on both the original and our developed datasets. Finally,
we show that applying logic-driven data augmentation for fine-tuning and
prompting can enhance generalisation in both discriminative and generative
models, offering a path to improving their robustness for tasks involving
logical reasoning. Source code and data are made publicly available at
https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The short version (v3) was accepted for oral presentation at the
  first LLM@IJCAI 2023 non-archival symposium, and the full version was
  accepted by ICONIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhisheng Tang, Mayank Kejriwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial reasoning, an important faculty of human cognition with many
practical applications, is one of the core commonsense skills that is not
purely language-based and, for satisfying (as opposed to optimal) solutions,
requires some minimum degree of planning. Existing benchmarks of Commonsense
Spatial Reasoning (CSR) tend to evaluate how Large Language Models (LLMs)
interpret text-based spatial $\textit{descriptions}$ rather than directly
evaluate a plan produced by the LLM in response to a $\textit{specific}$
spatial reasoning problem. In this paper, we construct a large-scale benchmark
called GRASP, which consists of 16,000 grid-based environments where the agent
is tasked with an energy collection problem. These environments include 100
grid instances instantiated using each of the 160 different grid settings,
involving five different energy distributions, two modes of agent starting
position, and two distinct obstacle configurations, as well as three kinds of
agent constraints. Using GRASP, we compare classic baseline approaches, such as
random walk and greedy search methods, with advanced LLMs like GPT-3.5-Turbo,
GPT-4o, and GPT-o1-mini. The experimental results indicate that even these
advanced LLMs struggle to consistently achieve satisfactory solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T3: A Novel Zero-shot Transfer Learning Framework Iteratively Training
  on an Assistant Task for a Target Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xindi Tong, Yujin Zhu, Shijian Fan, Liang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long text summarization, gradually being essential for efficiently processing
large volumes of information, stays challenging for Large Language Models
(LLMs) such as GPT and LLaMA families because of the insufficient open-sourced
training datasets and the high requirement of contextual details dealing. To
address the issue, we design a novel zero-shot transfer learning framework,
abbreviated as T3, to iteratively training a baseline LLM on an assistant task
for the target task, where the former should own richer data resources and
share structural or semantic similarity with the latter. In practice, T3 is
approached to deal with the long text summarization task by utilizing question
answering as the assistant task, and further validated its effectiveness on the
BBC summary, NarraSum, FairytaleQA, and NLQuAD datasets, with up to nearly 14%
improvement in ROUGE, 35% improvement in BLEU, and 16% improvement in Factscore
compared to three baseline LLMs, demonstrating its potential for more
assistant-target task combinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can AI-Generated Text be Reliably Detected? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11156v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11156v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) perform impressively well in various
applications. However, the potential for misuse of these models in activities
such as plagiarism, generating fake news, and spamming has raised concern about
their responsible use. Consequently, the reliable detection of AI-generated
text has become a critical area of research. AI text detectors have shown to be
effective under their specific settings. In this paper, we stress-test the
robustness of these AI text detectors in the presence of an attacker. We
introduce recursive paraphrasing attack to stress test a wide range of
detection schemes, including the ones using the watermarking as well as neural
network-based detectors, zero shot classifiers, and retrieval-based detectors.
Our experiments conducted on passages, each approximately 300 tokens long,
reveal the varying sensitivities of these detectors to our attacks. Our
findings indicate that while our recursive paraphrasing method can
significantly reduce detection rates, it only slightly degrades text quality in
many cases, highlighting potential vulnerabilities in current detection systems
in the presence of an attacker. Additionally, we investigate the susceptibility
of watermarked LLMs to spoofing attacks aimed at misclassifying human-written
text as AI-generated. We demonstrate that an attacker can infer hidden AI text
signatures without white-box access to the detection method, potentially
leading to reputational risks for LLM developers. Finally, we provide a
theoretical framework connecting the AUROC of the best possible detector to the
Total Variation distance between human and AI text distributions. This analysis
offers insights into the fundamental challenges of reliable detection as
language models continue to advance. Our code is publicly available at
https://github.com/vinusankars/Reliability-of-AI-text-detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning with Human Judgement: The Role of Pairwise Preference in Large
  Language Model Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16950v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16950v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated promising capabilities as
automatic evaluators in assessing the quality of generated natural language.
However, LLMs still exhibit biases in evaluation and often struggle to generate
coherent evaluations that align with human assessments. In this work, we first
conduct a systematic study of the misalignment between LLM evaluators and human
evaluation, revealing that existing calibration methods aimed at mitigating
biases of LLMs are insufficient for effectively aligning LLM evaluators.
Inspired by the use of preference data in RLHF, we formulate the evaluation as
a ranking problem and introduce Pairwise-preference Search (PAIRS), an
uncertainty-guided search-based rank aggregation method that employs LLMs to
conduct pairwise comparisons locally and efficiently ranks candidate texts
globally. PAIRS achieves state-of-the-art performance on representative
evaluation tasks in long-form generations and demonstrates significant
improvements over direct scoring. Furthermore, we provide insights into the
role of pairwise preference in quantifying the transitivity of LLMs and
demonstrate how PAIRS benefits from calibration using debiased pairwise
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL2KQL: From Natural Language to Kusto Query 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02933v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02933v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinye Tang, Amir H. Abdi, Jeremias Eichelbaum, Mahan Das, Alex Klein, Nihal Irmak Pakis, William Blum, Daniel L Mace, Tanvi Raja, Namrata Padmanabhan, Ye Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data is growing rapidly in volume and complexity. Proficiency in database
query languages is pivotal for crafting effective queries. As coding assistants
become more prevalent, there is significant opportunity to enhance database
query languages. The Kusto Query Language (KQL) is a widely used query language
for large semi-structured data such as logs, telemetries, and time-series for
big data analytics platforms. This paper introduces NL2KQL an innovative
framework that uses large language models (LLMs) to convert natural language
queries (NLQs) to KQL queries. The proposed NL2KQL framework includes several
key components: Schema Refiner which narrows down the schema to its most
pertinent elements; the Few-shot Selector which dynamically selects relevant
examples from a few-shot dataset; and the Query Refiner which repairs syntactic
and semantic errors in KQL queries. Additionally, this study outlines a method
for generating large datasets of synthetic NLQ-KQL pairs which are valid within
a specific database contexts. To validate NL2KQL's performance, we utilize an
array of online (based on query execution) and offline (based on query parsing)
metrics. Through ablation studies, the significance of each framework component
is examined, and the datasets used for benchmarking are made publicly
available. This work is the first of its kind and is compared with available
baselines to demonstrate its effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenge Summary U-MedSAM: Uncertainty-aware MedSAM for Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08881v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08881v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Xiaoyu Liu, Peng Huang, Pu Huang, Shu Hu, Hongtu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical Image Foundation Models have proven to be powerful tools for mask
prediction across various datasets. However, accurately assessing the
uncertainty of their predictions remains a significant challenge. To address
this, we propose a new model, U-MedSAM, which integrates the MedSAM model with
an uncertainty-aware loss function and the Sharpness-Aware Minimization
(SharpMin) optimizer. The uncertainty-aware loss function automatically
combines region-based, distribution-based, and pixel-based loss designs to
enhance segmentation accuracy and robustness. SharpMin improves generalization
by finding flat minima in the loss landscape, thereby reducing overfitting. Our
method was evaluated in the CVPR24 MedSAM on Laptop challenge, where U-MedSAM
demonstrated promising performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2405.17496</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Hallucinations in Practical <span class="highlight-title">Code</span> Generation: Phenomena, Mechanism,
  and Mitigation <span class="chip">ISSTA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Zhang, Yanlin Wang, Chong Wang, Jiachi Chen, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code generation aims to automatically generate code from input requirements,
significantly enhancing development efficiency. Recent large language models
(LLMs) based approaches have shown promising results and revolutionized code
generation task. Despite the promising performance, LLMs often generate
contents with hallucinations, especially for the code generation scenario
requiring the handling of complex contextual dependencies in practical
development process. Although previous study has analyzed hallucinations in
LLM-powered code generation, the study is limited to standalone function
generation. In this paper, we conduct an empirical study to study the
phenomena, mechanism, and mitigation of LLM hallucinations within more
practical and complex development contexts in repository-level generation
scenario. First, we manually examine the code generation results from six
mainstream LLMs to establish a hallucination taxonomy of LLM-generated code.
Next, we elaborate on the phenomenon of hallucinations, analyze their
distribution across different models. We then analyze causes of hallucinations
and identify four potential factors contributing to hallucinations. Finally, we
propose an RAG-based mitigation method, which demonstrates consistent
effectiveness in all studied LLMs. The replication package including code,
data, and experimental results is available at
https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ISSTA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keeping LLMs Aligned After <span class="highlight-title">Fine-tuning</span>: The Crucial Role of <span class="highlight-title">Prompt</span>
  Templates <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public LLMs such as the Llama 2-Chat underwent alignment training and were
considered safe. Recently Qi et al. [2024] reported that even benign
fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the
models. The current paper is about methods and best practices to mitigate such
loss of alignment. We focus on the setting where a public model is fine-tuned
before serving users for specific usage, where the model should improve on the
downstream task while maintaining alignment. Through extensive experiments on
several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct
v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt
templates used during fine-tuning and inference play a crucial role in
preserving safety alignment, and proposes the ``Pure Tuning, Safe Testing''
(PTST) strategy -- fine-tune models without a safety prompt, but include it at
test time. This seemingly counterintuitive strategy incorporates an intended
distribution shift to encourage alignment preservation. Fine-tuning experiments
on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the
rise of unsafe behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Propensity of Generative AI for Producing Harmful
  Disinformation During an Election Cycle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06120v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06120v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik J Schlicht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Artificial Intelligence offers a powerful tool for adversaries who
wish to engage in influence operations, such as the Chinese Spamouflage
operation and the Russian Internet Research Agency effort that both sought to
interfere with recent US election cycles. Therefore, this study seeks to
investigate the propensity of current generative AI models for producing
harmful disinformation during an election cycle. The probability that different
generative AI models produced disinformation when given adversarial prompts was
evaluated, in addition the associated harm. This allows for the expected harm
for each model to be computed and it was discovered that Copilot and Gemini
tied for the overall safest performance by realizing the lowest expected harm,
while GPT-4o produced the greatest rates of harmful disinformation, resulting
in much higher expected harm scores. The impact of disinformation category was
also investigated and Gemini was safest within the political category of
disinformation due to mitigation attempts made by developers during the
election, while Copilot was safest for topics related to health. Moreover,
characteristics of adversarial roles were discovered that led to greater
expected harm across all models. Finally, classification models were developed
that predicted disinformation production based on the conditions considered in
this study, which offers insight into factors important for predicting
disinformation production. Based on all of these insights, recommendations are
provided that seek to mitigate factors that lead to harmful disinformation
being produced by generative AI models. It is hoped that developers will use
these insights to improve future models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLPF: Reinforcement Learning from Prediction Feedback for User
  Summarization with LLMs <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxing Wu, Lin Ning, Luyang Liu, Harrison Lee, Neo Wu, Chao Wang, Sushant Prakash, Shawn O'Banion, Bradley Green, Jun Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-powered personalization agent systems employ Large Language Models (LLMs)
to predict users' behavior from their past activities. However, their
effectiveness often hinges on the ability to effectively leverage extensive,
long user historical data due to its inherent noise and length of such data.
Existing pretrained LLMs may generate summaries that are concise but lack the
necessary context for downstream tasks, hindering their utility in
personalization systems. To address these challenges, we introduce
Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to
generate concise, human-readable user summaries that are optimized for
downstream task performance. By maximizing the usefulness of the generated
summaries, RLPF effectively distills extensive user history data while
preserving essential information for downstream tasks. Our empirical evaluation
demonstrates significant improvements in both extrinsic downstream task utility
and intrinsic summary quality, surpassing baseline methods by up to 22% on
downstream task performance and achieving an up to 84.59% win rate on
Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable
74% reduction in context length while improving performance on 16 out of 19
unseen tasks and/or datasets, showcasing its generalizability. This approach
offers a promising solution for enhancing LLM personalization by effectively
transforming long, noisy user histories into informative and human-readable
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Study of Multi-Agent Deep Reinforcement Learning for Safe
  and Robust Autonomous Highway Ramp Entry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larry Schester, Luis E. Ortiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicles today can drive themselves on highways and driverless robotaxis
operate in major cities, with more sophisticated levels of autonomous driving
expected to be available and become more common in the future. Yet, technically
speaking, so-called "Level 5" (L5) operation, corresponding to full autonomy,
has not been achieved. For that to happen, functions such as fully autonomous
highway ramp entry must be available, and provide provably safe, and reliably
robust behavior to enable full autonomy. We present a systematic study of a
highway ramp function that controls the vehicles forward-moving actions to
minimize collisions with the stream of highway traffic into which a merging
(ego) vehicle enters. We take a game-theoretic multi-agent (MA) approach to
this problem and study the use of controllers based on deep reinforcement
learning (DRL). The virtual environment of the MA DRL uses self-play with
simulated data where merging vehicles safely learn to control longitudinal
position during a taper-type merge. The work presented in this paper extends
existing work by studying the interaction of more than two vehicles (agents)
and does so by systematically expanding the road scene with additional traffic
and ego vehicles. While previous work on the two-vehicle setting established
that collision-free controllers are theoretically impossible in fully
decentralized, non-coordinated environments, we empirically show that
controllers learned using our approach are nearly ideal when measured against
idealized optimal controllers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures; added support ack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Complete Characterization of Learnability for Stochastic Noisy Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steve Hanneke, Kun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the stochastic noisy bandit problem with an unknown reward function
$f^*$ in a known function class $\mathcal{F}$. Formally, a model $M$ maps arms
$\pi$ to a probability distribution $M(\pi)$ of reward. A model class
$\mathcal{M}$ is a collection of models. For each model $M$, define its mean
reward function $f^M(\pi)=\mathbb{E}_{r \sim M(\pi)}[r]$. In the bandit
learning problem, we proceed in rounds, pulling one arm $\pi$ each round and
observing a reward sampled from $M(\pi)$. With knowledge of $\mathcal{M}$,
supposing that the true model $M\in \mathcal{M}$, the objective is to identify
an arm $\hat{\pi}$ of near-maximal mean reward $f^M(\hat{\pi})$ with high
probability in a bounded number of rounds. If this is possible, then the model
class is said to be learnable.
  Importantly, a result of \cite{hanneke2023bandit} shows there exist model
classes for which learnability is undecidable. However, the model class they
consider features deterministic rewards, and they raise the question of whether
learnability is decidable for classes containing sufficiently noisy models. For
the first time, we answer this question in the positive by giving a complete
characterization of learnability for model classes with arbitrary noise. In
addition to that, we also describe the full spectrum of possible optimal query
complexities. Further, we prove adaptivity is sometimes necessary to achieve
the optimal query complexity. Last, we revisit an important complexity measure
for interactive decision making, the Decision-Estimation-Coefficient
\citep{foster2021statistical,foster2023tight}, and propose a new variant of the
DEC which also characterizes learnability in this setting.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Popularity Bias in Third-Party Library Recommendations Using
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Di Sipio, Juri Di Rocco, Davide Di Ruscio, Vladyslav Bulhakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems for software engineering (RSSE) play a crucial role in
automating development tasks by providing relevant suggestions according to the
developer's context. However, they suffer from the so-called popularity bias,
i.e., the phenomenon of recommending popular items that might be irrelevant to
the current task. In particular, the long-tail effect can hamper the system's
performance in terms of accuracy, thus leading to false positives in the
provided recommendations. Foundation models are the most advanced generative
AI-based models that achieve relevant results in several SE tasks.
  This paper aims to investigate the capability of large language models (LLMs)
to address the popularity bias in recommender systems of third-party libraries
(TPLs). We conduct an ablation study experimenting with state-of-the-art
techniques to mitigate the popularity bias, including fine-tuning and
popularity penalty mechanisms. Our findings reveal that the considered LLMs
cannot address the popularity bias in TPL recommenders, even though fine-tuning
and post-processing penalty mechanism contributes to increasing the overall
diversity of the provided recommendations. In addition, we discuss the
limitations of LLMs in this context and suggest potential improvements to
address the popularity bias in TPL recommenders, thus paving the way for
additional experiments in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 1st International Workshop on Fairness in Software
  Systems, co-located with SANER2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grey-Box Fuzzing in Constrained Ultra-Large Systems: Lessons for SE
  Community 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhao Yu, Yanlun Tu, Zhanlei Zhang, Tiehua Zhang, Cheng Xu, Weigang Wu, Hong Jin Kang, Xi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Testing ultra-large microservices-based FinTech systems presents significant
challenges, including restricted access to production environments, complex
dependencies, and stringent security constraints. We propose SandBoxFuzz, a
scalable grey-box fuzzing technique that addresses these limitations by
leveraging aspect-oriented programming and runtime reflection to enable dynamic
specification mining, generating targeted inputs for constrained environments.
SandBoxFuzz also introduces a log-based coverage mechanism, seamlessly
integrated into the build pipeline, eliminating the need for runtime coverage
agents that are often infeasible in industrial settings. SandBoxFuzz has been
successfully deployed to Ant Group's production line and, compared to an
initial solution built on a state-of-the-art fuzzing framework, it demonstrates
superior performance in their microservices software. SandBoxFuzz achieves a
7.5% increase in branch coverage, identifies 1,850 additional exceptions, and
reduces setup time from hours to minutes, highlighting its effectiveness and
practical utility in a real-world industrial environment. By open-sourcing
SandBoxFuzz, we provide a practical and effective tool for researchers and
practitioners to test large-scale microservices systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test Wars: A Comparative Study of SBST, Symbolic Execution, and
  LLM-Based Approaches to Unit Test Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azat Abdullin, Pouria Derakhshanfar, Annibale Panichella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating tests automatically is a key and ongoing area of focus in software
engineering research. The emergence of Large Language Models (LLMs) has opened
up new opportunities, given their ability to perform a wide spectrum of tasks.
However, the effectiveness of LLM-based approaches compared to traditional
techniques such as search-based software testing (SBST) and symbolic execution
remains uncertain. In this paper, we perform an extensive study of automatic
test generation approaches based on three tools: EvoSuite for SBST, Kex for
symbolic execution, and TestSpark for LLM-based test generation. We evaluate
tools performance on the GitBug Java dataset and compare them using various
execution-based and feature-based metrics. Our results show that while
LLM-based test generation is promising, it falls behind traditional methods in
terms of coverage. However, it significantly outperforms them in mutation
scores, suggesting that LLMs provide a deeper semantic understanding of code.
LLM-based approach also performed worse than SBST and symbolic execution-based
approaches w.r.t. fault detection capabilities. Additionally, our feature-based
analysis shows that all tools are primarily affected by the complexity and
internal dependencies of the class under test (CUT), with LLM-based approaches
being especially sensitive to the CUT size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring <span class="highlight-title">Code</span> Comprehension in Scientific Programming: Preliminary
  Insights from Research Scientists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10037v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10037v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alyssia Chen, Carol Wong, Bonita Sharif, Anthony Peruma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific software-defined as computer programs, scripts, or code used in
scientific research, data analysis, modeling, or simulation-has become central
to modern research. However, there is limited research on the readability and
understandability of scientific code, both of which are vital for effective
collaboration and reproducibility in scientific research. This study surveys 57
research scientists from various disciplines to explore their programming
backgrounds, practices, and the challenges they face regarding code
readability. Our findings reveal that most participants learn programming
through self-study or on the-job training, with 57.9% lacking formal
instruction in writing readable code. Scientists mainly use Python and R,
relying on comments and documentation for readability. While most consider code
readability essential for scientific reproducibility, they often face issues
with inadequate documentation and poor naming conventions, with challenges
including cryptic names and inconsistent conventions. Our findings also show
low adoption of code quality tools and a trend towards utilizing large language
models to improve code quality. These findings offer practical insights into
enhancing coding practices and supporting sustainable development in scientific
software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 33rd IEEE/ACM International Conference on Program Comprehension
  (ICPC 2025) - Early Research Achievements Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metamorphic Testing for Smart Contract Validation:A Case Study of
  Ethereum-Based Crowdfunding Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irving Jared Villanueva, Madhusudan Srinivasan, Faqeer Ur Rehman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blockchain smart contracts play a crucial role in automating and securing
agreements in diverse domains such as finance, healthcare, and supply chains.
Despite their critical applications, testing these contracts often receives
less attention than their development, leaving significant risks due to the
immutability of smart contracts post-deployment. A key challenge in the testing
of smart contracts is the oracle problem, where the exact expected outcomes are
not well defined, complicating systematic testing efforts.
  Metamorphic Testing (MT) addresses the oracle problem by using Metamorphic
Relations (MRs) to validate smart contracts. MRs define how output should
change relative to specific input modifications, determining whether the tests
pass or fail. In this work, we apply MT to test an Ethereum-based crowdfunding
smart contract, focusing on core functionalities such as state transitions and
donation tracking.
  We identify a set of MRs tailored for smart contract testing and generate
test cases for these MRs. To assess the effectiveness of this approach, we use
the Vertigo mutation testing tool to create faulty versions of the smart
contract. The experimental results show that our MRs detected 25.65% of the
total mutants generated, with the most effective MRs achieving a mutant-killing
rate of 89%. These results highlight the utility of MT to ensure the
reliability and quality of blockchain-based smart contracts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Mistakes: Understanding Ad-hoc Logs through Analyzing
  Accidental Commits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Hung Chou, Yiyang Min, April Yi Wang, James A. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developers often insert temporary "print" or "log" instructions into their
code to help them better understand runtime behavior, usually when the code is
not behaving as they expected. Despite the fact that such monitoring
instructions, or "ad-hoc logs," are so commonly used by developers, there is
almost no existing literature that studies developers' practices in how they
use them. This paucity of knowledge of the use of these ephemeral logs may be
largely due to the fact that they typically only exist in the developers' local
environments and are removed before they commit their code to their revision
control system. In this work, we overcome this challenge by observing that
developers occasionally mistakenly forget to remove such instructions before
committing, and then they remove them shortly later. Additionally, we further
study such developer logging practices by watching and analyzing live-streamed
coding videos. Through these empirical approaches, we study where, how, and why
developers use ad-hoc logs to better understand their code and its execution.
We collect 27 GB of accidental commits that removed 548,880 ad-hoc logs in
JavaScript from GitHub Archive repositories to provide the first large-scale
dataset and empirical studies on ad-hoc logging practices. Our results reveal
several illuminating findings, including a particular propensity for developers
to use ad-hoc logs in asynchronous and callback functions. Our findings provide
both empirical evidence and a valuable dataset for researchers and tool
developers seeking to enhance ad-hoc logging practices, and potentially deepen
our understanding of developers' practices towards understanding of software's
runtime behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MSR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Effectiveness of LLMs in Automated Self-Admitted
  Technical Debt Repayment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Sadegh Sheikhaei, Yuan Tian, Shaowei Wang, Bowen Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Admitted Technical Debt (SATD), cases where developers intentionally
acknowledge suboptimal solutions in code through comments, poses a significant
challenge to software maintainability. Left unresolved, SATD can degrade code
quality and increase maintenance costs. While Large Language Models (LLMs) have
shown promise in tasks like code generation and program repair, their potential
in automated SATD repayment remains underexplored.
  In this paper, we identify three key challenges in training and evaluating
LLMs for SATD repayment: (1) dataset representativeness and scalability, (2)
removal of irrelevant SATD repayments, and (3) limitations of existing
evaluation metrics. To address the first two dataset-related challenges, we
adopt a language-independent SATD tracing tool and design a 10-step filtering
pipeline to extract SATD repayments from repositories, resulting two
large-scale datasets: 58,722 items for Python and 97,347 items for Java. To
improve evaluation, we introduce two diff-based metrics, BLEU-diff and
CrystalBLEU-diff, which measure code changes rather than whole code.
Additionally, we propose another new metric, LEMOD, which is both interpretable
and informative. Using our new benchmarks and evaluation metrics, we evaluate
two types of automated SATD repayment methods: fine-tuning smaller models, and
prompt engineering with five large-scale models. Our results reveal that
fine-tuned small models achieve comparable Exact Match (EM) scores to
prompt-based approaches but underperform on BLEU-based metrics and LEMOD.
Notably, Gemma-2-9B leads in EM, addressing 10.1% of Python and 8.1% of Java
SATDs, while Llama-3.1-70B-Instruct and GPT-4o-mini excel on BLEU-diff,
CrystalBLEU-diff, and LEMOD metrics. Our work contributes a robust benchmark,
improved evaluation metrics, and a comprehensive evaluation of LLMs, advancing
research on automated SATD repayment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint submitted to ACM Transactions on Software
  Engineering and Methodology (TOSEM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models in Software Development Tasks: An Experimental Analysis
  of Energy and Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Negar Alizadeh, Boris Belchev, Nishant Saurabh, Patricia Kelbert, Fernando Castor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of generative AI-based coding assistants like ChatGPT and Github
Copilot is a reality in contemporary software development. Many of these tools
are provided as remote APIs. Using third-party APIs raises data privacy and
security concerns for client companies, which motivates the use of
locally-deployed language models. In this study, we explore the trade-off
between model accuracy and energy consumption, aiming to provide valuable
insights to help developers make informed decisions when selecting a language
model. We investigate the performance of 18 families of LLMs in typical
software development tasks on two real-world infrastructures, a commodity GPU
and a powerful AI-specific GPU. Given that deploying LLMs locally requires
powerful infrastructure which might not be affordable for everyone, we consider
both full-precision and quantized models. Our findings reveal that employing a
big LLM with a higher energy budget does not always translate to significantly
improved accuracy. Additionally, quantized versions of large models generally
offer better efficiency and accuracy compared to full-precision versions of
medium-sized ones. Apart from that, not a single model is suitable for all
types of software development tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clinicians don't know what explanations they need: A case study on
  eliciting AI software explainability requirements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09592v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09592v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tor Sporsem, Stine Rasdal Finserås, Inga Strümke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyses how software developers elicit explainability
requirements when creating a software application with an AI component, through
a case study using AI in the medical context of predicting cerebral palsy (CP)
risk in infants. Following a small software development team at a Norwegian
hospital, we observe their process of simultaneously developing the AI
application and discovering what explanations clinicians require from the AI
predictions. Since clinicians struggled to articulate their explainability
needs before interacting with the system, an iterative approach proved
effective: the team started with minimal explanations and refined these based
on clinicians' responses during real patient examinations. Our preliminary
findings from the first two iterations show that clinicians valued
"interrogative explanations" - i.e., tools that let them explore and compare
the AI predictions with their own assessments - over detailed technical
explanations of the AI model's inner workings. Based on our analysis, we
suggest that successful explainability requirements emerge through iterative
collaboration between developers and users rather than being fully specified
upfront. To the best of our knowledge, this is the first empirical case study
on eliciting explainability requirements in software engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-user Comprehension of Transfer Risks in Smart Contracts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yustynn Panicker, Ezekiel Soremekun, Sudipta Chattopadhyay, Sumei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart contracts are increasingly used in critical use cases (e.g., financial
transactions). Thus, it is pertinent to ensure that end-users understand the
transfer risks in smart contracts. To address this, we investigate end-user
comprehension of risks in the most popular Ethereum smart contract (i.e., USD
Tether (USDT)) and their prevalence in the top ERC-20 smart contracts. We focus
on five transfer risks with severe impact on transfer outcomes and user
objectives, including users being blacklisted, contract being paused, and
contract being arbitrarily upgraded. Firstly, we conducted a user study
investigating end-user comprehension of smart contract transfer risks with 110
participants and USDT/MetaMask. Secondly, we performed manual and automated
source code analysis of the next top (78) ERC-20 smart contracts (after USDT)
to identify the prevalence of these risks. Results show that end-users do not
comprehend real risks: most (up to 71.8% of) users believe contract upgrade and
blacklisting are highly severe/surprising. More importantly, twice as many
users find it easier to discover successful outcomes than risky outcomes using
the USDT/MetaMask UI flow. These results hold regardless of the self-rated
programming and Web3 proficiency of participants. Furthermore, our source code
analysis demonstrates that the examined risks are prevalent in up to 19.2% of
the top ERC-20 contracts. Additionally, we discovered (three) other risks with
up to 25.6% prevalence in these contracts. This study informs the need to
provide explainable smart contracts, understandable UI and relevant information
for risky outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conditionally Accepted at CHI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Process Models: A Vision for Business Process Management in the
  Age of Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00900v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00900v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timotheus Kampik, Christian Warmuth, Adrian Rebmann, Ron Agam, Lukas N. P. Egger, Andreas Gerber, Johannes Hoffart, Jonas Kolk, Philipp Herzig, Gero Decker, Han van der Aa, Artem Polyvyanyy, Stefanie Rinderle-Ma, Ingo Weber, Matthias Weidlich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The continued success of Large Language Models (LLMs) and other generative
artificial intelligence approaches highlights the advantages that large
information corpora can have over rigidly defined symbolic models, but also
serves as a proof-point of the challenges that purely statistics-based
approaches have in terms of safety and trustworthiness. As a framework for
contextualizing the potential, as well as the limitations of LLMs and other
foundation model-based technologies, we propose the concept of a Large Process
Model (LPM) that combines the correlation power of LLMs with the analytical
precision and reliability of knowledge-based systems and automated reasoning
approaches. LPMs are envisioned to directly utilize the wealth of process
management experience that experts have accumulated, as well as process
performance data of organizations with diverse characteristics, e.g.,\
regarding size, region, or industry. In this vision, the proposed LPM would
allow organizations to receive context-specific (tailored) process and other
business models, analytical deep-dives, and improvement recommendations. As
such, they would allow to substantially decrease the time and effort required
for business transformation, while also allowing for deeper, more impactful,
and more actionable insights than previously possible. We argue that
implementing an LPM is feasible, but also highlight limitations and research
challenges that need to be solved to implement particular aspects of the LPM
vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TraceFL: Interpretability-Driven Debugging in Federated Learning via
  Neuron Provenance <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13632v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13632v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waris Gill, Ali Anwar, Muhammad Ali Gulzar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Federated Learning, clients train models on local data and send updates to
a central server, which aggregates them into a global model using a fusion
algorithm. This collaborative yet privacy-preserving training comes at a cost.
FL developers face significant challenges in attributing global model
predictions to specific clients. Localizing responsible clients is a crucial
step towards (a) excluding clients primarily responsible for incorrect
predictions and (b) encouraging clients who contributed high-quality models to
continue participating in the future. Existing ML debugging approaches are
inherently inapplicable as they are designed for single-model, centralized
training.
  We introduce TraceFL, a fine-grained neuron provenance capturing mechanism
that identifies clients responsible for a global model's prediction by tracking
the flow of information from individual clients to the global model. Since
inference on different inputs activates a different set of neurons of the
global model, TraceFL dynamically quantifies the significance of the global
model's neurons in a given prediction, identifying the most crucial neurons in
the global model. It then maps them to the corresponding neurons in every
participating client to determine each client's contribution, ultimately
localizing the responsible client. We evaluate TraceFL on six datasets,
including two real-world medical imaging datasets and four neural networks,
including advanced models such as GPT. TraceFL achieves 99% accuracy in
localizing the responsible client in FL tasks spanning both image and text
classification tasks. At a time when state-of-the-artML debugging approaches
are mostly domain-specific (e.g., image classification only), TraceFL is the
first technique to enable highly accurate automated reasoning across a wide
range of FL applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2025 IEEE/ACM 47th International Conference on Software
  Engineering (ICSE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agile System Development Lifecycle for AI Systems: Decision Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asif Q. Gill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agile system development life cycle (SDLC) focuses on typical functional and
non-functional system requirements for developing traditional software systems.
However, Artificial Intelligent (AI) systems are different in nature and have
distinct attributes such as (1) autonomy, (2) adaptiveness, (3) content
generation, (4) decision-making, (5) predictability and (6) recommendation.
Agile SDLC needs to be enhanced to support the AI system development and
ongoing post-deployment adaptation. The challenge is: how can agile SDLC be
enhanced to support AI systems? The scope of this paper is limited to AI system
enabled decision automation. Thus, this paper proposes the use of decision
science to enhance the agile SDLC to support the AI system development.
Decision science is the study of decision-making, which seems useful to
identify, analyse and describe decisions and their architecture subject to
automation via AI systems. Specifically, this paper discusses the decision
architecture in detail within the overall context of agile SDLC for AI systems.
The application of the proposed approach is demonstrated with the help of an
example scenario of insurance claim processing. This initial work indicated the
usability of a decision science to enhancing the agile SDLC for designing and
implementing the AI systems for decision-automation. This work provides an
initial foundation for further work in this new area of decision architecture
and agile SDLC for AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11, 4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM Hallucinations in Practical <span class="highlight-title">Code</span> Generation: Phenomena, Mechanism,
  and Mitigation <span class="chip">ISSTA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Zhang, Yanlin Wang, Chong Wang, Jiachi Chen, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code generation aims to automatically generate code from input requirements,
significantly enhancing development efficiency. Recent large language models
(LLMs) based approaches have shown promising results and revolutionized code
generation task. Despite the promising performance, LLMs often generate
contents with hallucinations, especially for the code generation scenario
requiring the handling of complex contextual dependencies in practical
development process. Although previous study has analyzed hallucinations in
LLM-powered code generation, the study is limited to standalone function
generation. In this paper, we conduct an empirical study to study the
phenomena, mechanism, and mitigation of LLM hallucinations within more
practical and complex development contexts in repository-level generation
scenario. First, we manually examine the code generation results from six
mainstream LLMs to establish a hallucination taxonomy of LLM-generated code.
Next, we elaborate on the phenomenon of hallucinations, analyze their
distribution across different models. We then analyze causes of hallucinations
and identify four potential factors contributing to hallucinations. Finally, we
propose an RAG-based mitigation method, which demonstrates consistent
effectiveness in all studied LLMs. The replication package including code,
data, and experimental results is available at
https://github.com/DeepSoftwareAnalytics/LLMCodingHallucination
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ISSTA 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">96</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaceXBench: Evaluating Multimodal LLMs on Face Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartik Narayan, Vibashan VS, Vishal M. Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) demonstrate impressive
problem-solving abilities across a wide range of tasks and domains. However,
their capacity for face understanding has not been systematically studied. To
address this gap, we introduce FaceXBench, a comprehensive benchmark designed
to evaluate MLLMs on complex face understanding tasks. FaceXBench includes
5,000 multimodal multiple-choice questions derived from 25 public datasets and
a newly created dataset, FaceXAPI. These questions cover 14 tasks across 6
broad categories, assessing MLLMs' face understanding abilities in bias and
fairness, face authentication, recognition, analysis, localization and tool
retrieval. Using FaceXBench, we conduct an extensive evaluation of 26
open-source MLLMs alongside 2 proprietary models, revealing the unique
challenges in complex face understanding tasks. We analyze the models across
three evaluation settings: zero-shot, in-context task description, and
chain-of-thought prompting. Our detailed analysis reveals that current MLLMs,
including advanced models like GPT-4o, and GeminiPro 1.5, show significant room
for improvement. We believe FaceXBench will be a crucial resource for
developing MLLMs equipped to perform sophisticated face understanding. Code:
https://github.com/Kartik-3004/facexbench
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://kartik-3004.github.io/facexbench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Monocular Scene Flow Estimation in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqing Liang, Abhishek Badki, Hang Su, James Tompkin, Orazio Gallo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large models have shown generalization across datasets for many low-level
vision tasks, like depth estimation, but no such general models exist for scene
flow. Even though scene flow has wide potential use, it is not used in practice
because current predictive models do not generalize well. We identify three key
challenges and propose solutions for each.First, we create a method that
jointly estimates geometry and motion for accurate prediction. Second, we
alleviate scene flow data scarcity with a data recipe that affords us 1M
annotated training samples across diverse synthetic scenes. Third, we evaluate
different parameterizations for scene flow prediction and adopt a natural and
effective parameterization. Our resulting model outperforms existing methods as
well as baselines built on large-scale models in terms of 3D end-point error,
and shows zero-shot generalization to the casually captured videos from DAVIS
and the robotic manipulation scenes from RoboTAP. Overall, our approach makes
scene flow prediction more practical in-the-wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://research.nvidia.com/labs/zero_msf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3rd Workshop on Maritime Computer Vision (MaCVi) 2025: Challenge Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Kiefer, Lojze Žust, Jon Muhovič, Matej Kristan, Janez Perš, Matija Teršek, Uma Mudenagudi Chaitra Desai, Arnold Wiliem, Marten Kreis, Nikhil Akalwadi, Yitong Quan, Zhiqiang Zhong, Zhe Zhang, Sujie Liu, Xuran Chen, Yang Yang, Matej Fabijanić, Fausto Ferreira, Seongju Lee, Junseok Lee, Kyoobin Lee, Shanliang Yao, Runwei Guan, Xiaoyu Huang, Yi Ni, Himanshu Kumar, Yuan Feng, Yi-Ching Cheng, Tzu-Yu Lin, Chia-Ming Lee, Chih-Chung Hsu, Jannik Sheikh, Andreas Michel, Wolfgang Gross, Martin Weinmann, Josip Šarić, Yipeng Lin, Xiang Yang, Nan Jiang, Yutang Lu, Fei Feng, Ali Awad, Evan Lucas, Ashraf Saleem, Ching-Heng Cheng, Yu-Fan Lin, Tzu-Yu Lin, Chih-Chung Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 3rd Workshop on Maritime Computer Vision (MaCVi) 2025 addresses maritime
computer vision for Unmanned Surface Vehicles (USV) and underwater. This report
offers a comprehensive overview of the findings from the challenges. We provide
both statistical and qualitative analyses, evaluating trends from over 700
submissions. All datasets, evaluation code, and the leaderboard are available
to the public at https://macvi.org/workshop/macvi25.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Part of the MaCVi 2025 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffStereo: High-Frequency Aware Diffusion Model for Stereo Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyun Cao, Yuan Shi, Bin Xia, Xiaoyu Jin, Wenming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) have achieved promising performance in image
restoration but haven't been explored for stereo images. The application of DM
in stereo image restoration is confronted with a series of challenges. The need
to reconstruct two images exacerbates DM's computational cost. Additionally,
existing latent DMs usually focus on semantic information and remove
high-frequency details as redundancy during latent compression, which is
precisely what matters for image restoration. To address the above problems, we
propose a high-frequency aware diffusion model, DiffStereo for stereo image
restoration as the first attempt at DM in this domain. Specifically, DiffStereo
first learns latent high-frequency representations (LHFR) of HQ images. DM is
then trained in the learned space to estimate LHFR for stereo images, which are
fused into a transformer-based stereo image restoration network providing
beneficial high-frequency information of corresponding HQ images. The
resolution of LHFR is kept the same as input images, which preserves the
inherent texture from distortion. And the compression in channels alleviates
the computational burden of DM. Furthermore, we devise a position encoding
scheme when integrating the LHFR into the restoration network, enabling
distinctive guidance in different depths of the restoration network.
Comprehensive experiments verify that by combining generative DM and
transformer, DiffStereo achieves both higher reconstruction accuracy and better
perceptual quality on stereo super-resolution, deblurring, and low-light
enhancement compared with state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Fashion Products Performance Forecasting: A <span class="highlight-title">Survey</span> on Evolutions,
  Models and Emerging Trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Avogaro, Luigi Capogrosso, Andrea Toaiari, Franco Fummi, Marco Cristani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fast fashion industry's insatiable demand for new styles and rapid
production cycles has led to a significant environmental burden.
Overproduction, excessive waste, and harmful chemicals have contributed to the
negative environmental impact of the industry. To mitigate these issues, a
paradigm shift that prioritizes sustainability and efficiency is urgently
needed. Integrating learning-based predictive analytics into the fashion
industry represents a significant opportunity to address environmental
challenges and drive sustainable practices. By forecasting fashion trends and
optimizing production, brands can reduce their ecological footprint while
remaining competitive in a rapidly changing market. However, one of the key
challenges in forecasting fashion sales is the dynamic nature of consumer
preferences. Fashion is acyclical, with trends constantly evolving and
resurfacing. In addition, cultural changes and unexpected events can disrupt
established patterns. This problem is also known as New Fashion Products
Performance Forecasting (NFPPF), and it has recently gained more and more
interest in the global research landscape. Given its multidisciplinary nature,
the field of NFPPF has been approached from many different angles. This
comprehensive survey wishes to provide an up-to-date overview that focuses on
learning-based NFPPF strategies. The survey is based on the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow,
allowing for a systematic and complete literature review. In particular, we
propose the first taxonomy that covers the learning panorama for NFPPF,
examining in detail the different methodologies used to increase the amount of
multimodal information, as well as the state-of-the-art available datasets.
Finally, we discuss the challenges and future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Springer Nature Computer Science journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiMix: Reducing Computational Complexity in Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuange Zhang, Dengjie Li, Bo Liu, Zenghao Bao, Yao Zhou, Baisong Yang, Zhongying Liu, Yujie Zhong, Zheng Zhao, Tongtong Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benefiting from recent advancements in large language models and modality
alignment techniques, existing Large Vision-Language Models(LVLMs) have
achieved prominent performance across a wide range of scenarios. However, the
excessive computational complexity limits the widespread use of these models in
practical applications. We argue that one main bottleneck in computational
complexity is caused by the involvement of redundant vision sequences in model
computation. This is inspired by a reassessment of the efficiency of vision and
language information transmission in the language decoder of LVLMs. Then, we
propose a novel hierarchical vision-language interaction mechanism called
Hierarchical Vision injection for Mixture Attention (HiMix). In HiMix, only the
language sequence undergoes full forward propagation, while the vision sequence
interacts with the language at specific stages within each language decoder
layer. It is striking that our approach significantly reduces computational
complexity with minimal performance loss. Specifically, HiMix achieves a 10x
reduction in the computational cost of the language decoder across multiple
LVLM models while maintaining comparable performance. This highlights the
advantages of our method, and we hope our research brings new perspectives to
the field of vision-language understanding. Project Page:
https://xuange923.github.io/HiMix
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSTAR: Gaussian Surface Tracking and Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwei Zheng, Lixin Xue, Juan Zarate, Jie Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting techniques have enabled efficient photo-realistic
rendering of static scenes. Recent works have extended these approaches to
support surface reconstruction and tracking. However, tracking dynamic surfaces
with 3D Gaussians remains challenging due to complex topology changes, such as
surfaces appearing, disappearing, or splitting. To address these challenges, we
propose GSTAR, a novel method that achieves photo-realistic rendering, accurate
surface reconstruction, and reliable 3D tracking for general dynamic scenes
with changing topology. Given multi-view captures as input, GSTAR binds
Gaussians to mesh faces to represent dynamic objects. For surfaces with
consistent topology, GSTAR maintains the mesh topology and tracks the meshes
using Gaussians. In regions where topology changes, GSTAR adaptively unbinds
Gaussians from the mesh, enabling accurate registration and the generation of
new surfaces based on these optimized Gaussians. Additionally, we introduce a
surface-based scene flow method that provides robust initialization for
tracking between frames. Experiments demonstrate that our method effectively
tracks and reconstructs dynamic surfaces, enabling a range of applications. Our
project page with the code release is available at
https://chengwei-zheng.github.io/GSTAR/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MutualForce: Mutual-Aware Enhancement for 4D Radar-LiDAR 3D Object
  Detection <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyuan Peng, Huawei Sun, Kay Bierzynski, Anton Fischbacher, Lorenzo Servadei, Robert Wille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radar and LiDAR have been widely used in autonomous driving as LiDAR provides
rich structure information, and radar demonstrates high robustness under
adverse weather. Recent studies highlight the effectiveness of fusing radar and
LiDAR point clouds. However, challenges remain due to the modality misalignment
and information loss during feature extractions. To address these issues, we
propose a 4D radar-LiDAR framework to mutually enhance their representations.
Initially, the indicative features from radar are utilized to guide both radar
and LiDAR geometric feature learning. Subsequently, to mitigate their sparsity
gap, the shape information from LiDAR is used to enrich radar BEV features.
Extensive experiments on the View-of-Delft (VoD) dataset demonstrate our
approach's superiority over existing methods, achieving the highest mAP of
71.76% across the entire area and 86.36\% within the driving corridor.
Especially for cars, we improve the AP by 4.17% and 4.20% due to the strong
indicative features and symmetric shapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Egoistic Rigid Body Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niclas Führling, Giuseppe Thadeu Freitas de Abreu, David González G., Osvaldo Gonsa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a robust and self-reliant (or "egoistic") variation of the rigid
body localization (RBL) problem, in which a primary rigid body seeks to
estimate the pose (i.e., location and orientation) of another rigid body (or
"target"), relative to its own, without the assistance of external
infrastructure, without prior knowledge of the shape of the target, and taking
into account the possibility that the available observations are incomplete.
Three complementary contributions are then offered for such a scenario. The
first is a method to estimate the translation vector between the center point
of both rigid bodies, which unlike existing techniques does not require that
both objects have the same shape or even the same number of landmark points.
This technique is shown to significantly outperform the state-of-the-art (SotA)
under complete information, but to be sensitive to data erasures, even when
enhanced by matrix completion methods. The second contribution, designed to
offer improved performance in the presence of incomplete information, offers a
robust alternative to the latter, at the expense of a slight relative loss
under complete information. Finally, the third contribution is a scheme for the
estimation of the rotation matrix describing the relative orientation of the
target rigid body with respect to the primary. Comparisons of the proposed
schemes and SotA techniques demonstrate the advantage of the contributed
methods in terms of root mean square error (RMSE) performance under fully
complete information and incomplete conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disharmony: Forensics using Reverse Lighting Harmonization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Wootaek Shin, Jack Sampson, Vijaykrishnan Narayanan, Andres Marquez, Mahantesh Halappanavar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content generation and manipulation approaches based on deep learning methods
have seen significant advancements, leading to an increased need for techniques
to detect whether an image has been generated or edited. Another area of
research focuses on the insertion and harmonization of objects within images.
In this study, we explore the potential of using harmonization data in
conjunction with a segmentation model to enhance the detection of edited image
regions. These edits can be either manually crafted or generated using deep
learning methods. Our findings demonstrate that this approach can effectively
identify such edits. Existing forensic models often overlook the detection of
harmonized objects in relation to the background, but our proposed Disharmony
Network addresses this gap. By utilizing an aggregated dataset of harmonization
techniques, our model outperforms existing forensic networks in identifying
harmonized objects integrated into their backgrounds, and shows potential for
detecting various forms of edits, including virtual try-on tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hypercone Assisted Contour Generation for Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annita Vapsi, Andrés Muñoz, Nancy Thomas, Keshav Ramani, Daniel Borrajo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in the field of out-of-distribution (OOD) detection have
placed great emphasis on learning better representations suited to this task.
While there are distance-based approaches, distributional awareness has seldom
been exploited for better performance. We present HAC$_k$-OOD, a novel OOD
detection method that makes no distributional assumption about the data, but
automatically adapts to its distribution. Specifically, HAC$_k$-OOD constructs
a set of hypercones by maximizing the angular distance to neighbors in a given
data-point's vicinity to approximate the contour within which in-distribution
(ID) data-points lie. Experimental results show state-of-the-art FPR@95 and
AUROC performance on Near-OOD detection and on Far-OOD detection on the
challenging CIFAR-100 benchmark without explicitly training for OOD
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Clustering for Efficient Phenotype Segmentation of UAV
  Hyperspectral Data <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ciem Cornelissen, Sam Leroux, Pieter Simoens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs) combined with Hyperspectral imaging (HSI)
offer potential for environmental and agricultural applications by capturing
detailed spectral information that enables the prediction of invisible features
like biochemical leaf properties. However, the data-intensive nature of HSI
poses challenges for remote devices, which have limited computational resources
and storage. This paper introduces an Online Hyperspectral Simple Linear
Iterative Clustering algorithm (OHSLIC) framework for real-time tree phenotype
segmentation. OHSLIC reduces inherent noise and computational demands through
adaptive incremental clustering and a lightweight neural network, which
phenotypes trees using leaf contents such as chlorophyll, carotenoids, and
anthocyanins. A hyperspectral dataset is created using a custom simulator that
incorporates realistic leaf parameters, and light interactions. Results
demonstrate that OHSLIC achieves superior regression accuracy and segmentation
performance compared to pixel- or window-based methods while significantly
reducing inference time. The method`s adaptive clustering enables dynamic
trade-offs between computational efficiency and accuracy, paving the way for
scalable edge-device deployment in HSI applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted WACV 2025 GeoCV workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CSHNet: A Novel Information Asymmetric Image Translation Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Yang, Haoyuan Shi, Zihan Wang, Nannan Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements in cross-domain image translation, challenges persist in
asymmetric tasks such as SAR-to-Optical and Sketch-to-Instance conversions,
which involve transforming data from a less detailed domain into one with
richer content. Traditional CNN-based methods are effective at capturing fine
details but struggle with global structure, leading to unwanted merging of
image regions. To address this, we propose the CNN-Swin Hybrid Network
(CSHNet), which combines two key modules: Swin Embedded CNN (SEC) and CNN
Embedded Swin (CES), forming the SEC-CES-Bottleneck (SCB). SEC leverages CNN's
detailed feature extraction while integrating the Swin Transformer's structural
bias. CES, in turn, preserves the Swin Transformer's global integrity,
compensating for CNN's lack of focus on structure. Additionally, CSHNet
includes two components designed to enhance cross-domain information retention:
the Interactive Guided Connection (IGC), which enables dynamic information
exchange between SEC and CES, and Adaptive Edge Perception Loss (AEPL), which
maintains structural boundaries during translation. Experimental results show
that CSHNet outperforms existing methods in both visual quality and performance
metrics across scene-level and instance-level datasets. Our code is available
at: https://github.com/XduShi/CSHNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-guided Deep Multi-View Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinrong Cui, Xiaohuang Wu, Haitao Zhang, Chongjie Dong, Jie Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep multi-view clustering seeks to utilize the abundant information from
multiple views to improve clustering performance. However, most of the existing
clustering methods often neglect to fully mine multi-view structural
information and fail to explore the distribution of multi-view data, limiting
clustering performance. To address these limitations, we propose a
structure-guided deep multi-view clustering model. Specifically, we introduce a
positive sample selection strategy based on neighborhood relationships, coupled
with a corresponding loss function. This strategy constructs multi-view nearest
neighbor graphs to dynamically redefine positive sample pairs, enabling the
mining of local structural information within multi-view data and enhancing the
reliability of positive sample selection. Additionally, we introduce a Gaussian
distribution model to uncover latent structural information and introduce a
loss function to reduce discrepancies between view embeddings. These two
strategies explore multi-view structural information and data distribution from
different perspectives, enhancing consistency across views and increasing
intra-cluster compactness. Experimental evaluations demonstrate the efficacy of
our method, showing significant improvements in clustering performance on
multiple benchmark datasets compared to state-of-the-art multi-view clustering
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vision-Language Framework for Multispectral Scene Representation Using
  Language-Grounded Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enes Karanfil, Nevrez Imamoglu, Erkut Erdem, Aykut Erdem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene understanding in remote sensing often faces challenges in generating
accurate representations for complex environments such as various land use
areas or coastal regions, which may also include snow, clouds, or haze. To
address this, we present a vision-language framework named Spectral LLaVA,
which integrates multispectral data with vision-language alignment techniques
to enhance scene representation and description. Using the BigEarthNet v2
dataset from Sentinel-2, we establish a baseline with RGB-based scene
descriptions and further demonstrate substantial improvements through the
incorporation of multispectral information. Our framework optimizes a
lightweight linear projection layer for alignment while keeping the vision
backbone of SpectralGPT frozen. Our experiments encompass scene classification
using linear probing and language modeling for jointly performing scene
classification and description generation. Our results highlight Spectral
LLaVA's ability to produce detailed and accurate descriptions, particularly for
scenarios where RGB data alone proves inadequate, while also enhancing
classification performance by refining SpectralGPT features into semantically
meaningful representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACE: Anatomically Consistent Embeddings in Composition and Decomposition <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Zhou, Haozhe Luo, Mohammad Reza Hosseinzadeh Taher, Jiaxuan Pang, Xiaowei Ding, Michael Gotway, Jianming Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images acquired from standardized protocols show consistent
macroscopic or microscopic anatomical structures, and these structures consist
of composable/decomposable organs and tissues, but existing self-supervised
learning (SSL) methods do not appreciate such composable/decomposable structure
attributes inherent to medical images. To overcome this limitation, this paper
introduces a novel SSL approach called ACE to learn anatomically consistent
embedding via composition and decomposition with two key branches: (1) global
consistency, capturing discriminative macro-structures via extracting global
features; (2) local consistency, learning fine-grained anatomical details from
composable/decomposable patch features via corresponding matrix matching.
Experimental results across 6 datasets 2 backbones, evaluated in few-shot
learning, fine-tuning, and property analysis, show ACE's superior robustness,
transferability, and clinical potential. The innovations of our ACE lie in
grid-wise image cropping, leveraging the intrinsic properties of
compositionality and decompositionality of medical images, bridging the
semantic gap from high-level pathologies to low-level tissue anomalies, and
providing a new SSL method for medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatio-temporal Graph Learning on Adaptive Mined Key Frames for
  High-performance Multi-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Futian Wang, Fengxiang Liu, Xiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of multi-object tracking, the challenge of accurately capturing
the spatial and temporal relationships between objects in video sequences
remains a significant hurdle. This is further complicated by frequent
occurrences of mutual occlusions among objects, which can lead to tracking
errors and reduced performance in existing methods. Motivated by these
challenges, we propose a novel adaptive key frame mining strategy that
addresses the limitations of current tracking approaches. Specifically, we
introduce a Key Frame Extraction (KFE) module that leverages reinforcement
learning to adaptively segment videos, thereby guiding the tracker to exploit
the intrinsic logic of the video content. This approach allows us to capture
structured spatial relationships between different objects as well as the
temporal relationships of objects across frames. To tackle the issue of object
occlusions, we have developed an Intra-Frame Feature Fusion (IFF) module.
Unlike traditional graph-based methods that primarily focus on inter-frame
feature fusion, our IFF module uses a Graph Convolutional Network (GCN) to
facilitate information exchange between the target and surrounding objects
within a frame. This innovation significantly enhances target
distinguishability and mitigates tracking loss and appearance similarity due to
occlusions. By combining the strengths of both long and short trajectories and
considering the spatial relationships between objects, our proposed tracker
achieves impressive results on the MOT17 dataset, i.e., 68.6 HOTA, 81.0 IDF1,
66.6 AssA, and 893 IDS, proving its effectiveness and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FECT: Classification of Breast Cancer Pathological Images Based on
  Fusion Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Hao, Yiqing Liu, Siqi Zeng, Yonghong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is one of the most common cancers among women globally, with
early diagnosis and precise classification being crucial. With the advancement
of deep learning and computer vision, the automatic classification of breast
tissue pathological images has emerged as a research focus. Existing methods
typically rely on singular cell or tissue features and lack design
considerations for morphological characteristics of challenging-to-classify
categories, resulting in suboptimal classification performance. To address
these problems, we proposes a novel breast cancer tissue classification model
that Fused features of Edges, Cells, and Tissues (FECT), employing the
ResMTUNet and an attention-based aggregator to extract and aggregate these
features. Extensive testing on the BRACS dataset demonstrates that our model
surpasses current advanced methods in terms of classification accuracy and F1
scores. Moreover, due to its feature fusion that aligns with the diagnostic
approach of pathologists, our model exhibits interpretability and holds promise
for significant roles in future clinical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffVSR: Enhancing Real-World Video Super-Resolution with Diffusion
  Models for Advanced Visual Quality and Temporal Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohui Li, Yihao Liu, Shuo Cao, Ziyan Chen, Shaobin Zhuang, Xiangyu Chen, Yinan He, Yi Wang, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated exceptional capabilities in image
generation and restoration, yet their application to video super-resolution
faces significant challenges in maintaining both high fidelity and temporal
consistency. We present DiffVSR, a diffusion-based framework for real-world
video super-resolution that effectively addresses these challenges through key
innovations. For intra-sequence coherence, we develop a multi-scale temporal
attention module and temporal-enhanced VAE decoder that capture fine-grained
motion details. To ensure inter-sequence stability, we introduce a noise
rescheduling mechanism with an interweaved latent transition approach, which
enhances temporal consistency without additional training overhead. We propose
a progressive learning strategy that transitions from simple to complex
degradations, enabling robust optimization despite limited high-quality video
data. Extensive experiments demonstrate that DiffVSR delivers superior results
in both visual quality and temporal consistency, setting a new performance
standard in real-world video super-resolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: \url{https://xh9998.github.io/DiffVSR-project/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Actions for Enhanced Embodied Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinliang Zheng, Jianxiong Li, Dongxiu Liu, Yinan Zheng, Zhihao Wang, Zhonghong Ou, Yu Liu, Jingjing Liu, Ya-Qin Zhang, Xianyuan Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training on diverse, internet-scale data is a key factor in the success of
recent large foundation models. Yet, using the same recipe for building
embodied agents has faced noticeable difficulties. Despite the availability of
many crowd-sourced embodied datasets, their action spaces often exhibit
significant heterogeneity due to distinct physical embodiment and control
interfaces for different robots, causing substantial challenges in developing
embodied foundation models using cross-domain data. In this paper, we introduce
UniAct, a new embodied foundation modeling framework operating in a tokenized
Universal Action Space. Our learned universal actions capture the generic
atomic behaviors across diverse robots by exploiting their shared structural
features, and enable enhanced cross-domain data utilization and
cross-embodiment generalizations by eliminating the notorious heterogeneity.
The universal actions can be efficiently translated back to heterogeneous
actionable commands by simply adding embodiment-specific details, from which
fast adaptation to new robots becomes simple and straightforward. Our 0.5B
instantiation of UniAct outperforms 14X larger SOTA embodied foundation models
in extensive evaluations on various real-world and simulation robots,
showcasing exceptional cross-embodiment control and adaptation capability,
highlighting the crucial benefit of adopting universal actions. Project page:
https://github.com/2toinf/UniAct
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jef Jonkers, Luc Duchateau, Glenn Van Wallendael, Sofie Van Hoecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical landmark localization in 2D/3D images is a critical task in
medical imaging. Although many general-purpose tools exist for landmark
localization in classical computer vision tasks, such as pose estimation, they
lack the specialized features and modularity necessary for anatomical landmark
localization applications in the medical domain. Therefore, we introduce
landmarker, a Python package built on PyTorch. The package provides a
comprehensive, flexible toolkit for developing and evaluating landmark
localization algorithms, supporting a range of methodologies, including static
and adaptive heatmap regression. landmarker enhances the accuracy of landmark
identification, streamlines research and development processes, and supports
various image formats and preprocessing pipelines. Its modular design allows
users to customize and extend the toolkit for specific datasets and
applications, accelerating innovation in medical imaging. landmarker addresses
a critical need for precision and customization in landmark localization tasks
not adequately met by existing general-purpose pose estimation tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Classifier Ensemble for Efficient Uncertainty Calibration of Deep Neural
  Networks for Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Schulze, Nikolas Ebert, Laurenz Reichardt, Oliver Wasenmüller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates novel classifier ensemble techniques for uncertainty
calibration applied to various deep neural networks for image classification.
We evaluate both accuracy and calibration metrics, focusing on Expected
Calibration Error (ECE) and Maximum Calibration Error (MCE). Our work compares
different methods for building simple yet efficient classifier ensembles,
including majority voting and several metamodel-based approaches. Our
evaluation reveals that while state-of-the-art deep neural networks for image
classification achieve high accuracy on standard datasets, they frequently
suffer from significant calibration errors. Basic ensemble techniques like
majority voting provide modest improvements, while metamodel-based ensembles
consistently reduce ECE and MCE across all architectures. Notably, the largest
of our compared metamodels demonstrate the most substantial calibration
improvements, with minimal impact on accuracy. Moreover, classifier ensembles
with metamodels outperform traditional model ensembles in calibration
performance, while requiring significantly fewer parameters. In comparison to
traditional post-hoc calibration methods, our approach removes the need for a
separate calibration dataset. These findings underscore the potential of our
proposed metamodel-based classifier ensembles as an efficient and effective
approach to improving model calibration, thereby contributing to more reliable
deep learning systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at International Conference on Computer
  Vision Theory and Applications (VISAPP), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Confident Image Regions for Source-Free Domain-Adaptive
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Lamine Mekhalfi, Davide Boscaini, Fabio Poiesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain-adaptive object detection is an interesting but scarcely
addressed topic. It aims at adapting a source-pretrained detector to a distinct
target domain without resorting to source data during adaptation. So far, there
is no data augmentation scheme tailored to source-free domain-adaptive object
detection. To this end, this paper presents a novel data augmentation approach
that cuts out target image regions where the detector is confident, augments
them along with their respective pseudo-labels, and joins them into a
challenging target image to adapt the detector. As the source data is out of
reach during adaptation, we implement our approach within a teacher-student
learning paradigm to ensure that the model does not collapse during the
adaptation procedure. We evaluated our approach on three adaptation benchmarks
of traffic scenes, scoring new state-of-the-art on two of them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Structure-Informed Machinery Part Segmentation with Foundation
  Models and Graph Neural Networks <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Schwingshackl, Fabio Francisco Oberweger, Markus Murschitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel approach to few-shot semantic segmentation for
machinery with multiple parts that exhibit spatial and hierarchical
relationships. Our method integrates the foundation models CLIPSeg and Segment
Anything Model (SAM) with the interest point detector SuperPoint and a graph
convolutional network (GCN) to accurately segment machinery parts. By providing
1 to 25 annotated samples, our model, evaluated on a purely synthetic dataset
depicting a truck-mounted loading crane, achieves effective segmentation across
various levels of detail. Training times are kept under five minutes on
consumer GPUs. The model demonstrates robust generalization to real data,
achieving a qualitative synthetic-to-real generalization with a $J\&F$ score of
92.2 on real data using 10 synthetic support samples. When benchmarked on the
DAVIS 2017 dataset, it achieves a $J\&F$ score of 71.5 in semi-supervised video
segmentation with three support samples. This method's fast training times and
effective generalization to real data make it a valuable tool for autonomous
systems interacting with machinery and infrastructure, and illustrate the
potential of combined and orchestrated foundation models for few-shot
segmentation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Winter Conference on Applications of Computer Vision
  (WACV) 2025. Code and available at
  https://github.com/AIT-Assistive-Autonomous-Systems/Hopomop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Change Captioning in Remote Sensing: SECOND-CC Dataset and
  MModalCC Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Can Karaca, M. Enes Ozelbas, Saadettin Berber, Orkhan Karimli, Turabi Yildirim, M. Fatih Amasyali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing change captioning (RSICC) aims to describe changes between
bitemporal images in natural language. Existing methods often fail under
challenges like illumination differences, viewpoint changes, blur effects,
leading to inaccuracies, especially in no-change regions. Moreover, the images
acquired at different spatial resolutions and have registration errors tend to
affect the captions. To address these issues, we introduce SECOND-CC, a novel
RSICC dataset featuring high-resolution RGB image pairs, semantic segmentation
maps, and diverse real-world scenarios. SECOND-CC which contains 6,041 pairs of
bitemporal RS images and 30,205 sentences describing the differences between
images. Additionally, we propose MModalCC, a multimodal framework that
integrates semantic and visual data using advanced attention mechanisms,
including Cross-Modal Cross Attention (CMCA) and Multimodal Gated Cross
Attention (MGCA). Detailed ablation studies and attention visualizations
further demonstrate its effectiveness and ability to address RSICC challenges.
Comprehensive experiments show that MModalCC outperforms state-of-the-art RSICC
methods, including RSICCformer, Chg2Cap, and PSNet with +4.6% improvement on
BLEU4 score and +9.6% improvement on CIDEr score. We will make our dataset and
codebase publicly available to facilitate future research at
https://github.com/ChangeCapsInRS/SecondCC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE Transactions on Geoscience
  and Remote Sensing journal for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpatialCoT: Advancing Spatial Reasoning through Coordinate Alignment and
  Chain-of-Thought for Embodied Task Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yaochen Hu, Lingfeng Zhang, Yingxue Zhang, Shuang Wu, Tongtong Cao, Guowei Huang, Guangjian Tian, Xingyue Quan, Jianye Hao, Yuzheng Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial reasoning is an essential problem in embodied AI research. Efforts to
enhance spatial reasoning abilities through supplementary spatial data and
fine-tuning have proven limited and ineffective when addressing complex
embodied tasks, largely due to their dependence on language-based outputs.
While some approaches have introduced a point-based action space to mitigate
this issue, they fall short in managing more intricate tasks within complex
environments. This deficiency arises from their failure to fully exploit the
inherent thinking and reasoning capabilities that are fundamental strengths of
Vision-Language Models (VLMs). To address these limitations, we propose a novel
approach named SpatialCoT, specifically designed to bolster the spatial
reasoning capabilities of VLMs. Our approach comprises two stages: spatial
coordinate bi-directional alignment, which aligns vision-language inputs with
spatial coordinates, and chain-of-thought spatial grounding, which harnesses
the reasoning capabilities of language models for advanced spatial reasoning.
We evaluate SpatialCoT on challenging navigation and manipulation tasks, both
in simulation and real-world settings. Experimental results demonstrate that
our method significantly outperforms previous state-of-the-art approaches in
both tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLIP-PCQA: Exploring Subjective-Aligned Vision-Language Modeling for
  Point Cloud Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Liu, Yujie Zhang, Ziyu Shan, Yiling Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, No-Reference Point Cloud Quality Assessment (NR-PCQA)
research has achieved significant progress. However, existing methods mostly
seek a direct mapping function from visual data to the Mean Opinion Score
(MOS), which is contradictory to the mechanism of practical subjective
evaluation. To address this, we propose a novel language-driven PCQA method
named CLIP-PCQA. Considering that human beings prefer to describe visual
quality using discrete quality descriptions (e.g., "excellent" and "poor")
rather than specific scores, we adopt a retrieval-based mapping strategy to
simulate the process of subjective assessment. More specifically, based on the
philosophy of CLIP, we calculate the cosine similarity between the visual
features and multiple textual features corresponding to different quality
descriptions, in which process an effective contrastive loss and learnable
prompts are introduced to enhance the feature extraction. Meanwhile, given the
personal limitations and bias in subjective experiments, we further covert the
feature similarities into probabilities and consider the Opinion Score
Distribution (OSD) rather than a single MOS as the final target. Experimental
results show that our CLIP-PCQA outperforms other State-Of-The-Art (SOTA)
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FiLo++: Zero-/Few-Shot Anomaly Detection by Fused Fine-Grained
  Descriptions and Deformable Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection methods typically require extensive normal samples from the
target class for training, limiting their applicability in scenarios that
require rapid adaptation, such as cold start. Zero-shot and few-shot anomaly
detection do not require labeled samples from the target class in advance,
making them a promising research direction. Existing zero-shot and few-shot
approaches often leverage powerful multimodal models to detect and localize
anomalies by comparing image-text similarity. However, their handcrafted
generic descriptions fail to capture the diverse range of anomalies that may
emerge in different objects, and simple patch-level image-text matching often
struggles to localize anomalous regions of varying shapes and sizes. To address
these issues, this paper proposes the FiLo++ method, which consists of two key
components. The first component, Fused Fine-Grained Descriptions (FusDes),
utilizes large language models to generate anomaly descriptions for each object
category, combines both fixed and learnable prompt templates and applies a
runtime prompt filtering method, producing more accurate and task-specific
textual descriptions. The second component, Deformable Localization (DefLoc),
integrates the vision foundation model Grounding DINO with position-enhanced
text descriptions and a Multi-scale Deformable Cross-modal Interaction (MDCI)
module, enabling accurate localization of anomalies with various shapes and
sizes. In addition, we design a position-enhanced patch matching approach to
improve few-shot anomaly detection performance. Experiments on multiple
datasets demonstrate that FiLo++ achieves significant performance improvements
compared with existing methods. Code will be available at
https://github.com/CASIA-IVA-Lab/FiLo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-D-Piece: Image Tokenizer Meets Quality-Controllable Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keita Miwa, Kento Sasaki, Hidehisa Arai, Tsubasa Takahashi, Yu Yamaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current image tokenization methods require a large number of tokens to
capture the information contained within images. Although the amount of
information varies across images, most image tokenizers only support
fixed-length tokenization, leading to inefficiency in token allocation. In this
study, we introduce One-D-Piece, a discrete image tokenizer designed for
variable-length tokenization, achieving quality-controllable mechanism. To
enable variable compression rate, we introduce a simple but effective
regularization mechanism named "Tail Token Drop" into discrete one-dimensional
image tokenizers. This method encourages critical information to concentrate at
the head of the token sequence, enabling support of variadic tokenization,
while preserving state-of-the-art reconstruction quality. We evaluate our
tokenizer across multiple reconstruction quality metrics and find that it
delivers significantly better perceptual quality than existing
quality-controllable compression methods, including JPEG and WebP, at smaller
byte sizes. Furthermore, we assess our tokenizer on various downstream computer
vision tasks, including image classification, object detection, semantic
segmentation, and depth estimation, confirming its adaptability to numerous
applications compared to other variable-rate methods. Our approach demonstrates
the versatility of variable-length discrete image tokenization, establishing a
new paradigm in both compression efficiency and reconstruction performance.
Finally, we validate the effectiveness of tail token drop via detailed analysis
of tokenizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our Project Page:
  https://turingmotors.github.io/one-d-piece-tokenizer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LWGANet: A Lightweight Group Attention Backbone for Remote Sensing
  Visual Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Lu, Si-Bao Chen, Chris H. Q. Ding, Jin Tang, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing (RS) visual tasks have gained significant academic and
practical importance. However, they encounter numerous challenges that hinder
effective feature extraction, including the detection and recognition of
multiple objects exhibiting substantial variations in scale within a single
image. While prior dual-branch or multi-branch architectural strategies have
been effective in managing these object variances, they have concurrently
resulted in considerable increases in computational demands and parameter
counts. Consequently, these architectures are rendered less viable for
deployment on resource-constrained devices. Contemporary lightweight backbone
networks, designed primarily for natural images, frequently encounter
difficulties in effectively extracting features from multi-scale objects, which
compromises their efficacy in RS visual tasks. This article introduces LWGANet,
a specialized lightweight backbone network tailored for RS visual tasks,
incorporating a novel lightweight group attention (LWGA) module designed to
address these specific challenges. LWGA module, tailored for RS imagery,
adeptly harnesses redundant features to extract a wide range of spatial
information, from local to global scales, without introducing additional
complexity or computational overhead. This facilitates precise feature
extraction across multiple scales within an efficient framework.LWGANet was
rigorously evaluated across twelve datasets, which span four crucial RS visual
tasks: scene classification, oriented object detection, semantic segmentation,
and change detection. The results confirm LWGANet's widespread applicability
and its ability to maintain an optimal balance between high performance and low
complexity, achieving SOTA results across diverse datasets. LWGANet emerged as
a novel solution for resource-limited scenarios requiring robust RS image
processing capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, Remote sensing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ X-Dyna: Expressive Dynamic Human Image Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, Zeyuan Chen, Shijie Zhou, Linjie Luo, Gordon Wetzstein, Mohammad Soleymani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce X-Dyna, a novel zero-shot, diffusion-based pipeline for
animating a single human image using facial expressions and body movements
derived from a driving video, that generates realistic, context-aware dynamics
for both the subject and the surrounding environment. Building on prior
approaches centered on human pose control, X-Dyna addresses key shortcomings
causing the loss of dynamic details, enhancing the lifelike qualities of human
video animations. At the core of our approach is the Dynamics-Adapter, a
lightweight module that effectively integrates reference appearance context
into the spatial attentions of the diffusion backbone while preserving the
capacity of motion modules in synthesizing fluid and intricate dynamic details.
Beyond body pose control, we connect a local control module with our model to
capture identity-disentangled facial expressions, facilitating accurate
expression transfer for enhanced realism in animated scenes. Together, these
components form a unified framework capable of learning physical human motion
and natural scene dynamics from a diverse blend of human and scene videos.
Comprehensive qualitative and quantitative evaluations demonstrate that X-Dyna
outperforms state-of-the-art methods, creating highly lifelike and expressive
animations. The code is available at https://github.com/bytedance/X-Dyna.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:https://x-dyna.github.io/xdyna.github.io/
  Code:https://github.com/bytedance/X-Dyna</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao He, Jianqiang Ren, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 2D cartoon style is a prominent art form in digital character creation,
particularly popular among younger audiences. While advancements in digital
human technology have spurred extensive research into photorealistic digital
humans and 3D characters, interactive 2D cartoon characters have received
comparatively less attention. Unlike 3D counterparts, which require
sophisticated construction and resource-intensive rendering, Live2D, a
widely-used format for 2D cartoon characters, offers a more efficient
alternative, which allows to animate 2D characters in a manner that simulates
3D movement without the necessity of building a complete 3D model. Furthermore,
Live2D employs lightweight HTML5 (H5) rendering, improving both accessibility
and efficiency. In this technical report, we introduce Textoon, an innovative
method for generating diverse 2D cartoon characters in the Live2D format based
on text descriptions. The Textoon leverages cutting-edge language and vision
models to comprehend textual intentions and generate 2D appearance, capable of
creating a wide variety of stunning and interactive 2D characters within one
minute. The project homepage is https://human3daigc.github.io/Textoon_webpage/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffuEraser: A Diffusion Model for Video Inpainting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowen Li, Haolan Xue, Peiran Ren, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent video inpainting algorithms integrate flow-based pixel propagation
with transformer-based generation to leverage optical flow for restoring
textures and objects using information from neighboring frames, while
completing masked regions through visual Transformers. However, these
approaches often encounter blurring and temporal inconsistencies when dealing
with large masks, highlighting the need for models with enhanced generative
capabilities. Recently, diffusion models have emerged as a prominent technique
in image and video generation due to their impressive performance. In this
paper, we introduce DiffuEraser, a video inpainting model based on stable
diffusion, designed to fill masked regions with greater details and more
coherent structures. We incorporate prior information to provide initialization
and weak conditioning,which helps mitigate noisy artifacts and suppress
hallucinations. Additionally, to improve temporal consistency during
long-sequence inference, we expand the temporal receptive fields of both the
prior model and DiffuEraser, and further enhance consistency by leveraging the
temporal smoothing property of Video Diffusion Models. Experimental results
demonstrate that our proposed method outperforms state-of-the-art techniques in
both content completeness and temporal consistency while maintaining acceptable
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages, 13figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations on Object Attributes using Multiview Images
  and Negative Instructions <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Tan, Yuzhi Li, Shengwei Meng, Xiang Yuan, Weiping Li, Tong Mo, Bingce Wang, Xu Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current popular Large Vision-Language Models (LVLMs) are suffering from
Hallucinations on Object Attributes (HoOA), leading to incorrect determination
of fine-grained attributes in the input images. Leveraging significant
advancements in 3D generation from a single image, this paper proposes a novel
method to mitigate HoOA in LVLMs. This method utilizes multiview images sampled
from generated 3D representations as visual prompts for LVLMs, thereby
providing more visual information from other viewpoints. Furthermore, we
observe the input order of multiple multiview images significantly affects the
performance of LVLMs. Consequently, we have devised Multiview Image Augmented
VLM (MIAVLM), incorporating a Multiview Attributes Perceiver (MAP) submodule
capable of simultaneously eliminating the influence of input image order and
aligning visual information from multiview images with Large Language Models
(LLMs). Besides, we designed and employed negative instructions to mitigate
LVLMs' bias towards ``Yes" responses. Comprehensive experiments demonstrate the
effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Early Alzheimer Disease Detection with MRI Scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Rafsan, Tamer Oraby, Upal Roy, Sanjeev Kumar, Hansapani Rodrigo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's Disease is a neurodegenerative condition characterized by
dementia and impairment in neurological function. The study primarily focuses
on the individuals above age 40, affecting their memory, behavior, and
cognitive processes of the brain. Alzheimer's disease requires diagnosis by a
detailed assessment of MRI scans and neuropsychological tests of the patients.
This project compares existing deep learning models in the pursuit of enhancing
the accuracy and efficiency of AD diagnosis, specifically focusing on the
Convolutional Neural Network, Bayesian Convolutional Neural Network, and the
U-net model with the Open Access Series of Imaging Studies brain MRI dataset.
Besides, to ensure robustness and reliability in the model evaluations, we
address the challenge of imbalance in data. We then perform rigorous evaluation
to determine strengths and weaknesses for each model by considering
sensitivity, specificity, and computational efficiency. This comparative
analysis would shed light on the future role of AI in revolutionizing AD
diagnostics but also paved ways for future innovation in medical imaging and
the management of neurodegenerative diseases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Attention Networks for Enhanced Segmentation and Depth
  Estimation of Subsurface Defects in Pulse Thermography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Salah, Naoufel Werghi, Davor Svetinovic, Yusra Abdulrahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-driven pulse thermography (PT) has become a crucial tool in
non-destructive testing (NDT), enabling automatic detection of hidden anomalies
in various industrial components. Current state-of-the-art techniques feed
segmentation and depth estimation networks compressed PT sequences using either
Principal Component Analysis (PCA) or Thermographic Signal Reconstruction
(TSR). However, treating these two modalities independently constrains the
performance of PT inspection models as these representations possess
complementary semantic features. To address this limitation, this work proposes
PT-Fusion, a multi-modal attention-based fusion network that fuses both PCA and
TSR modalities for defect segmentation and depth estimation of subsurface
defects in PT setups. PT-Fusion introduces novel feature fusion modules,
Encoder Attention Fusion Gate (EAFG) and Attention Enhanced Decoding Block
(AEDB), to fuse PCA and TSR features for enhanced segmentation and depth
estimation of subsurface defects. In addition, a novel data augmentation
technique is proposed based on random data sampling from thermographic
sequences to alleviate the scarcity of PT datasets. The proposed method is
benchmarked against state-of-the-art PT inspection models, including U-Net,
attention U-Net, and 3D-CNN on the Universit\'e Laval IRT-PVC dataset. The
results demonstrate that PT-Fusion outperforms the aforementioned models in
defect segmentation and depth estimation accuracies with a margin of 10%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pulse thermography, infrared thermography, defect segmentation,
  multi-modal networks, attention mechanism</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RichSpace: Enriching Text-to-Video <span class="highlight-title">Prompt</span> Space via Text Embedding
  Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuefan Cao, Chengyue Gong, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video generation models have made impressive progress, but they still
struggle with generating videos with complex features. This limitation often
arises from the inability of the text encoder to produce accurate embeddings,
which hinders the video generation model. In this work, we propose a novel
approach to overcome this challenge by selecting the optimal text embedding
through interpolation in the embedding space. We demonstrate that this method
enables the video generation model to produce the desired videos. Additionally,
we introduce a simple algorithm using perpendicular foot embeddings and cosine
similarity to identify the optimal interpolation embedding. Our findings
highlight the importance of accurate text embeddings and offer a pathway for
improving text-to-video generation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aneumo: A Large-Scale Comprehensive Synthetic Dataset of Aneurysm
  Hemodynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xigui Li, Yuanye Zhou, Feiyang Xiao, Xin Guo, Yichi Zhang, Chen Jiang, Jianchao Ge, Xiansheng Wang, Qimeng Wang, Taiwei Zhang, Chensen Lin, Yuan Cheng, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intracranial aneurysm (IA) is a common cerebrovascular disease that is
usually asymptomatic but may cause severe subarachnoid hemorrhage (SAH) if
ruptured. Although clinical practice is usually based on individual factors and
morphological features of the aneurysm, its pathophysiology and hemodynamic
mechanisms remain controversial. To address the limitations of current
research, this study constructed a comprehensive hemodynamic dataset of
intracranial aneurysms. The dataset is based on 466 real aneurysm models, and
10,000 synthetic models were generated by resection and deformation operations,
including 466 aneurysm-free models and 9,534 deformed aneurysm models. The
dataset also provides medical image-like segmentation mask files to support
insightful analysis. In addition, the dataset contains hemodynamic data
measured at eight steady-state flow rates (0.001 to 0.004 kg/s), including
critical parameters such as flow velocity, pressure, and wall shear stress,
providing a valuable resource for investigating aneurysm pathogenesis and
clinical prediction. This dataset will help advance the understanding of the
pathologic features and hemodynamic mechanisms of intracranial aneurysms and
support in-depth research in related fields. Dataset hosted at
https://github.com/Xigui-Li/Aneumo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar
  Editor <span class="chip">3DV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyue Liu, Kunming Luo, Heng Li, Qi Zhang, Yuan Liu, Li Yi, Ping Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce GaussianAvatar-Editor, an innovative framework for text-driven
editing of animatable Gaussian head avatars that can be fully controlled in
expression, pose, and viewpoint. Unlike static 3D Gaussian editing, editing
animatable 4D Gaussian avatars presents challenges related to motion occlusion
and spatial-temporal inconsistency. To address these issues, we propose the
Weighted Alpha Blending Equation (WABE). This function enhances the blending
weight of visible Gaussians while suppressing the influence on non-visible
Gaussians, effectively handling motion occlusion during editing. Furthermore,
to improve editing quality and ensure 4D consistency, we incorporate
conditional adversarial learning into the editing process. This strategy helps
to refine the edited results and maintain consistency throughout the animation.
By integrating these methods, our GaussianAvatar-Editor achieves photorealistic
and consistent results in animatable 4D Gaussian editing. We conduct
comprehensive experiments across various subjects to validate the effectiveness
of our proposed techniques, which demonstrates the superiority of our approach
over existing methods. More results and code are available at: [Project
Link](https://xiangyueliu.github.io/GaussianAvatar-Editor/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 3DV 2025. [Project
  Link](https://xiangyueliu.github.io/GaussianAvatar-Editor/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable artificial intelligence (XAI): from inherent explainability
  to <span class="highlight-title">large language model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuseini Mumuni, Alhassan Mumuni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has continued to achieve tremendous success in
recent times. However, the decision logic of these frameworks is often not
transparent, making it difficult for stakeholders to understand, interpret or
explain their behavior. This limitation hinders trust in machine learning
systems and causes a general reluctance towards their adoption in practical
applications, particularly in mission-critical domains like healthcare and
autonomous driving. Explainable AI (XAI) techniques facilitate the
explainability or interpretability of machine learning models, enabling users
to discern the basis of the decision and possibly avert undesirable behavior.
This comprehensive survey details the advancements of explainable AI methods,
from inherently interpretable models to modern approaches for achieving
interpretability of various black box models, including large language models
(LLMs). Additionally, we review explainable AI techniques that leverage LLM and
vision-language model (VLM) frameworks to automate or improve the
explainability of other machine learning models. The use of LLM and VLM as
interpretability methods particularly enables high-level, semantically
meaningful explanations of model decisions and behavior. Throughout the paper,
we highlight the scientific principles, strengths and weaknesses of
state-of-the-art methods and outline different areas of improvement. Where
appropriate, we also present qualitative and quantitative comparison results of
various methods to show how they compare. Finally, we discuss the key
challenges of XAI and directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discrete Prior-based Temporal-coherent Content Prediction for Blind Face
  Video Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianxin Xie, Bingbing Zheng, Wen Xue, Yunfei Zhang, Le Jiang, Ruotao Xu, Si Wu, Hau-San Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Blind face video restoration aims to restore high-fidelity details from
videos subjected to complex and unknown degradations. This task poses a
significant challenge of managing temporal heterogeneity while at the same time
maintaining stable face attributes. In this paper, we introduce a Discrete
Prior-based Temporal-Coherent content prediction transformer to address the
challenge, and our model is referred to as DP-TempCoh. Specifically, we
incorporate a spatial-temporal-aware content prediction module to synthesize
high-quality content from discrete visual priors, conditioned on degraded video
tokens. To further enhance the temporal coherence of the predicted content, a
motion statistics modulation module is designed to adjust the content, based on
discrete motion priors in terms of cross-frame mean and variance. As a result,
the statistics of the predicted content can match with that of real videos over
time. By performing extensive experiments, we verify the effectiveness of the
design elements and demonstrate the superior performance of our DP-TempCoh in
both synthetically and naturally degraded video restoration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surface-SOS: <span class="highlight-title">Self-Supervised</span> Object Segmentation via Neural Surface
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyun Zheng, Liwei Liao, Jianbo Jiao, Feng Gao, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised Object Segmentation (SOS) aims to segment objects without any
annotations. Under conditions of multi-camera inputs, the structural, textural
and geometrical consistency among each view can be leveraged to achieve
fine-grained object segmentation. To make better use of the above information,
we propose Surface representation based Self-supervised Object Segmentation
(Surface-SOS), a new framework to segment objects for each view by 3D surface
representation from multi-view images of a scene. To model high-quality
geometry surfaces for complex scenes, we design a novel scene representation
scheme, which decomposes the scene into two complementary neural representation
modules respectively with a Signed Distance Function (SDF). Moreover,
Surface-SOS is able to refine single-view segmentation with multi-view
unlabeled images, by introducing coarse segmentation masks as additional input.
To the best of our knowledge, Surface-SOS is the first self-supervised approach
that leverages neural surface representation to break the dependence on large
amounts of annotated data and strong constraints. These constraints typically
involve observing target objects against a static background or relying on
temporal supervision in videos. Extensive experiments on standard benchmarks
including LLFF, CO3D, BlendedMVS, TUM and several real-world scenes show that
Surface-SOS always yields finer object masks than its NeRF-based counterparts
and surpasses supervised single-view baselines remarkably. Code is available
at: https://github.com/zhengxyun/Surface-SOS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Scale Feature Extraction and Fusion Deep Learning Method for
  Classification of Wheat Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajjad Saleem, Adil Hussain, Nabila Majeed, Zahid Akhtar, Kamran Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wheat is an important source of dietary fiber and protein that is negatively
impacted by a number of risks to its growth. The difficulty of identifying and
classifying wheat diseases is discussed with an emphasis on wheat loose smut,
leaf rust, and crown and root rot. Addressing conditions like crown and root
rot, this study introduces an innovative approach that integrates multi-scale
feature extraction with advanced image segmentation techniques to enhance
classification accuracy. The proposed method uses neural network models
Xception, Inception V3, and ResNet 50 to train on a large wheat disease
classification dataset 2020 in conjunction with an ensemble of machine vision
classifiers, including voting and stacking. The study shows that the suggested
methodology has a superior accuracy of 99.75% in the classification of wheat
diseases when compared to current state-of-the-art approaches. A deep learning
ensemble model Xception showed the highest accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-informed DeepCT: Sinogram Wavelet Decomposition Meets Masked
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Zhou, Tan Liu, Bing Yu, Yanru Gong, Liu Shi, Qiegen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion model shows remarkable potential on sparse-view computed tomography
(SVCT) reconstruction. However, when a network is trained on a limited sample
space, its generalization capability may be constrained, which degrades
performance on unfamiliar data. For image generation tasks, this can lead to
issues such as blurry details and inconsistencies between regions. To alleviate
this problem, we propose a Sinogram-based Wavelet random decomposition And
Random mask diffusion Model (SWARM) for SVCT reconstruction. Specifically,
introducing a random mask strategy in the sinogram effectively expands the
limited training sample space. This enables the model to learn a broader range
of data distributions, enhancing its understanding and generalization of data
uncertainty. In addition, applying a random training strategy to the
high-frequency components of the sinogram wavelet enhances feature
representation and improves the ability to capture details in different
frequency bands, thereby improving performance and robustness. Two-stage
iterative reconstruction method is adopted to ensure the global consistency of
the reconstructed image while refining its details. Experimental results
demonstrate that SWARM outperforms competing approaches in both quantitative
and qualitative performance across various datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IE-Bench: Advancing the Measurement of Text-Driven Image Editing for
  Human Perception Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangkun Sun, Bowen Qu, Xiaoyu Liang, Songlin Fan, Wei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-driven image editing have been significant, yet the
task of accurately evaluating these edited images continues to pose a
considerable challenge. Different from the assessment of text-driven image
generation, text-driven image editing is characterized by simultaneously
conditioning on both text and a source image. The edited images often retain an
intrinsic connection to the original image, which dynamically change with the
semantics of the text. However, previous methods tend to solely focus on
text-image alignment or have not aligned with human perception. In this work,
we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to
enhance the assessment of text-driven edited images. IE-Bench includes a
database contains diverse source images, various editing prompts and the
corresponding results different editing methods, and total 3,010 Mean Opinion
Scores (MOS) provided by 25 human subjects. Furthermore, we introduce IE-QA, a
multi-modality source-aware quality assessment method for text-driven image
editing. To the best of our knowledge, IE-Bench offers the first IQA dataset
and model tailored for text-driven image editing. Extensive experiments
demonstrate IE-QA's superior subjective-alignments on the text-driven image
editing task compared with previous metrics. We will make all related data and
code available to the public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ForestProtector: An IoT Architecture Integrating Machine Vision and Deep
  Reinforcement Learning for Efficient Wildfire Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Bonilla-Ormachea, Horacio Cuizaga, Edwin Salcedo, Sebastian Castro, Sergio Fernandez-Testa, Misael Mamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection of forest fires is crucial to minimizing the environmental
and socioeconomic damage they cause. Indeed, a fire's duration directly
correlates with the difficulty and cost of extinguishing it. For instance, a
fire burning for 1 minute might require 1 liter of water to extinguish, while a
2-minute fire could demand 100 liters, and a 10-minute fire might necessitate
1,000 liters. On the other hand, existing fire detection systems based on novel
technologies (e.g., remote sensing, PTZ cameras, UAVs) are often expensive and
require human intervention, making continuous monitoring of large areas
impractical. To address this challenge, this work proposes a low-cost forest
fire detection system that utilizes a central gateway device with computer
vision capabilities to monitor a 360{\deg} field of view for smoke at long
distances. A deep reinforcement learning agent enhances surveillance by
dynamically controlling the camera's orientation, leveraging real-time sensor
data (smoke levels, ambient temperature, and humidity) from distributed IoT
devices. This approach enables automated wildfire monitoring across expansive
areas while reducing false positives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the proceedings of the 11th International
  Conference on Automation, Robotics, and Applications (ICARA 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TalkingEyes: Pluralistic Speech-Driven 3D Eye Gaze Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiang Zhuang, Chunshan Ma, Yao Cheng, Xuan Cheng, Jing Liao, Juncong Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although significant progress has been made in the field of speech-driven 3D
facial animation recently, the speech-driven animation of an indispensable
facial component, eye gaze, has been overlooked by recent research. This is
primarily due to the weak correlation between speech and eye gaze, as well as
the scarcity of audio-gaze data, making it very challenging to generate 3D eye
gaze motion from speech alone. In this paper, we propose a novel data-driven
method which can generate diverse 3D eye gaze motions in harmony with the
speech. To achieve this, we firstly construct an audio-gaze dataset that
contains about 14 hours of audio-mesh sequences featuring high-quality eye gaze
motion, head motion and facial motion simultaneously. The motion data is
acquired by performing lightweight eye gaze fitting and face reconstruction on
videos from existing audio-visual datasets. We then tailor a novel
speech-to-motion translation framework in which the head motions and eye gaze
motions are jointly generated from speech but are modeled in two separate
latent spaces. This design stems from the physiological knowledge that the
rotation range of eyeballs is less than that of head. Through mapping the
speech embedding into the two latent spaces, the difficulty in modeling the
weak correlation between speech and non-verbal motion is thus attenuated.
Finally, our TalkingEyes, integrated with a speech-driven 3D facial motion
generator, can synthesize eye gaze motion, eye blinks, head motion and facial
motion collectively from speech. Extensive quantitative and qualitative
evaluations demonstrate the superiority of the proposed method in generating
diverse and natural 3D eye gaze motions from speech. The project page of this
paper is: https://lkjkjoiuiu.github.io/TalkingEyes_Home/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon
  Visuomotor Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Yiqing Yang, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a low-cost quadruped manipulation system that solves long-horizon
real-world tasks, trained by reinforcement learning purely in simulation. The
system comprises 1) a hierarchical design of a high-level policy for
visual-mobile manipulation following instructions, and a low-level policy for
quadruped movement and limb-control, 2) a progressive policy expansion approach
for solving the long-horizon task together with a teacher-student framework for
efficient high-level training of the high-level visuomotor policy, and 3) a
suite of techniques for minimizing sim-to-real gaps.
  With budget-friendly but limited reliability and performance hardware, and
just one wrist-mounted RGB camera, the entire system fully trained in
simulation achieves high success rates for long horizon tasks involving search,
move, grasp, and drop-into, with fluid sim-to-real transfer in a wide variety
of indoor and outdoor scenes and lighting conditions.Extensive real-world
evaluations show that on the long horizon mobile manipulation tasks, our system
achieves good performance when transferred to real both in terms of task
success rate and execution efficiency. Finally, we discuss the necessity of our
sim-to-real techniques for legged mobile manipulation, and show their ablation
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FoundationStereo: Zero-Shot Stereo Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tremendous progress has been made in deep stereo matching to excel on
benchmark datasets through per-domain fine-tuning. However, achieving strong
zero-shot generalization - a hallmark of foundation models in other computer
vision tasks - remains challenging for stereo matching. We introduce
FoundationStereo, a foundation model for stereo depth estimation designed to
achieve strong zero-shot generalization. To this end, we first construct a
large-scale (1M stereo pairs) synthetic training dataset featuring large
diversity and high photorealism, followed by an automatic self-curation
pipeline to remove ambiguous samples. We then design a number of network
architecture components to enhance scalability, including a side-tuning feature
backbone that adapts rich monocular priors from vision foundation models to
mitigate the sim-to-real gap, and long-range context reasoning for effective
cost volume filtering. Together, these components lead to strong robustness and
accuracy across domains, establishing a new standard in zero-shot stereo depth
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLORA: Formal Language Model Enables Robust Training-free Zero-shot
  Object Referring Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Chen, Zijing Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object Referring Analysis (ORA), commonly known as referring expression
comprehension, requires the identification and localization of specific objects
in an image based on natural descriptions. Unlike generic object detection, ORA
requires both accurate language understanding and precise visual localization,
making it inherently more complex. Although recent pre-trained large visual
grounding detectors have achieved significant progress, they heavily rely on
extensively labeled data and time-consuming learning. To address these, we
introduce a novel, training-free framework for zero-shot ORA, termed FLORA
(Formal Language for Object Referring and Analysis). FLORA harnesses the
inherent reasoning capabilities of large language models (LLMs) and integrates
a formal language model - a logical framework that regulates language within
structured, rule-based descriptions - to provide effective zero-shot ORA. More
specifically, our formal language model (FLM) enables an effective,
logic-driven interpretation of object descriptions without necessitating any
training processes. Built upon FLM-regulated LLM outputs, we further devise a
Bayesian inference framework and employ appropriate off-the-shelf interpretive
models to finalize the reasoning, delivering favorable robustness against LLM
hallucinations and compelling ORA performance in a training-free manner. In
practice, our FLORA boosts the zero-shot performance of existing pretrained
grounding detectors by up to around 45%. Our comprehensive evaluation across
different challenging datasets also confirms that FLORA consistently surpasses
current state-of-the-art zero-shot methods in both detection and segmentation
tasks associated with zero-shot ORA. We believe our probabilistic parsing and
reasoning of the LLM outputs elevate the reliability and interpretability of
zero-shot ORA. We shall release codes upon publication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MVTamperBench: Evaluating Robustness of Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19794v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19794v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Agarwal, Srikant Panda, Angeline Charles, Bhargava Kumar, Hitesh Patel, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Dong-Kyu Chae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have driven major advances in video
understanding, yet their vulnerability to adversarial tampering and
manipulations remains underexplored. To address this gap, we introduce
MVTamperBench, a benchmark that systematically evaluates MLLM robustness
against five prevalent tampering techniques: rotation, masking, substitution,
repetition, and dropping. Built from 3.4K original videos-expanded to over 17K
tampered clips spanning 19 video tasks.
  MVTamperBench challenges models to detect manipulations in spatial and
temporal coherence. We evaluate 45 recent MLLMs from 15+ model families,
revealing substantial variability in resilience across tampering types and
showing that larger parameter counts do not necessarily guarantee robustness.
MVTamperBench sets a new benchmark for developing tamper-resilient MLLM in
safety-critical applications, including detecting clickbait, preventing harmful
content distribution, and enforcing policies on media platforms. We release all
code and data to foster open research in trustworthy video understanding.
  Code: https://amitbcp.github.io/MVTamperBench/ Data:
https://huggingface.co/datasets/Srikant86/MVTamperBench
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid
  Prototyping in Virtual Reality Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Augusto Pinheiro de Sousa, Heiko Hamann, Oliver Deussen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SLAM is a foundational technique with broad applications in robotics and
AR/VR. SLAM simulations evaluate new concepts, but testing on
resource-constrained devices, such as VR HMDs, faces challenges: high
computational cost and restricted sensor data access. This work proposes a
sparse framework using mesh geometry projections as features, which improves
efficiency and circumvents direct sensor data access, advancing SLAM research
as we demonstrate in VR and through numerical evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Niu, Sheng Zhong, Xiuyuan Lu, Shaojie Shen, Guillermo Gallego, Yi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based visual odometry is a specific branch of visual Simultaneous
Localization and Mapping (SLAM) techniques, which aims at solving tracking and
mapping subproblems (typically in parallel), by exploiting the special working
principles of neuromorphic (i.e., event-based) cameras. Due to the
motion-dependent nature of event data, explicit data association (i.e., feature
matching) under large-baseline view-point changes is difficult to establish,
making direct methods a more rational choice. However, state-of-the-art direct
methods are limited by the high computational complexity of the mapping
sub-problem and the degeneracy of camera pose tracking in certain degrees of
freedom (DoF) in rotation. In this paper, we tackle these issues by building an
event-based stereo visual-inertial odometry system on top of a direct pipeline.
Specifically, to speed up the mapping operation, we propose an efficient
strategy for sampling contour points according to the local dynamics of events.
The mapping performance is also improved in terms of structure completeness and
local smoothness by merging the temporal stereo and static stereo results. To
circumvent the degeneracy of camera pose tracking in recovering the pitch and
yaw components of general 6-DoF motion, we introduce IMU measurements as motion
priors via pre-integration. To this end, a compact back-end is proposed for
continuously updating the IMU bias and predicting the linear velocity, enabling
an accurate motion prediction for camera pose tracking. The resulting system
scales well with modern high-resolution event cameras and leads to better
global positioning accuracy in large-scale outdoor environments. Extensive
evaluations on five publicly available datasets featuring different resolutions
and scenarios justify the superior performance of the proposed system against
five state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BILTS: A Bi-Invariant Similarity Measure for Robust Object Trajectory
  Recognition under Reference Frame Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arno Verduyn, Erwin Aertbeliën, Glenn Maes, Joris De Schutter, Maxim Vochten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When similar object motions are performed in diverse contexts but are meant
to be recognized under a single classification, these contextual variations act
as disturbances that negatively affect accurate motion recognition. In this
paper, we focus on contextual variations caused by reference frame variations.
To robustly deal with these variations, similarity measures have been
introduced that compare object motion trajectories in a context-invariant
manner. However, most are highly sensitive to noise near singularities, where
the measure is not uniquely defined, and lack bi-invariance (invariance to both
world and body frame variations). To address these issues, we propose the novel
\textit{Bi-Invariant Local Trajectory-Shape Similarity} (BILTS) measure.
Compared to other measures, the BILTS measure uniquely offers bi-invariance,
boundedness, and third-order shape identity. Aimed at practical
implementations, we devised a discretized and regularized version of the BILTS
measure which shows exceptional robustness to singularities. This is
demonstrated through rigorous recognition experiments using multiple datasets.
On average, BILTS attained the highest recognition ratio and least sensitivity
to contextual variations compared to other invariant object motion similarity
measures. We believe that the BILTS measure is a valuable tool for recognizing
motions performed in diverse contexts and has potential in other applications,
including the recognition, segmentation, and adaptation of both motion and
force trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted as a regular research paper for
  consideration in the Journal of Intelligent & Robotic Systems. The content in
  this preprint is identical to the version submitted for peer review, except
  for formatting differences required by the journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Diversity and Uncertainty in Active learning with
  <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Doucet, Benjamin Estermann, Till Aczel, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the integration of diversity-based and uncertainty-based
sampling strategies in active learning, particularly within the context of
self-supervised pre-trained models. We introduce a straightforward heuristic
called TCM that mitigates the cold start problem while maintaining strong
performance across various data levels. By initially applying TypiClust for
diversity sampling and subsequently transitioning to uncertainty sampling with
Margin, our approach effectively combines the strengths of both strategies. Our
experiments demonstrate that TCM consistently outperforms existing methods
across various datasets in both low and high data regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024 Workshop on Practical Machine Learning for Low
  Resource Settings (PML4LRS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Compression Autoen<span class="highlight-title">code</span>r for Efficient High-Resolution Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10733v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10733v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder
models for accelerating high-resolution diffusion models. Existing autoencoder
models have demonstrated impressive results at a moderate spatial compression
ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for
high spatial compression ratios (e.g., 64x). We address this challenge by
introducing two key techniques: (1) Residual Autoencoding, where we design our
models to learn residuals based on the space-to-channel transformed features to
alleviate the optimization difficulty of high spatial-compression autoencoders;
(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases
training strategy for mitigating the generalization penalty of high
spatial-compression autoencoders. With these designs, we improve the
autoencoder's spatial compression ratio up to 128 while maintaining the
reconstruction quality. Applying our DC-AE to latent diffusion models, we
achieve significant speedup without accuracy drop. For example, on ImageNet
512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup
on H100 GPU for UViT-H while achieving a better FID, compared with the widely
used SD-VAE-f8 autoencoder. Our code is available at
https://github.com/mit-han-lab/efficientvit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. First two authors contributed equally to this work. Update:
  fix typo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generate E-commerce Product Background by Integrating Category
  Commonality and Personalized Style <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohan Wang, Wei Feng, Yaoyu Li, Zheng Zhang, Jingjing Lv, Junjie Shen, Zhangang Lin, Jingping Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state-of-the-art methods for e-commerce product background generation
suffer from the inefficiency of designing product-wise prompts when scaling up
the production, as well as the ineffectiveness of describing fine-grained
styles when customizing personalized backgrounds for some specific brands. To
address these obstacles, we integrate the category commonality and personalized
style into diffusion models. Concretely, we propose a Category-Wise Generator
to enable large-scale background generation with only one model for the first
time. A unique identifier in the prompt is assigned to each category, whose
attention is located on the background by a mask-guided cross attention layer
to learn the category-wise style. Furthermore, for products with specific and
fine-grained requirements in layout, elements, etc, a Personality-Wise
Generator is devised to learn such personalized style directly from a reference
image to resolve textual ambiguities, and is trained in a self-supervised
manner for more efficient training data usage. To advance research in this
field, the first large-scale e-commerce product background generation dataset
BG60k is constructed, which covers more than 60k product images from over 2k
categories. Experiments demonstrate that our method could generate high-quality
backgrounds for different categories, and maintain the personalized background
style of reference images. BG60k will be available at
\url{https://github.com/Whileherham/BG60k}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LayerAnimate: Layer-specific Control for Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxue Yang, Lue Fan, Zuzeng Lin, Feng Wang, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Animated video separates foreground and background elements into layers, with
distinct processes for sketching, refining, coloring, and in-betweening.
Existing video generation methods typically treat animation as a monolithic
data domain, lacking fine-grained control over individual layers. In this
paper, we introduce LayerAnimate, a novel architectural approach that enhances
fine-grained control over individual animation layers within a video diffusion
model, allowing users to independently manipulate foreground and background
elements in distinct layers. To address the challenge of limited layer-specific
data, we propose a data curation pipeline that features automated element
segmentation, motion-state hierarchical merging, and motion coherence
refinement. Through quantitative and qualitative comparisons, and user study,
we demonstrate that LayerAnimate outperforms current methods in terms of
animation quality, control precision, and usability, making it an ideal tool
for both professional animators and amateur enthusiasts. This framework opens
up new possibilities for layer-specific animation applications and creative
flexibility. Our code is available at https://layeranimate.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://layeranimate.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Deep Learning for Polyp Segmentation: Techniques, Challenges
  and Future Trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18373v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18373v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Mei, Tao Zhou, Kaiwen Huang, Yizhe Zhang, Yi Zhou, Ye Wu, Huazhu Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection and assessment of polyps play a crucial role in the
prevention and treatment of colorectal cancer (CRC). Polyp segmentation
provides an effective solution to assist clinicians in accurately locating and
segmenting polyp regions. In the past, people often relied on manually
extracted lower-level features such as color, texture, and shape, which often
had issues capturing global context and lacked robustness to complex scenarios.
With the advent of deep learning, more and more outstanding medical image
segmentation algorithms based on deep learning networks have emerged, making
significant progress in this field. This paper provides a comprehensive review
of polyp segmentation algorithms. We first review some traditional algorithms
based on manually extracted features and deep segmentation algorithms, then
detail benchmark datasets related to the topic. Specifically, we carry out a
comprehensive evaluation of recent deep learning models and results based on
polyp sizes, considering the pain points of research topics and differences in
network structures. Finally, we discuss the challenges of polyp segmentation
and future trends in this field. The models, benchmark datasets, and source
code links we collected are all published at
https://github.com/taozh2017/Awesome-Polyp-Segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Have been published in Visual Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Isolated Diffusion: Optimizing Multi-Concept Text-to-Image Generation
  Training-Freely with Isolated Diffusion Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models have achieved great success in
synthesizing high-quality and diverse images given target text prompts. Despite
the revolutionary image generation ability, current state-of-the-art models
still struggle to deal with multi-concept generation accurately in many cases.
This phenomenon is known as ``concept bleeding" and displays as the unexpected
overlapping or merging of various concepts. This paper presents a general
approach for text-to-image diffusion models to address the mutual interference
between different subjects and their attachments in complex scenes, pursuing
better text-image consistency. The core idea is to isolate the synthesizing
processes of different concepts. We propose to bind each attachment to
corresponding subjects separately with split text prompts. Besides, we
introduce a revision method to fix the concept bleeding problem in
multi-subject synthesis. We first depend on pre-trained object detection and
segmentation models to obtain the layouts of subjects. Then we isolate and
resynthesize each subject individually with corresponding text prompts to avoid
mutual interference. Overall, we achieve a training-free strategy, named
Isolated Diffusion, to optimize multi-concept text-to-image synthesis. It is
compatible with the latest Stable Diffusion XL (SDXL) and prior Stable
Diffusion (SD) models. We compare our approach with alternative methods using a
variety of multi-concept text prompts and demonstrate its effectiveness with
clear advantages in text-image consistency and user study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Visualization and Computer Graphics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expression <span class="highlight-title">Prompt</span> Collaboration <span class="highlight-title">Transformer</span> for Universal Referring
  Video Object Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Chen, Jiacheng Lin, Guojin Zhong, Haolong Fu, Ke Nai, Kailun Yang, Zhiyong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-guided Video Object Segmentation (A-VOS) and Referring Video Object
Segmentation (R-VOS) are two highly related tasks that both aim to segment
specific objects from video sequences according to expression prompts. However,
due to the challenges of modeling representations for different modalities,
existing methods struggle to strike a balance between interaction flexibility
and localization precision. In this paper, we address this problem from two
perspectives: the alignment of audio and text and the deep interaction among
audio, text, and visual modalities. First, we propose a universal architecture,
the Expression Prompt Collaboration Transformer, herein EPCFormer. Next, we
propose an Expression Alignment (EA) mechanism for audio and text. The proposed
EPCFormer exploits the fact that audio and text prompts referring to the same
objects are semantically equivalent by using contrastive learning for both
types of expressions. Then, to facilitate deep interactions among audio, text,
and visual modalities, we introduce an Expression-Visual Attention (EVA)
module. The knowledge of video object segmentation in terms of the expression
prompts can seamlessly transfer between the two tasks by deeply exploring
complementary cues between text and audio. Experiments on well-recognized
benchmarks demonstrate that our EPCFormer attains state-of-the-art results on
both tasks. The source code will be made publicly available at
https://github.com/lab206/EPCFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Knowledge-Based Systems (KBS). The source code will be
  made publicly available at https://github.com/lab206/EPCFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tarsier2: Advancing Large Vision-Language Models from Detailed Video
  Description to Comprehensive Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, Yuan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Tarsier2, a state-of-the-art large vision-language model (LVLM)
designed for generating detailed and accurate video descriptions, while also
exhibiting superior general video understanding capabilities. Tarsier2 achieves
significant advancements through three key upgrades: (1) Scaling pre-training
data from 11M to 40M video-text pairs, enriching both volume and diversity; (2)
Performing fine-grained temporal alignment during supervised fine-tuning; (3)
Using model-based sampling to automatically construct preference data and
applying DPO training for optimization. Extensive experiments show that
Tarsier2-7B consistently outperforms leading proprietary models, including
GPT-4o and Gemini 1.5 Pro, in detailed video description tasks. On the DREAM-1K
benchmark, Tarsier2-7B improves F1 by 2.8\% over GPT-4o and 5.8\% over
Gemini-1.5-Pro. In human side-by-side evaluations, Tarsier2-7B shows a +8.6\%
performance advantage over GPT-4o and +24.9\% over Gemini-1.5-Pro. Tarsier2-7B
also sets new state-of-the-art results across 15 public benchmarks, spanning
tasks such as video question-answering, video grounding, hallucination test,
and embodied question-answering, demonstrating its versatility as a robust
generalist vision-language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continuous Urban Change Detection from Satellite Image Time Series with
  Temporal Feature Refinement and Multi-Task Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Hafner, Heng Fang, Hossein Azizpour, Yifang Ban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urbanization advances at unprecedented rates, resulting in negative effects
on the environment and human well-being. Remote sensing has the potential to
mitigate these effects by supporting sustainable development strategies with
accurate information on urban growth. Deep learning-based methods have achieved
promising urban change detection results from optical satellite image pairs
using convolutional neural networks (ConvNets), transformers, and a multi-task
learning setup. However, transformers have not been leveraged for urban change
detection with multi-temporal data, i.e., >2 images, and multi-task learning
methods lack integration approaches that combine change and segmentation
outputs. To fill this research gap, we propose a continuous urban change
detection method that identifies changes in each consecutive image pair of a
satellite image time series (SITS). Specifically, we propose a temporal feature
refinement (TFR) module that utilizes self-attention to improve ConvNet-based
multi-temporal building representations. Furthermore, we propose a multi-task
integration (MTI) module that utilizes Markov networks to find an optimal
building map time series based on segmentation and dense change outputs. The
proposed method effectively identifies urban changes based on high-resolution
SITS acquired by the PlanetScope constellation (F1 score 0.551) and Gaofen-2
(F1 score 0.440). Moreover, our experiments on two challenging datasets
demonstrate the effectiveness of the proposed method compared to bi-temporal
and multi-temporal urban change detection and segmentation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at IEEE Transactions on Geoscience and Remote Sensing,
  Code will be available at https://github.com/SebastianHafner/ContUrbanCD.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mamba2D: A Natively Multi-Dimensional State-Space Model for Vision Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16146v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16146v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enis Baty, Alejandro Hernández Díaz, Chris Bridges, Rebecca Davidson, Steve Eckersley, Simon Hadfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-Space Models (SSMs) have recently emerged as a powerful and efficient
alternative to the long-standing transformer architecture. However, existing
SSM conceptualizations retain deeply rooted biases from their roots in natural
language processing. This constrains their ability to appropriately model the
spatially-dependent characteristics of visual inputs. In this paper, we address
these limitations by re-deriving modern selective state-space techniques,
starting from a natively multidimensional formulation. Currently, prior works
attempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on
arbitrary combinations of 1D scan directions to capture spatial dependencies.
In contrast, Mamba2D improves upon this with a single 2D scan direction that
factors in both dimensions of the input natively, effectively modelling spatial
dependencies when constructing hidden states. Mamba2D shows comparable
performance to prior adaptations of SSMs for vision tasks, on standard image
classification evaluations with the ImageNet-1K dataset. Source code is
available at https://github.com/cocoalex00/Mamba2D.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Synthesis for Zero-Shot Model Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15977v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15977v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyun Yang, Juan Cao, Danding Wang, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, generative models are shaping various fields such as art, design,
and human-computer interaction, yet accompanied by challenges related to
copyright infringement and content management. In response, existing research
seeks to identify the unique fingerprints on the images they generate, which
can be leveraged to attribute the generated images to their source models.
Existing methods, however, are constrained to identifying models within a
static set included in the classifier training, failing to adapt to newly
emerged unseen models dynamically. To bridge this gap, we aim to develop a
generalized model fingerprint extractor capable of zero-shot attribution,
effectively attributes unseen models without exposure during training. Central
to our method is a model synthesis technique, which generates numerous
synthetic models mimicking the fingerprint patterns of real-world generative
models. The design of the synthesis technique is motivated by observations on
how the basic generative model's architecture building blocks and parameters
influence fingerprint patterns, and it is validated through two designed
metrics that examine synthetic models' fidelity and diversity. Our experiments
demonstrate that this fingerprint extractor, trained solely on synthetic
models, achieves impressive zero-shot generalization on a wide range of
real-world generative models, improving model identification and verification
accuracy on unseen models by over 40% and 15%, respectively, compared to
existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-stage Deep Learning Artifact Reduction for Pallel-beam Computed
  Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Shi, Daniel M. Pelt, K. Joost Batenburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed Tomography (CT) using synchrotron radiation is a powerful technique
that, compared to lab-CT techniques, boosts high spatial and temporal
resolution while also providing access to a range of contrast-formation
mechanisms. The acquired projection data is typically processed by a
computational pipeline composed of multiple stages. Artifacts introduced during
data acquisition can propagate through the pipeline, and degrade image quality
in the reconstructed images. Recently, deep learning has shown significant
promise in enhancing image quality for images representing scientific data.
This success has driven increasing adoption of deep learning techniques in CT
imaging. Various approaches have been proposed to incorporate deep learning
into computational pipelines, but each has limitations in addressing artifacts
effectively and efficiently in synchrotron CT, either in properly addressing
the specific artifacts, or in computational efficiency.
  Recognizing these challenges, we introduce a novel method that incorporates
separate deep learning models at each stage of the tomography
pipeline-projection, sinogram, and reconstruction-to address specific artifacts
locally in a data-driven way. Our approach includes bypass connections that
feed both the outputs from previous stages and raw data to subsequent stages,
minimizing the risk of error propagation. Extensive evaluations on both
simulated and real-world datasets illustrate that our approach effectively
reduces artifacts and outperforms comparison methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IncSAR: A Dual Fusion Incremental Learning Framework for SAR Target
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Karantaidis, Athanasios Pantsios, Ioannis Kompatsiaris, Symeon Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have achieved significant success in Synthetic
Aperture Radar (SAR) target recognition using predefined datasets in static
scenarios. However, real-world applications demand that models incrementally
learn new information without forgetting previously acquired knowledge. The
challenge of catastrophic forgetting, where models lose past knowledge when
adapting to new tasks, remains a critical issue. In this paper, we introduce
IncSAR, an incremental learning framework designed to tackle catastrophic
forgetting in SAR target recognition. IncSAR combines the power of a Vision
Transformer (ViT) and a custom-designed Convolutional Neural Network (CNN) in a
dual-branch architecture, integrated via a late-fusion strategy. Additionally,
we explore the use of TinyViT to reduce computational complexity and propose an
attention mechanism to dynamically enhance feature representation. To mitigate
the speckle noise inherent in SAR images, we employ a denoising module based on
a neural network approximation of Robust Principal Component Analysis (RPCA),
leveraging a simple neural network for efficient noise reduction in SAR
imagery. Moreover, a random projection layer improves the linear separability
of features, and a variant of Linear Discriminant Analysis (LDA) decorrelates
extracted class prototypes for better generalization. Extensive experiments on
the MSTAR, SAR-AIRcraft-1.0, and OpenSARShip benchmark datasets demonstrate
that IncSAR significantly outperforms state-of-the-art approaches, achieving a
99.63\% average accuracy and a 0.33\% performance drop, representing an 89\%
improvement in retention compared to existing techniques. The source code is
available at https://github.com/geokarant/IncSAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLSBench: Unveiling Visual Leakage in Multimodal Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhao Hu, Dongrui Liu, Hao Li, Xuanjing Huang, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety concerns of Multimodal large language models (MLLMs) have gradually
become an important problem in various applications. Surprisingly, previous
works indicate a counter-intuitive phenomenon that using textual unlearning to
align MLLMs achieves comparable safety performances with MLLMs trained with
image-text pairs. To explain such a counter-intuitive phenomenon, we discover a
visual safety information leakage (VSIL) problem in existing multimodal safety
benchmarks, i.e., the potentially risky and sensitive content in the image has
been revealed in the textual query. In this way, MLLMs can easily refuse these
sensitive text-image queries according to textual queries. However, image-text
pairs without VSIL are common in real-world scenarios and are overlooked by
existing multimodal safety benchmarks. To this end, we construct multimodal
visual leakless safety benchmark (VLSBench) preventing visual safety leakage
from image to textual query with 2.4k image-text pairs. Experimental results
indicate that VLSBench poses a significant challenge to both open-source and
close-source MLLMs, including LLaVA, Qwen2-VL, Llama3.2-Vision, and GPT-4o.
This study demonstrates that textual alignment is enough for multimodal safety
scenarios with VSIL, while multimodal alignment is a more promising solution
for multimodal safety scenarios without VSIL. Please see our code and data at:
https://hxhcreate.github.io/vlsbench.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SARATR-X: Towards Building A Foundation Model for SAR Target Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09365v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09365v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Li, Wei Yang, Yuenan Hou, Li Liu, Yongxiang Liu, Xiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable progress in synthetic aperture radar automatic target
recognition (SAR ATR), recent efforts have concentrated on detecting and
classifying a specific category, e.g., vehicles, ships, airplanes, or
buildings. One of the fundamental limitations of the top-performing SAR ATR
methods is that the learning paradigm is supervised, task-specific,
limited-category, closed-world learning, which depends on massive amounts of
accurately annotated samples that are expensively labeled by expert SAR
analysts and have limited generalization capability and scalability. In this
work, we make the first attempt towards building a foundation model for SAR
ATR, termed SARATR-X. SARATR-X learns generalizable representations via
self-supervised learning (SSL) and provides a cornerstone for label-efficient
model adaptation to generic SAR target detection and classification tasks.
Specifically, SARATR-X is trained on 0.18 M unlabelled SAR target samples,
which are curated by combining contemporary benchmarks and constitute the
largest publicly available dataset till now. Considering the characteristics of
SAR images, a backbone tailored for SAR ATR is carefully designed, and a
two-step SSL method endowed with multi-scale gradient features was applied to
ensure the feature diversity and model scalability of SARATR-X. The
capabilities of SARATR-X are evaluated on classification under few-shot and
robustness settings and detection across various categories and scenes, and
impressive performance is achieved, often competitive with or even superior to
prior fully supervised, semi-supervised, or self-supervised algorithms. Our
SARATR-X and the curated dataset are released at
https://github.com/waterdisappear/SARATR-X to foster research into foundation
models for SAR image interpretation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating analytical variability in fMRI results with style transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03703v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03703v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elodie Germani, Camille Maumet, Elisa Fromont
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to improve the reproducibility of neuroimaging
results by converting statistic maps across different functional MRI pipelines.
We make the assumption that pipelines used to compute fMRI statistic maps can
be considered as a style component and we propose to use different generative
models, among which, Generative Adversarial Networks (GAN) and Diffusion Models
(DM) to convert statistic maps across different pipelines. We explore the
performance of multiple GAN frameworks, and design a new DM framework for
unsupervised multi-domain styletransfer. We constrain the generation of 3D fMRI
statistic maps using the latent space of an auxiliary classifier that
distinguishes statistic maps from different pipelines and extend traditional
sampling techniques used in DM to improve the transition performance. Our
experiments demonstrate that our proposed methods aresuccessful: pipelines can
indeed be transferred as a style component, providing animportant source of
data augmentation for future medical studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating lensed quasars discovery and modeling with physics-informed
  variational autoen<span class="highlight-title">code</span>rs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irham T. Andika, Stefan Schuldt, Sherry H. Suyu, Satadru Bag, Raoul Cañameras, Alejandra Melo, Claudio Grillo, James H. H. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strongly lensed quasars provide valuable insights into the rate of cosmic
expansion, the distribution of dark matter in foreground deflectors, and the
characteristics of quasar hosts. However, detecting them in astronomical images
is difficult due to the prevalence of non-lensing objects. To address this
challenge, we developed a generative deep learning model called VariLens, built
upon a physics-informed variational autoencoder. This model seamlessly
integrates three essential modules: image reconstruction, object
classification, and lens modeling, offering a fast and comprehensive approach
to strong lens analysis. VariLens is capable of rapidly determining both (1)
the probability that an object is a lens system and (2) key parameters of a
singular isothermal ellipsoid (SIE) mass model -- including the Einstein radius
($\theta_\mathrm{E}$), lens center, and ellipticity -- in just milliseconds
using a single CPU. A direct comparison of VariLens estimates with traditional
lens modeling for 20 known lensed quasars within the Subaru Hyper Suprime-Cam
(HSC) footprint shows good agreement, with both results consistent within
$2\sigma$ for systems with $\theta_\mathrm{E}<3$ arcsecs. To identify new
lensed quasar candidates, we begin with an initial sample of approximately 80
million sources, combining HSC data with multiwavelength information from
various surveys. After applying a photometric preselection aimed at locating
$z>1.5$ sources, the number of candidates is reduced to 710,966. Subsequently,
VariLens highlights 13,831 sources, each showing a high likelihood of being a
lens. A visual assessment of these objects results in 42 promising candidates
that await spectroscopic confirmation. These results underscore the potential
of automated deep learning pipelines to efficiently detect and model strong
lenses in large datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the Astronomy & Astrophysics journal and updated to
  reflect the revised version. The paper consists of 17 main pages, 14 figures,
  and 5 tables. We welcome feedback and comments from readers!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WaveDH: Wavelet Sub-bands Guided ConvNet for Efficient Image Dehazing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01604v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01604v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongmin Hwang, Daeyoung Han, Cheolkon Jung, Moongu Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The surge in interest regarding image dehazing has led to notable
advancements in deep learning-based single image dehazing approaches,
exhibiting impressive performance in recent studies. Despite these strides,
many existing methods fall short in meeting the efficiency demands of practical
applications. In this paper, we introduce WaveDH, a novel and compact ConvNet
designed to address this efficiency gap in image dehazing. Our WaveDH leverages
wavelet sub-bands for guided up-and-downsampling and frequency-aware feature
refinement. The key idea lies in utilizing wavelet decomposition to extract
low-and-high frequency components from feature levels, allowing for faster
processing while upholding high-quality reconstruction. The downsampling block
employs a novel squeeze-and-attention scheme to optimize the feature
downsampling process in a structurally compact manner through wavelet domain
learning, preserving discriminative features while discarding noise components.
In our upsampling block, we introduce a dual-upsample and fusion mechanism to
enhance high-frequency component awareness, aiding in the reconstruction of
high-frequency details. Departing from conventional dehazing methods that treat
low-and-high frequency components equally, our feature refinement block
strategically processes features with a frequency-aware approach. By employing
a coarse-to-fine methodology, it not only refines the details at frequency
levels but also significantly optimizes computational costs. The refinement is
performed in a maximum 8x downsampled feature space, striking a favorable
efficiency-vs-accuracy trade-off. Extensive experiments demonstrate that our
method, WaveDH, outperforms many state-of-the-art methods on several image
dehazing benchmarks with significantly reduced computational costs. Our code is
available at https://github.com/AwesomeHwang/WaveDH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-guided Image Restoration and Semantic Enhancement for Text-to-Image
  Person Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09059v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09059v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delong Liu, Haiwen Li, Zhicheng Zhao, Yuan Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Text-to-Image Person Retrieval (TIPR) is to retrieve specific
person images according to the given textual descriptions. A primary challenge
in this task is bridging the substantial representational gap between visual
and textual modalities. The prevailing methods map texts and images into
unified embedding space for matching, while the intricate semantic
correspondences between texts and images are still not effectively constructed.
To address this issue, we propose a novel TIPR framework to build fine-grained
interactions and alignment between person images and the corresponding texts.
Specifically, via fine-tuning the Contrastive Language-Image Pre-training
(CLIP) model, a visual-textual dual encoder is firstly constructed, to
preliminarily align the image and text features. Secondly, a Text-guided Image
Restoration (TIR) auxiliary task is proposed to map abstract textual entities
to specific image regions, improving the alignment between local textual and
visual embeddings. Additionally, a cross-modal triplet loss is presented to
handle hard samples, and further enhance the model's discriminability for minor
differences. Moreover, a pruning-based text data augmentation approach is
proposed to enhance focus on essential elements in descriptions, thereby
avoiding excessive model attention to less significant information. The
experimental results show our proposed method outperforms state-of-the-art
methods on three popular benchmark datasets, and the code will be made publicly
available at https://github.com/Delong-liu-bupt/SEN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was withdrawn due to a dispute among the authors regarding
  the content of the article</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-guided Synthetic Geometric Augmentation for Zero-shot 3D
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kohei Torimi, Ryosuke Yamada, Daichi Otsuka, Kensho Hara, Yuki M. Asano, Hirokatsu Kataoka, Yoshimitsu Aoki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot recognition models require extensive training data for
generalization. However, in zero-shot 3D classification, collecting 3D data and
captions is costly and laborintensive, posing a significant barrier compared to
2D vision. Recent advances in generative models have achieved unprecedented
realism in synthetic data production, and recent research shows the potential
for using generated data as training data. Here, naturally raising the
question: Can synthetic 3D data generated by generative models be used as
expanding limited 3D datasets? In response, we present a synthetic 3D dataset
expansion method, Textguided Geometric Augmentation (TeGA). TeGA is tailored
for language-image-3D pretraining, which achieves SoTA in zero-shot 3D
classification, and uses a generative textto-3D model to enhance and extend
limited 3D datasets. Specifically, we automatically generate text-guided
synthetic 3D data and introduce a consistency filtering strategy to discard
noisy samples where semantics and geometric shapes do not match with text. In
the experiment to double the original dataset size using TeGA, our approach
demonstrates improvements over the baselines, achieving zeroshot performance
gains of 3.0% on Objaverse-LVIS, 4.6% on ScanObjectNN, and 8.7% on ModelNet40.
These results demonstrate that TeGA effectively bridges the 3D data gap,
enabling robust zero-shot 3D classification even with limited real training
data and paving the way for zero-shot 3D vision application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SuperNeRF-GAN: A Universal 3D-Consistent Super-Resolution Framework for
  Efficient and Enhanced 3D-Aware Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Zheng, Linzhi Huang, Yizhou Yu, Yi Chang, Yilin Wang, Rui Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural volume rendering techniques, such as NeRF, have revolutionized
3D-aware image synthesis by enabling the generation of images of a single scene
or object from various camera poses. However, the high computational cost of
NeRF presents challenges for synthesizing high-resolution (HR) images. Most
existing methods address this issue by leveraging 2D super-resolution, which
compromise 3D-consistency. Other methods propose radiance manifolds or
two-stage generation to achieve 3D-consistent HR synthesis, yet they are
limited to specific synthesis tasks, reducing their universality. To tackle
these challenges, we propose SuperNeRF-GAN, a universal framework for
3D-consistent super-resolution. A key highlight of SuperNeRF-GAN is its
seamless integration with NeRF-based 3D-aware image synthesis methods and it
can simultaneously enhance the resolution of generated images while preserving
3D-consistency and reducing computational cost. Specifically, given a
pre-trained generator capable of producing a NeRF representation such as
tri-plane, we first perform volume rendering to obtain a low-resolution image
with corresponding depth and normal map. Then, we employ a NeRF
Super-Resolution module which learns a network to obtain a high-resolution
NeRF. Next, we propose a novel Depth-Guided Rendering process which contains
three simple yet effective steps, including the construction of a
boundary-correct multi-depth map through depth aggregation, a normal-guided
depth super-resolution and a depth-guided NeRF rendering. Experimental results
demonstrate the superior efficiency, 3D-consistency, and quality of our
approach. Additionally, ablation studies confirm the effectiveness of our
proposed components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DX2CT: Diffusion Model for 3D CT Reconstruction from Bi or Mono-planar
  2D X-ray(s) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Su Jeong, Hye Bin Yoo, Il Yong Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational tomography (CT) provides high-resolution medical imaging, but
it can expose patients to high radiation. X-ray scanners have low radiation
exposure, but their resolutions are low. This paper proposes a new conditional
diffusion model, DX2CT, that reconstructs three-dimensional (3D) CT volumes
from bi or mono-planar X-ray image(s). Proposed DX2CT consists of two key
components: 1) modulating feature maps extracted from two-dimensional (2D)
X-ray(s) with 3D positions of CT volume using a new transformer and 2)
effectively using the modulated 3D position-aware feature maps as conditions of
DX2CT. In particular, the proposed transformer can provide conditions with rich
information of a target CT slice to the conditional diffusion model, enabling
high-quality CT reconstruction. Our experiments with the bi or mono-planar
X-ray(s) benchmark datasets show that proposed DX2CT outperforms several
state-of-the-art methods. Our codes and model will be available at:
https://www.github.com/intyeger/DX2CT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoRe: Class Patch Attention Needs Regularization for Weakly Supervised
  Semantic Segmentation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11076v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11076v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly Supervised Semantic Segmentation (WSSS) with image-level labels
typically uses Class Activation Maps (CAM) to achieve dense predictions.
Recently, Vision Transformer (ViT) has provided an alternative to generate
localization maps from class-patch attention. However, due to insufficient
constraints on modeling such attention, we observe that the Localization
Attention Maps (LAM) often struggle with the artifact issue, i.e., patch
regions with minimal semantic relevance are falsely activated by class tokens.
In this work, we propose MoRe to address this issue and further explore the
potential of LAM. Our findings suggest that imposing additional regularization
on class-patch attention is necessary. To this end, we first view the attention
as a novel directed graph and propose the Graph Category Representation module
to implicitly regularize the interaction among class-patch entities. It ensures
that class tokens dynamically condense the related patch information and
suppress unrelated artifacts at a graph level. Second, motivated by the
observation that CAM from classification weights maintains smooth localization
of objects, we devise the Localization-informed Regularization module to
explicitly regularize the class-patch attention. It directly mines the token
relations from CAM and further supervises the consistency between class and
patch tokens in a learnable manner. Extensive experiments are conducted on
PASCAL VOC and MS COCO, validating that MoRe effectively addresses the artifact
issue and achieves state-of-the-art performance, surpassing recent single-stage
and even multi-stage methods. Code is available at
https://github.com/zwyang6/MoRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Elucidating the Design Space of Dataset Condensation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13733v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13733v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shitong Shao, Zikai Zhou, Huanran Chen, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset condensation, a concept within data-centric learning, efficiently
transfers critical attributes from an original dataset to a synthetic version,
maintaining both diversity and realism. This approach significantly improves
model training efficiency and is adaptable across multiple application areas.
Previous methods in dataset condensation have faced challenges: some incur high
computational costs which limit scalability to larger datasets (e.g., MTT,
DREAM, and TESLA), while others are restricted to less optimal design spaces,
which could hinder potential improvements, especially in smaller datasets
(e.g., SRe2L, G-VBSM, and RDED). To address these limitations, we propose a
comprehensive design framework that includes specific, effective strategies
like implementing soft category-aware matching and adjusting the learning rate
schedule. These strategies are grounded in empirical evidence and theoretical
backing. Our resulting approach, Elucidate Dataset Condensation (EDC),
establishes a benchmark for both small and large-scale dataset condensation. In
our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on
ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a
compression ratio of 0.78%. This performance exceeds those of SRe2L, G-VBSM,
and RDED by margins of 27.3%, 17.2%, and 6.6%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing small projectors and multiple views for efficient vision
  <span class="highlight-title">pretrain</span>ing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Krishna Agrawal, Arna Ghosh, Shagun Sodhani, Adam Oberman, Blake Richards
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in self-supervised (SSL) visual representation learning has
led to the development of several different proposed frameworks that rely on
augmentations of images but use different loss functions. However, there are
few theoretically grounded principles to guide practice, so practical
implementation of each SSL framework requires several heuristics to achieve
competitive performance. In this work, we build on recent analytical results to
design practical recommendations for competitive and efficient SSL that are
grounded in theory. Specifically, recent theory tells us that existing SSL
frameworks are minimizing the same idealized loss, which is to learn features
that best match the data similarity kernel defined by the augmentations used.
We show how this idealized loss can be reformulated to a functionally
equivalent loss that is more efficient to compute. We study the implicit bias
of using gradient descent to minimize our reformulated loss function and find
that using a stronger orthogonalization constraint with a reduced projector
dimensionality should yield good representations. Furthermore, the theory tells
us that approximating the reformulated loss should be improved by increasing
the number of augmentations, and as such using multiple augmentations should
lead to improved convergence. We empirically verify our findings on CIFAR, STL
and Imagenet datasets, wherein we demonstrate an improved linear readout
performance when training a ResNet-backbone using our theoretically grounded
recommendations. Remarkably, we also demonstrate that by leveraging these
insights, we can reduce the pretraining dataset size by up to 2$\times$ while
maintaining downstream accuracy simply by using more data augmentations. Taken
together, our work provides theoretically grounded recommendations that can be
used to improve SSL convergence and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OPCap:Object-aware <span class="highlight-title">Prompt</span>ing Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiyang Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of image captioning, the phenomenon where missing or nonexistent
objects are used to explain an image is referred to as object bias (or
hallucination). To mitigate this issue, we propose a target-aware prompting
strategy. This method first extracts object labels and their spatial
information from the image using an object detector. Then, an attribute
predictor further refines the semantic features of the objects. These refined
features are subsequently integrated and fed into the decoder, enhancing the
model's understanding of the image context. Experimental results on the COCO
and nocaps datasets demonstrate that OPCap effectively mitigates hallucination
and significantly improves the quality of generated captions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting
  and Planning via World Models for Autonomous Driving <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14197v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14197v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Yang, Jianbiao Mei, Yukai Ma, Siliang Du, Wenqing Chen, Yijie Qian, Yuxiang Feng, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models envision potential future states based on various ego actions.
They embed extensive knowledge about the driving environment, facilitating safe
and scalable autonomous driving. Most existing methods primarily focus on
either data generation or the pretraining paradigms of world models. Unlike the
aforementioned prior works, we propose Drive-OccWorld, which adapts a
vision-centric 4D forecasting world model to end-to-end planning for autonomous
driving. Specifically, we first introduce a semantic and motion-conditional
normalization in the memory module, which accumulates semantic and dynamic
information from historical BEV embeddings. These BEV features are then
conveyed to the world decoder for future occupancy and flow forecasting,
considering both geometry and spatiotemporal modeling. Additionally, we propose
injecting flexible action conditions, such as velocity, steering angle,
trajectory, and commands, into the world model to enable controllable
generation and facilitate a broader range of downstream applications.
Furthermore, we explore integrating the generative capabilities of the 4D world
model with end-to-end planning, enabling continuous forecasting of future
states and the selection of optimal trajectories using an occupancy-based cost
function. Comprehensive experiments conducted on the nuScenes,
nuScenes-Occupancy, and Lyft-Level5 datasets illustrate that our method can
generate plausible and controllable 4D occupancy, paving the way for
advancements in driving world generation and end-to-end planning. Project page:
https://drive-occworld.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Plug-and-Play HIO Approach for Phase Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cagatay Isil, Figen S. Oktem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the phase retrieval problem, the aim is the recovery of an unknown image
from intensity-only measurements such as Fourier intensity. Although there are
several solution approaches, solving this problem is challenging due to its
nonlinear and ill-posed nature. Recently, learning-based approaches have
emerged as powerful alternatives to the analytical methods for several inverse
problems. In the context of phase retrieval, a novel plug-and-play approach
that exploits learning-based prior and efficient update steps has been
presented at the Computational Optical Sensing and Imaging topical meeting,
with demonstrated state-of-the-art performance. The key idea was to incorporate
learning-based prior to the Gerchberg-Saxton type algorithms through
plug-and-play regularization. In this paper, we present the mathematical
development of the method including the derivation of its analytical update
steps based on half-quadratic splitting and comparatively evaluate its
performance through extensive simulations on a large test dataset. The results
show the effectiveness of the method in terms of both image quality,
computational efficiency, and robustness to initialization and noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction-Guided Fusion of Multi-Layer Visual Features in Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08443v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08443v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Li, Yi Zheng, Haotian Chen, Xiaolei Chen, Yuxuan Liang, Chenghang Lai, Bin Li, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have achieved remarkable success in a
wide range of multimodal tasks by integrating pre-trained vision encoders and
large language models. However, current LVLMs primarily rely on visual features
extracted from the final layers of the vision encoder, overlooking the
complementary information available in shallower layers. While recent
approaches have explored the use of multilayer visual features in LVLMs, they
tend to be task-agnostic and fail to examine the dependencies of hierarchical
visual features on specific tasks. To address these gaps, we systematically
investigate the contributions of visual features from different encoder layers
using 18 benchmarks spanning 6 task categories. Our findings reveal that
multilayer features provide complementary strengths with varying task
dependencies, and uniform fusion leads to suboptimal performance. Building on
these insights, we propose the instruction-guided vision aggregator, a module
that dynamically integrates multi-layer visual features based on textual
instructions, without increasing the number of visual tokens. Extensive
evaluations demonstrate the superior performance of our method. Additionally,
an in-depth analysis of the aggregator's behavior highlights the dominance of
mid-to-high-level features in semantic-rich tasks and the critical role of
low-level features in fine-grained perception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Myriad: Large Multimodal Model by Applying Vision Experts for Industrial
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19070v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19070v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanze Li, Haolin Wang, Shihao Yuan, Ming Liu, Debin Zhao, Yiwen Guo, Chen Xu, Guangming Shi, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the training configuration, traditional industrial anomaly detection
(IAD) methods have to train a specific model for each deployment scenario,
which is insufficient to meet the requirements of modern design and
manufacturing. On the contrary, large multimodal models~(LMMs) have shown
eminent generalization ability on various vision tasks, and their perception
and comprehension capabilities imply the potential of applying LMMs on IAD
tasks. However, we observe that even though the LMMs have abundant knowledge
about industrial anomaly detection in the textual domain, the LMMs are unable
to leverage the knowledge due to the modality gap between textual and visual
domains. To stimulate the relevant knowledge in LMMs and adapt the LMMs towards
anomaly detection tasks, we introduce existing IAD methods as vision experts
and present a novel large multimodal model applying vision experts for
industrial anomaly detection~(abbreviated to {Myriad}). Specifically, we
utilize the anomaly map generated by the vision experts as guidance for LMMs,
such that the vision model is guided to pay more attention to anomalous
regions. Then, the visual features are modulated via an adapter to fit the
anomaly detection tasks, which are fed into the language model together with
the vision expert guidance and human instructions to generate the final
outputs. Extensive experiments are applied on MVTec-AD, VisA, and PCB Bank
benchmarks demonstrate that our proposed method not only performs favorably
against state-of-the-art methods, but also inherits the flexibility and
instruction-following ability of LMMs in the field of IAD. Source code and
pre-trained models are publicly available at
\url{https://github.com/tzjtatata/Myriad}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TraceFL: Interpretability-Driven Debugging in Federated Learning via
  Neuron Provenance <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13632v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13632v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waris Gill, Ali Anwar, Muhammad Ali Gulzar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Federated Learning, clients train models on local data and send updates to
a central server, which aggregates them into a global model using a fusion
algorithm. This collaborative yet privacy-preserving training comes at a cost.
FL developers face significant challenges in attributing global model
predictions to specific clients. Localizing responsible clients is a crucial
step towards (a) excluding clients primarily responsible for incorrect
predictions and (b) encouraging clients who contributed high-quality models to
continue participating in the future. Existing ML debugging approaches are
inherently inapplicable as they are designed for single-model, centralized
training.
  We introduce TraceFL, a fine-grained neuron provenance capturing mechanism
that identifies clients responsible for a global model's prediction by tracking
the flow of information from individual clients to the global model. Since
inference on different inputs activates a different set of neurons of the
global model, TraceFL dynamically quantifies the significance of the global
model's neurons in a given prediction, identifying the most crucial neurons in
the global model. It then maps them to the corresponding neurons in every
participating client to determine each client's contribution, ultimately
localizing the responsible client. We evaluate TraceFL on six datasets,
including two real-world medical imaging datasets and four neural networks,
including advanced models such as GPT. TraceFL achieves 99% accuracy in
localizing the responsible client in FL tasks spanning both image and text
classification tasks. At a time when state-of-the-artML debugging approaches
are mostly domain-specific (e.g., image classification only), TraceFL is the
first technique to enable highly accurate automated reasoning across a wide
range of FL applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2025 IEEE/ACM 47th International Conference on Software
  Engineering (ICSE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Epicardium <span class="highlight-title">Prompt</span>-guided Real-time Cardiac Ultrasound Frame-to-volume
  Registration <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14534v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14534v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Lei, Jun Zhou, Jialun Pei, Baoliang Zhao, Yueming Jin, Yuen-Chun Jeremy Teoh, Jing Qin, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A comprehensive guidance view for cardiac interventional surgery can be
provided by the real-time fusion of the intraoperative 2D images and
preoperative 3D volume based on the ultrasound frame-to-volume registration.
However, cardiac ultrasound images are characterized by a low signal-to-noise
ratio and small differences between adjacent frames, coupled with significant
dimension variations between 2D frames and 3D volumes to be registered,
resulting in real-time and accurate cardiac ultrasound frame-to-volume
registration being a very challenging task. This paper introduces a lightweight
end-to-end Cardiac Ultrasound frame-to-volume Registration network, termed
CU-Reg. Specifically, the proposed model leverages epicardium prompt-guided
anatomical clues to reinforce the interaction of 2D sparse and 3D dense
features, followed by a voxel-wise local-global aggregation of enhanced
features, thereby boosting the cross-dimensional matching effectiveness of
low-quality ultrasound modalities. We further embed an inter-frame
discriminative regularization term within the hybrid supervised learning to
increase the distinction between adjacent slices in the same ultrasound volume
to ensure registration stability. Experimental results on the reprocessed CAMUS
dataset demonstrate that our CU-Reg surpasses existing methods in terms of
registration accuracy and efficiency, meeting the guidance requirements of
clinical cardiac interventional surgery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by MICCAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeuManifold: Neural Watertight Manifold Reconstruction with Efficient
  and High-Quality Rendering Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17134v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17134v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyue Wei, Fanbo Xiang, Sai Bi, Anpei Chen, Kalyan Sunkavalli, Zexiang Xu, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for generating high-quality watertight manifold meshes
from multi-view input images. Existing volumetric rendering methods are robust
in optimization but tend to generate noisy meshes with poor topology.
Differentiable rasterization-based methods can generate high-quality meshes but
are sensitive to initialization. Our method combines the benefits of both
worlds; we take the geometry initialization obtained from neural volumetric
fields, and further optimize the geometry as well as a compact neural texture
representation with differentiable rasterizers. Through extensive experiments,
we demonstrate that our method can generate accurate mesh reconstructions with
faithful appearance that are comparable to previous volume rendering methods
while being an order of magnitude faster in rendering. We also show that our
generated mesh and neural texture reconstruction is compatible with existing
graphics pipelines and enables downstream 3D applications such as simulation.
Project page: https://sarahweiii.github.io/neumanifold/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://sarahweiii.github.io/neumanifold/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Jena, Pratik Chaudhari, James C. Gee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper proposes FireANTs, the first multi-scale Adaptive Riemannian
Optimization algorithm for dense diffeomorphic image matching. One of the most
critical and understudied aspects of diffeomorphic image matching algorithms
are its highly ill-conditioned nature. We quantitatively capture the extent of
ill-conditioning in a typical MRI matching task, motivating the need for an
adaptive optimization algorithm for diffeomorphic matching. To this end,
FireANTs generalizes the concept of momentum and adaptive estimates of the
Hessian to mitigate this ill-conditioning in the non-Euclidean space of
diffeomorphisms. Unlike common non-Euclidean manifolds, we also formalize
considerations for multi-scale optimization of diffeomorphisms. Our rigorous
mathematical results and operational contributions lead to a state-of-the-art
dense matching algorithm that can be applied to generic image data with
remarkable accuracy and robustness. We demonstrate consistent improvements in
image matching performance across a spectrum of community-standard medical and
biological correspondence matching challenges spanning a wide variety of image
modalities, anatomies, resolutions, acquisition protocols, and preprocessing
pipelines. This improvement is supplemented by from 300x up to 3200x speedup
over existing state-of-the-art algorithms. For the first time, we perform
diffeomorphic matching of sub-micron mouse cortex volumes at native resolution.
Our fast implementation also enables hyperparameter studies that were
intractable with existing correspondence matching algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learnable Scaled Gradient Descent for Guaranteed Robust Tensor PCA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanlan Feng, Ce Zhu, Yipeng Liu, Saiprasad Ravishankar, Longxiu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust tensor principal component analysis (RTPCA) aims to separate the
low-rank and sparse components from multi-dimensional data, making it an
essential technique in the signal processing and computer vision fields.
Recently emerging tensor singular value decomposition (t-SVD) has gained
considerable attention for its ability to better capture the low-rank structure
of tensors compared to traditional matrix SVD. However, existing methods often
rely on the computationally expensive tensor nuclear norm (TNN), which limits
their scalability for real-world tensors. To address this issue, we explore an
efficient scaled gradient descent (SGD) approach within the t-SVD framework for
the first time, and propose the RTPCA-SGD method. Theoretically, we rigorously
establish the recovery guarantees of RTPCA-SGD under mild assumptions,
demonstrating that with appropriate parameter selection, it achieves linear
convergence to the true low-rank tensor at a constant rate, independent of the
condition number. To enhance its practical applicability, we further propose a
learnable self-supervised deep unfolding model, which enables effective
parameter learning. Numerical experiments on both synthetic and real-world
datasets demonstrate the superior performance of the proposed methods while
maintaining competitive computational efficiency, especially consuming less
time than RTPCA-TNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering <span class="highlight-title">Large Language Model</span> for Continual Video Question Answering
  with Collaborative <span class="highlight-title">Prompt</span>ing <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Cai, Zheng Wang, Jianjun Gao, Wenyang Liu, Ye Lu, Runzhong Zhang, Kim-Hui Yap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the rapid increase in online video content has underscored
the limitations of static Video Question Answering (VideoQA) models trained on
fixed datasets, as they struggle to adapt to new questions or tasks posed by
newly available content. In this paper, we explore the novel challenge of
VideoQA within a continual learning framework, and empirically identify a
critical issue: fine-tuning a large language model (LLM) for a sequence of
tasks often results in catastrophic forgetting. To address this, we propose
Collaborative Prompting (ColPro), which integrates specific question constraint
prompting, knowledge acquisition prompting, and visual temporal awareness
prompting. These prompts aim to capture textual question context, visual
content, and video temporal dynamics in VideoQA, a perspective underexplored in
prior research. Experimental results on the NExT-QA and DramaQA datasets show
that ColPro achieves superior performance compared to existing approaches,
achieving 55.14\% accuracy on NExT-QA and 71.24\% accuracy on DramaQA,
highlighting its practical relevance and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by main EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IOR: Inversed Objects Replay for Incremental Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04829v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04829v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijia An, Boyu Diao, Libo Huang, Ruiqi Liu, Zhulin An, Yongjun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Incremental Object Detection (IOD) methods partially alleviate
catastrophic forgetting when incrementally detecting new objects in real-world
scenarios. However, many of these methods rely on the assumption that unlabeled
old-class objects may co-occur with labeled new-class objects in the
incremental data. When unlabeled old-class objects are absent, the performance
of existing methods tends to degrade. The absence can be mitigated by
generating old-class samples, but it incurs high costs. This paper argues that
previous generation-based IOD suffers from redundancy, both in the use of
generative models, which require additional training and storage, and in the
overproduction of generated samples, many of which do not contribute
significantly to performance improvements. To eliminate the redundancy, we
propose Inversed Objects Replay (IOR). Specifically, we generate old-class
samples by inversing the original detectors, thus eliminating the necessity of
training and storing additional generative models. We propose augmented replay
to reuse the objects in generated samples, reducing redundant generations.
Moreover, we propose high-value knowledge distillation focusing on the
positions of old-class objects overwhelmed by the background, which transfers
the knowledge to the incremental detector. Extensive experiments conducted on
MS COCO 2017 demonstrate that our method can efficiently improve detection
performance in IOD scenarios with the absence of old-class objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenge Summary U-MedSAM: Uncertainty-aware MedSAM for Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08881v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08881v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Xiaoyu Liu, Peng Huang, Pu Huang, Shu Hu, Hongtu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical Image Foundation Models have proven to be powerful tools for mask
prediction across various datasets. However, accurately assessing the
uncertainty of their predictions remains a significant challenge. To address
this, we propose a new model, U-MedSAM, which integrates the MedSAM model with
an uncertainty-aware loss function and the Sharpness-Aware Minimization
(SharpMin) optimizer. The uncertainty-aware loss function automatically
combines region-based, distribution-based, and pixel-based loss designs to
enhance segmentation accuracy and robustness. SharpMin improves generalization
by finding flat minima in the loss landscape, thereby reducing overfitting. Our
method was evaluated in the CVPR24 MedSAM on Laptop challenge, where U-MedSAM
demonstrated promising performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2405.17496</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning <span class="chip">NeurIPS
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07227v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07227v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tieyuan Chen, Huabin Liu, Yi Wang, Yihang Chen, Tianyao He, Chaofan Gan, Huanyu He, Weiyao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video causal reasoning aims to achieve a high-level understanding of videos
from a causal perspective. However, it exhibits limitations in its scope,
primarily executed in a question-answering paradigm and focusing on brief video
segments containing isolated events and basic causal relations, lacking
comprehensive and structured causality analysis for videos with multiple
interconnected events. To fill this gap, we introduce a new task and dataset,
Multi-Event Causal Discovery (MECD). It aims to uncover the causal relations
between events distributed chronologically across long videos. Given visual
segments and textual descriptions of events, MECD identifies the causal
associations between these events to derive a comprehensive and structured
event-level video causal graph explaining why and how the result event
occurred. To address the challenges of MECD, we devise a novel framework
inspired by the Granger Causality method, incorporating an efficient mask-based
event prediction model to perform an Event Granger Test. It estimates causality
by comparing the predicted result event when premise events are masked versus
unmasked. Furthermore, we integrate causal inference techniques such as
front-door adjustment and counterfactual inference to mitigate challenges in
MECD like causality confounding and illusory causality. Additionally, context
chain reasoning is introduced to conduct more robust and generalized reasoning.
Experiments validate the effectiveness of our framework in reasoning complete
causal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,
respectively. Further experiments demonstrate that causal relation graphs can
also contribute to downstream video understanding tasks such as video question
answering and video event prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TPAMI Submission. continuous work of arXiv:2409.17647 (NeurIPS
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Language Driven Slice Discovery and Error Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07832v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07832v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam Visweswaran, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error slice discovery is crucial to diagnose and mitigate model errors.
Current clustering or discrete attribute-based slice discovery methods face key
limitations: 1) clustering results in incoherent slices, while assigning
discrete attributes to slices leads to incomplete coverage of error patterns
due to missing or insufficient attributes; 2) these methods lack complex
reasoning, preventing them from fully explaining model biases; 3) they fail to
integrate \textit{domain knowledge}, limiting their usage in specialized fields
\eg radiology. We propose\ladder (\underline{La}nguage-\underline{D}riven
\underline{D}iscovery and \underline{E}rror \underline{R}ectification), to
address the limitations by: (1) leveraging the flexibility of natural language
to address incompleteness, (2) employing LLM's latent \textit{domain knowledge}
and advanced reasoning to analyze sentences and derive testable hypotheses
directly, identifying biased attributes, and form coherent error slices without
clustering. Existing mitigation methods typically address only the
worst-performing group, often amplifying errors in other subgroups. In
contrast,\ladder generates pseudo attributes from the discovered hypotheses to
mitigate errors across all biases without explicit attribute annotations or
prior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural
and medical images -- comparing 200+ classifiers with diverse architectures,
pretraining strategies, and LLMs -- show that\ladder consistently outperforms
existing baselines in discovering and mitigating biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaNeRV: Meta Neural Representations for Videos with Spatial-Temporal
  Guidance <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialong Guo, Ke liu, Jiangchao Yao, Zhihua Wang, Jiajun Bu, Haishuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Representations for Videos (NeRV) has emerged as a promising implicit
neural representation (INR) approach for video analysis, which represents
videos as neural networks with frame indexes as inputs. However, NeRV-based
methods are time-consuming when adapting to a large number of diverse videos,
as each video requires a separate NeRV model to be trained from scratch. In
addition, NeRV-based methods spatially require generating a high-dimension
signal (i.e., an entire image) from the input of a low-dimension timestamp, and
a video typically consists of tens of frames temporally that have a minor
change between adjacent frames. To improve the efficiency of video
representation, we propose Meta Neural Representations for Videos, named
MetaNeRV, a novel framework for fast NeRV representation for unseen videos.
MetaNeRV leverages a meta-learning framework to learn an optimal parameter
initialization, which serves as a good starting point for adapting to new
videos. To address the unique spatial and temporal characteristics of video
modality, we further introduce spatial-temporal guidance to improve the
representation capabilities of MetaNeRV. Specifically, the spatial guidance
with a multi-resolution loss aims to capture the information from different
resolution stages, and the temporal guidance with an effective progressive
learning strategy could gradually refine the number of fitted frames during the
meta-learning process. Extensive experiments conducted on multiple datasets
demonstrate the superiority of MetaNeRV for video representations and video
compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keep It Accurate and Robust: An Enhanced Nuclei Analysis Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.03415v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.03415v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhua Zhang, Sen Yang, Meiwei Luo, Chuan He, Yuchen Li, Jun Zhang, Xiyue Wang, Fang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation and classification of nuclei in histology images is
critical but challenging due to nuclei heterogeneity, staining variations, and
tissue complexity. Existing methods often struggle with limited dataset
variability, with patches extracted from similar whole slide images (WSI),
making models prone to falling into local optima. Here we propose a new
framework to address this limitation and enable robust nuclear analysis. Our
method leverages dual-level ensemble modeling to overcome issues stemming from
limited dataset variation. Intra-ensembling applies diverse transformations to
individual samples, while inter-ensembling combines networks of different
scales. We also introduce enhancements to the HoVer-Net architecture, including
updated encoders, nested dense decoding and model regularization strategy. We
achieve state-of-the-art results on public benchmarks, including 1st place for
nuclear composition prediction and 3rd place for segmentation/classification in
the 2022 Colon Nuclei Identification and Counting (CoNIC) Challenge. This
success validates our approach for accurate histological nuclei analysis.
Extensive experiments and ablation studies provide insights into optimal
network design choices and training techniques. In conclusion, this work
proposes an improved framework advancing the state-of-the-art in nuclei
analysis. We release our code and models
(https://github.com/WinnieLaugh/CONIC_Pathology_AI) to serve as a toolkit for
the community.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perception of Visual Variables on Virtual Wall-Sized Tiled Displays in
  Immersive Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyun Han, Anastasia Bezerianos, Petra Isenberg, Isaac Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the perception of visual variables on wall-sized tiled
displays within an immersive environment. We designed and conducted two formal
user studies focusing on elementary visualization reading tasks in VR. The
first study compared three different virtual display arrangements (Flat,
Cylinder, and Cockpit). It showed that participants made smaller errors on
virtual curved walls (Cylinder and Cockpit) compared to Flat. Following that,
we compared the results with those from a previous study conducted in a
real-world setting. The comparative analysis showed that virtual curved walls
resulted in smaller errors than the real-world flat wall display, but with
longer task completion time. The second study evaluated the impact of four 3D
user interaction techniques (Selection, Walking, Steering, and Teleportation)
on performing the elementary task on the virtual Flat wall display. The results
confirmed that interaction techniques further improved task performance.
Finally, we discuss the limitations and future work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design Patterns for the Common Good: Building Better Technologies Using
  the Wisdom of Virtue Ethics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Louisa Conwill, Megan K. Levis, Karla Badillo-Urquiola, Walter J. Scheirer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtue ethics is a philosophical tradition that emphasizes the cultivation of
virtues in achieving the common good. It has been suggested to be an effective
framework for envisioning more ethical technology, yet previous work on virtue
ethics and technology design has remained at theoretical recommendations.
Therefore, we propose an approach for identifying user experience design
patterns that embody particular virtues to more concretely articulate virtuous
technology designs. As a proof of concept for our approach, we documented seven
design patterns for social media that uphold the virtues of Catholic Social
Teaching. We interviewed 24 technology researchers and industry practitioners
to evaluate these patterns. We found that overall the patterns enact the
virtues they were identified to embody; our participants valued that the
patterns fostered intentional conversations and personal connections. We pave a
path for technology professionals to incorporate diverse virtue traditions into
the development of technologies that support human flourishing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling High-dimensional Backstage: A <span class="highlight-title">Survey</span> for Reliable Visual
  Analytics with Dimensionality Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeon Jeon, Hyunwook Lee, Yun-Hsin Kuo, Taehyun Yang, Daniel Archambault, Sungahn Ko, Takanori Fujiwara, Kwan-Liu Ma, Jinwook Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction (DR) techniques are essential for visually analyzing
high-dimensional data. However, visual analytics using DR often face
unreliability, stemming from factors such as inherent distortions in DR
projections. This unreliability can lead to analytic insights that misrepresent
the underlying data, potentially resulting in misguided decisions. To tackle
these reliability challenges, we review 133 papers that address the
unreliability of visual analytics using DR. Through this review, we contribute
(1) a workflow model that describes the interaction between analysts and
machines in visual analytics using DR, and (2) a taxonomy that identifies where
and why reliability issues arise within the workflow, along with existing
solutions for addressing them. Our review reveals ongoing challenges in the
field, whose significance and urgency are validated by five expert researchers.
This review also finds that the current research landscape is skewed toward
developing new DR techniques rather than their interpretation or evaluation,
where we discuss how the HCI community can contribute to broadening this focus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the SIGCHI Conference on Human Factors in Computing
  Systems (CHI '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Exploration of Stopword Probabilities in Topic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangjiang Xue, Pierre Le Bras, David A. Robb, Mike J. Chantler, Stefano Padilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stopword removal is a critical stage in many Machine Learning methods but
often receives little consideration, it interferes with the model
visualizations and disrupts user confidence. Inappropriately chosen or hastily
omitted stopwords not only lead to suboptimal performance but also
significantly affect the quality of models, thus reducing the willingness of
practitioners and stakeholders to rely on the output visualizations. This paper
proposes a novel extraction method that provides a corpus-specific
probabilistic estimation of stopword likelihood and an interactive
visualization system to support their analysis. We evaluated our approach and
interface using real-world data, a commonly used Machine Learning method (Topic
Modelling), and a comprehensive qualitative experiment probing user confidence.
The results of our work show that our system increases user confidence in the
credibility of topic models by (1) returning reasonable probabilities, (2)
generating an appropriate and representative extension of common stopword
lists, and (3) providing an adjustable threshold for estimating and analyzing
stopwords visually. Finally, we discuss insights, recommendations, and best
practices to support practitioners while improving the output of Machine
Learning methods and topic model visualizations with robust stopword analysis
and removal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Impact of Generative Artificial Intelligence in Education:
  A Thematic Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Kaushik, Sargam Yadav, Andrew Browne, David Lillis, David Williams, Jack Mc Donnell, Peadar Grant, Siobhan Connolly Kernan, Shubham Sharma, Mansi Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in Generative Artificial intelligence (GenAI)
technology have been transformative for the field of education. Large Language
Models (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate
tasks, create content for personalised teaching, and handle repetitive tasks to
allow more time for creative thinking. However, it is important to develop
guidelines, policies, and assessment methods in the education sector to ensure
the responsible integration of these tools. In this article, thematic analysis
has been performed on seven essays obtained from professionals in the education
sector to understand the advantages and pitfalls of using GenAI models such as
ChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been
performed on the essays to extract further insights from the text. The study
found several themes which highlight benefits and drawbacks of GenAI tools, as
well as suggestions to overcome these limitations and ensure that students are
using these tools in a responsible and ethical manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do Programming Students Use Generative AI? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Rahe, Walid Maalej
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programming students have a widespread access to powerful Generative AI tools
like ChatGPT. While this can help understand the learning material and assist
with exercises, educators are voicing more and more concerns about an
over-reliance on generated outputs and lack of critical thinking skills. It is
thus important to understand how students actually use generative AI and what
impact this could have on their learning behavior. To this end, we conducted a
study including an exploratory experiment with 37 programming students, giving
them monitored access to ChatGPT while solving a code understanding and
improving exercise. While only 23 of the students actually opted to use the
chatbot, the majority of those eventually prompted it to simply generate a full
solution. We observed two prevalent usage strategies: to seek knowledge about
general concepts and to directly generate solutions. Instead of using the bot
to comprehend the code and their own mistakes, students often got trapped in a
vicious cycle of submitting wrong generated code and then asking the bot for a
fix. Those who self-reported using generative AI regularly were more likely to
prompt the bot to generate a solution. Our findings indicate that concerns
about potential decrease in programmers' agency and productivity with
Generative AI are justified. We discuss how researchers and educators can
respond to the potential risk of students uncritically over-relying on
generative AI. We also discuss potential modifications to our study design for
large-scale replications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint; accepted to ACM International Conference on the Foundations
  of Software Engineering (FSE) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discord's Design Encourages "Third Place" Social Media Experiences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JaeWon Kim, Thea Klein-Balajee, Ryan M. Kelly, Alexis Hiniker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In light of the diminishing presence of physical third places -- informal
gathering spaces essential for social connection -- this study explores how the
social media platform Discord fosters third-place experiences. Drawing on
Oldenburg's conceptual framework, we analyze how Discord's design elements
support the creation of virtual third places that foster both dyadic and
community-based relationships. Through 25 semi-structured interviews with
active Discord users, we identified 21 design elements aligned with Oldenburg's
third-place characteristics. These elements cluster around four core
principles: providing themed spaces for repeated interactions, supporting user
autonomy and customization, facilitating mutually engaging activities, and
enabling casual, low-pressure interactions. This work contributes to
understanding how intentional platform design can cultivate virtual spaces that
support meaningful social connections. The findings have implications for
designing future social technologies that can help address growing concerns
about social isolation in an increasingly digital world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeamVision: An AI-powered Learning Analytics System for Supporting
  Reflection in Team-based Healthcare Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vanessa Echeverria, Linxuan Zhao, Riordan Alfredo, Mikaela Milesi, Yuequiao Jin, Sophie Abel, Jie Yan, Lixiang Yan, Xinyu Li, Samantha Dix, Rosie Wotherspoon, Hollie Jaggard, Abra Osborne, Simon Buckingham Shum, Dragan Gasevic, Roberto Martinez-Maldonado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Healthcare simulations help learners develop teamwork and clinical skills in
a risk-free setting, promoting reflection on real-world practices through
structured debriefs. However, despite video's potential, it is hard to use,
leaving a gap in providing concise, data-driven summaries for supporting
effective debriefing. Addressing this, we present TeamVision, an AI-powered
multimodal learning analytics (MMLA) system that captures voice presence,
automated transcriptions, body rotation, and positioning data, offering
educators a dashboard to guide debriefs immediately after simulations. We
conducted an in-the-wild study with 56 teams (221 students) and recorded
debriefs led by six teachers using TeamVision. Follow-up interviews with 15
students and five teachers explored perceptions of its usefulness, accuracy,
and trustworthiness. This paper examines: i) how TeamVision was used in
debriefing, ii) what educators found valuable and challenging, and iii)
perceptions of its effectiveness. Results suggest TeamVision enables flexible
debriefing and highlights the challenges and implications of using AI-powered
systems in healthcare simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CHI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chatbot apologies: Beyond bullshit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        P. D. Magnus, Alessandra Buccella, Jason D'Cruz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Apologies serve essential functions for moral agents such as expressing
remorse, taking responsibility, and repairing trust. LLM-based chatbots
routinely produce output that has the linguistic form of an apology. However,
they do this simply because they are echoing the kinds of things that humans
say. Moreover, there are reasons to think that chatbots are not the kind of
linguistic or moral agents capable of apology. To put the point bluntly:
Chatbot apologies are bullshit. This paper offers several arguments for this
conclusion, drawing on the nature of morally-serious apologies, the linguistic
agency required to perform them, and the moral agency required for them to
matter. We conclude by considering some consequences for how chatbots should be
designed and how we ought to think about them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptions of Blind Adults on Non-Visual Mobile Text Entry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Gaines, Keith Vertanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text input on mobile devices without physical keys can be challenging for
people who are blind or low-vision. We interview 12 blind adults about their
experiences with current mobile text input to provide insights into what sorts
of interface improvements may be the most beneficial. We identify three primary
themes that were experiences or opinions shared by participants: the poor
accuracy of dictation, difficulty entering text in noisy environments, and
difficulty correcting errors in entered text. We also discuss an experimental
non-visual text input method with each participant to solicit opinions on the
method and probe their willingness to learn a novel method. We find that the
largest concern was the time required to learn a new technique. We find that
the majority of our participants do not use word predictions while typing but
instead find it faster to finish typing words manually. Finally, we distill
five future directions for non-visual text input: improved dictation, less
reliance on or improved audio feedback, improved error correction, reducing the
barrier to entry for new methods, and more fluid non-visual word predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nods of Agreement: Webcam-Driven Avatars Improve Meeting Outcomes and
  Avatar Satisfaction Over Audio-Driven or Static Avatars in All-Avatar Work
  Videoconferencing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13265v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13265v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang Ma, Ju Zhang, Lev Tankelevitch, Payod Panda, Torang Asadi, Charlie Hewitt, Lohit Petikam, James Clemoes, Marco Gillies, Xueni Pan, Sean Rintel, Marta Wilczkowiak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Avatars are edging into mainstream videoconferencing, but evaluation of how
avatar animation modalities contribute to work meeting outcomes has been
limited. We report a within-group videoconferencing experiment in which 68
employees of a global technology company, in 16 groups, used the same stylized
avatars in three modalities (static picture, audio-animation, and
webcam-animation) to complete collaborative decision-making tasks.
Quantitatively, for meeting outcomes, webcam-animated avatars improved meeting
effectiveness over the picture modality and were also reported to be more
comfortable and inclusive than both other modalities. In terms of avatar
satisfaction, there was a similar preference for webcam animation as compared
to both other modalities. Our qualitative analysis shows participants
expressing a preference for the holistic motion of webcam animation, and that
meaningful movement outweighs realism for meeting outcomes, as evidenced
through a systematic overview of ten thematic factors. We discuss implications
for research and commercial deployment and conclude that webcam-animated
avatars are a plausible alternative to video in work meetings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in PACM HCI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Redefining Affordance via Computational Rationality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09233v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09233v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Chi Liao, Christian Holz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affordances, a foundational concept in human-computer interaction and design,
have traditionally been explained by direct-perception theories, which assume
that individuals perceive action possibilities directly from the environment.
However, these theories fall short of explaining how affordances are perceived,
learned, refined, or misperceived, and how users choose between multiple
affordances in dynamic contexts. This paper introduces a novel affordance
theory grounded in Computational Rationality, positing that humans construct
internal representations of the world based on bounded sensory inputs. Within
these internal models, affordances are inferred through two core mechanisms:
feature recognition and hypothetical motion trajectories. Our theory redefines
affordance perception as a decision-making process, driven by two components:
confidence (the perceived likelihood of successfully executing an action) and
predicted utility (the expected value of the outcome). By balancing these
factors, individuals make informed decisions about which actions to take. Our
theory frames affordances perception as dynamic, continuously learned, and
refined through reinforcement and feedback. We validate the theory via thought
experiments and demonstrate its applicability across diverse types of
affordances (e.g., physical, digital, social). Beyond clarifying and
generalizing the understanding of affordances across contexts, our theory
serves as a foundation for improving design communication and guiding the
development of more adaptive and intuitive systems that evolve with user
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IUI 2025 Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XEQ Scale for Evaluating XAI Experience Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10662v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10662v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjana Wijekoon, Nirmalie Wiratunga, David Corsar, Kyle Martin, Ikechukwu Nkisi-Orji, Belen Díaz-Agudo, Derek Bridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable Artificial Intelligence (XAI) aims to improve the transparency of
autonomous decision-making through explanations. Recent literature has
emphasised users' need for holistic "multi-shot" explanations and personalised
engagement with XAI systems. We refer to this user-centred interaction as an
XAI Experience. Despite advances in creating XAI experiences, evaluating them
in a user-centred manner has remained challenging. In response, we developed
the XAI Experience Quality (XEQ) Scale. XEQ quantifies the quality of
experiences across four dimensions: learning, utility, fulfilment and
engagement. These contributions extend the state-of-the-art of XAI evaluation,
moving beyond the one-dimensional metrics frequently developed to assess
single-shot explanations. This paper presents the XEQ scale development and
validation process, including content validation with XAI experts, and
discriminant and construct validation through a large-scale pilot study. Our
pilot study results offer strong evidence that establishes the XEQ Scale as a
comprehensive framework for evaluating user-centred XAI experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement Learning from Human Feedback: Whose Culture, Whose Values,
  Whose Perspectives? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristian González Barman, Simon Lohse, Henk de Regt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We argue for the epistemic and ethical advantages of pluralism in
Reinforcement Learning from Human Feedback (RLHF) in the context of Large
Language Models (LLM). Drawing on social epistemology and pluralist philosophy
of science, we suggest ways in which RHLF can be made more responsive to human
needs and how we can address challenges along the way. The paper concludes with
an agenda for change, i.e. concrete, actionable steps to improve LLM
development.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">118</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Credit Risk Identification in Supply Chains Using Generative Adversarial
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhou Zhang, Xinshi Li, Yu Cheng, Zhenrui Chen, Qianying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Credit risk management within supply chains has emerged as a critical
research area due to its significant implications for operational stability and
financial sustainability. The intricate interdependencies among supply chain
participants mean that credit risks can propagate across networks, with impacts
varying by industry. This study explores the application of Generative
Adversarial Networks (GANs) to enhance credit risk identification in supply
chains. GANs enable the generation of synthetic credit risk scenarios,
addressing challenges related to data scarcity and imbalanced datasets. By
leveraging GAN-generated data, the model improves predictive accuracy while
effectively capturing dynamic and temporal dependencies in supply chain data.
The research focuses on three representative industries-manufacturing (steel),
distribution (pharmaceuticals), and services (e-commerce) to assess
industry-specific credit risk contagion. Experimental results demonstrate that
the GAN-based model outperforms traditional methods, including logistic
regression, decision trees, and neural networks, achieving superior accuracy,
recall, and F1 scores. The findings underscore the potential of GANs in
proactive risk management, offering robust tools for mitigating financial
disruptions in supply chains. Future research could expand the model by
incorporating external market factors and supplier relationships to further
enhance predictive capabilities. Keywords- Generative Adversarial Networks
(GANs); Supply Chain Risk; Credit Risk Identification; Machine Learning; Data
Augmentation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper will be published and indexed by IEEE at 2025 8th
  International Conference on Advanced Algorithms and Control Engineering
  (ICAACE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ColNet: Collaborative Optimization in Decentralized Federated Multi-task
  Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Feng, Nicolas Fazli Kohler, Alberto Huertas Celdran, Gerome Bovet, Burkhard Stiller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Federated Learning (FL) and Multi-Task Learning (MTL) has
been explored to address client heterogeneity, with Federated Multi-Task
Learning (FMTL) treating each client as a distinct task. However, most existing
research focuses on data heterogeneity (e.g., addressing non-IID data) rather
than task heterogeneity, where clients solve fundamentally different tasks.
Additionally, much of the work relies on centralized settings with a server
managing the federation, leaving the more challenging domain of decentralized
FMTL largely unexplored. Thus, this work bridges this gap by proposing ColNet,
a framework designed for heterogeneous tasks in decentralized federated
environments. ColNet divides models into the backbone and task-specific layers,
forming groups of similar clients, with group leaders performing
conflict-averse cross-group aggregation. A pool of experiments with different
federations demonstrated ColNet outperforms the compared aggregation schemes in
decentralized settings with label and task heterogeneity scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Deep Learning Model for epileptic seizure classification by using
  1D-CNN with multi-head attention mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Guhdar, Ramadhan J. Mstafa, Abdulhakeem O. Mohammed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Epilepsy is a prevalent neurological disorder globally, impacting around 50
million people \cite{WHO_epilepsy_50million}. Epileptic seizures result from
sudden abnormal electrical activity in the brain, which can be read as sudden
and significant changes in the EEG signal of the brain. The signal can vary in
severity and frequency, which results in loss of consciousness and muscle
contractions for a short period of time \cite{epilepsyfoundation_myoclonic}.
Individuals with epilepsy often face significant employment challenges due to
safety concerns in certain work environments. Many jobs that involve working at
heights, operating heavy machinery, or in other potentially hazardous settings
may be restricted for people with seizure disorders. This certainly limits job
options and economic opportunities for those living with epilepsy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Fashion Products Performance Forecasting: A <span class="highlight-title">Survey</span> on Evolutions,
  Models and Emerging Trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Avogaro, Luigi Capogrosso, Andrea Toaiari, Franco Fummi, Marco Cristani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fast fashion industry's insatiable demand for new styles and rapid
production cycles has led to a significant environmental burden.
Overproduction, excessive waste, and harmful chemicals have contributed to the
negative environmental impact of the industry. To mitigate these issues, a
paradigm shift that prioritizes sustainability and efficiency is urgently
needed. Integrating learning-based predictive analytics into the fashion
industry represents a significant opportunity to address environmental
challenges and drive sustainable practices. By forecasting fashion trends and
optimizing production, brands can reduce their ecological footprint while
remaining competitive in a rapidly changing market. However, one of the key
challenges in forecasting fashion sales is the dynamic nature of consumer
preferences. Fashion is acyclical, with trends constantly evolving and
resurfacing. In addition, cultural changes and unexpected events can disrupt
established patterns. This problem is also known as New Fashion Products
Performance Forecasting (NFPPF), and it has recently gained more and more
interest in the global research landscape. Given its multidisciplinary nature,
the field of NFPPF has been approached from many different angles. This
comprehensive survey wishes to provide an up-to-date overview that focuses on
learning-based NFPPF strategies. The survey is based on the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) methodological flow,
allowing for a systematic and complete literature review. In particular, we
propose the first taxonomy that covers the learning panorama for NFPPF,
examining in detail the different methodologies used to increase the amount of
multimodal information, as well as the state-of-the-art available datasets.
Finally, we discuss the challenges and future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Springer Nature Computer Science journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Autoregressive <span class="highlight-title">Transformer</span>s: Combining Byte-~and Word-Level
  Processing for Robust, Adaptable Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pit Neitemeier, Björn Deiseroth, Constantin Eichenberg, Lukas Balles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tokenization is a fundamental step in natural language processing, breaking
text into units that computational models can process. While learned subword
tokenizers have become the de-facto standard, they present challenges such as
large vocabularies, limited adaptability to new domains or languages, and
sensitivity to spelling errors and variations. To overcome these limitations,
we investigate a hierarchical architecture for autoregressive language
modelling that combines character-level and word-level processing. It employs a
lightweight character-level encoder to convert character sequences into word
embeddings, which are then processed by a word-level backbone model and decoded
back into characters via a compact character-level decoder. This method retains
the sequence compression benefits of word-level tokenization without relying on
a rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion
parameters, that hierarchical transformers match the downstream task
performance of subword-tokenizer-based models while exhibiting significantly
greater robustness to input perturbations. Additionally, during continued
pretraining on an out-of-domain language, our model trains almost twice as
fast, achieves superior performance on the target language, and retains more of
its previously learned knowledge. Hierarchical transformers pave the way for
NLP systems that are more robust, flexible, and generalizable across languages
and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Human-Guided, Data-Centric LLM Co-Pilots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evgeny Saveliev, Jiashuo Liu, Nabeel Seedat, Anders Boyd, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) has the potential to revolutionize healthcare, but its
adoption is often hindered by the disconnect between the needs of domain
experts and translating these needs into robust and valid ML tools. Despite
recent advances in LLM-based co-pilots to democratize ML for non-technical
domain experts, these systems remain predominantly focused on model-centric
aspects while overlooking critical data-centric challenges. This limitation is
problematic in complex real-world settings where raw data often contains
complex issues, such as missing values, label noise, and domain-specific
nuances requiring tailored handling. To address this we introduce CliMB-DC, a
human-guided, data-centric framework for LLM co-pilots that combines advanced
data-centric tools with LLM-driven reasoning to enable robust, context-aware
data processing. At its core, CliMB-DC introduces a novel, multi-agent
reasoning system that combines a strategic coordinator for dynamic planning and
adaptation with a specialized worker agent for precise execution. Domain
expertise is then systematically incorporated to guide the reasoning process
using a human-in-the-loop approach. To guide development, we formalize a
taxonomy of key data-centric challenges that co-pilots must address.
Thereafter, to address the dimensions of the taxonomy, we integrate
state-of-the-art data-centric tools into an extensible, open-source
architecture, facilitating the addition of new tools from the research
community. Empirically, using real-world healthcare datasets we demonstrate
CliMB-DC's ability to transform uncurated datasets into ML-ready formats,
significantly outperforming existing co-pilot baselines for handling
data-centric challenges. CliMB-DC promises to empower domain experts from
diverse domains -- healthcare, finance, social sciences and more -- to actively
participate in driving real-world impact using ML.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Saveliev, Liu & Seedat contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pairwise Elimination with Instance-Dependent Guarantees for Bandits with
  Cost Subsidy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishank Juneja, Carlee Joe-Wong, Osman Yağan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-armed bandits (MAB) are commonly used in sequential online
decision-making when the reward of each decision is an unknown random variable.
In practice however, the typical goal of maximizing total reward may be less
important than minimizing the total cost of the decisions taken, subject to a
reward constraint. For example, we may seek to make decisions that have at
least the reward of a reference ``default'' decision, with as low a cost as
possible. This problem was recently introduced in the Multi-Armed Bandits with
Cost Subsidy (MAB-CS) framework. MAB-CS is broadly applicable to problem
domains where a primary metric (cost) is constrained by a secondary metric
(reward), and the rewards are unknown. In our work, we address variants of
MAB-CS including ones with reward constrained by the reward of a known
reference arm or by the subsidized best reward. We introduce the
Pairwise-Elimination (PE) algorithm for the known reference arm variant and
generalize PE to PE-CS for the subsidized best reward variant. Our
instance-dependent analysis of PE and PE-CS reveals that both algorithms have
an order-wise logarithmic upper bound on Cost and Quality Regret, making our
policies the first with such a guarantee. Moreover, by comparing our upper and
lower bound results we establish that PE is order-optimal for all known
reference arm problem instances. Finally, experiments are conducted using the
MovieLens 25M and Goodreads datasets for both PE and PE-CS revealing the
effectiveness of PE and the superior balance between performance and
reliability offered by PE-CS compared to baselines from the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEANN: A Domain-Informed Neural Network for Epidemiological Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Baptiste Guimbaud, Marc Plantevit, Léa Maître, Rémy Cazabet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In epidemiology, traditional statistical methods such as logistic regression,
linear regression, and other parametric models are commonly employed to
investigate associations between predictors and health outcomes. However,
non-parametric machine learning techniques, such as deep neural networks
(DNNs), coupled with explainable AI (XAI) tools, offer new opportunities for
this task. Despite their potential, these methods face challenges due to the
limited availability of high-quality, high-quantity data in this field. To
address these challenges, we introduce SEANN, a novel approach for informed
DNNs that leverages a prevalent form of domain-specific knowledge: Pooled
Effect Sizes (PES). PESs are commonly found in published Meta-Analysis studies,
in different forms, and represent a quantitative form of a scientific
consensus. By direct integration within the learning procedure using a custom
loss, we experimentally demonstrate significant improvements in the
generalizability of predictive performances and the scientific plausibility of
extracted relationships compared to a domain-knowledge agnostic neural network
in a scarce and noisy data setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logarithmic Regret for Nonlinear Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Wang, Bruce D. Lee, Ingvar Ziemann, Nikolai Matni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of learning to control an unknown nonlinear dynamical
system through sequential interactions. Motivated by high-stakes applications
in which mistakes can be catastrophic, such as robotics and healthcare, we
study situations where it is possible for fast sequential learning to occur.
Fast sequential learning is characterized by the ability of the learning agent
to incur logarithmic regret relative to a fully-informed baseline. We
demonstrate that fast sequential learning is achievable in a diverse class of
continuous control problems where the system dynamics depend smoothly on
unknown parameters, provided the optimal control policy is persistently
exciting. Additionally, we derive a regret bound which grows with the square
root of the number of interactions for cases where the optimal policy is not
persistently exciting. Our results provide the first regret bounds for
controlling nonlinear dynamical systems depending nonlinearly on unknown
parameters. We validate the trends our theory predicts in simulation on a
simple dynamical system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DADA: Dual Averaging with Distance Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Moshtaghifar, Anton Rodomanov, Daniil Vankov, Sebastian Stich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel universal gradient method for solving convex optimization
problems. Our algorithm -- Dual Averaging with Distance Adaptation (DADA) -- is
based on the classical scheme of dual averaging and dynamically adjusts its
coefficients based on observed gradients and the distance between iterates and
the starting point, eliminating the need for problem-specific parameters. DADA
is a universal algorithm that simultaneously works for a broad spectrum of
problem classes, provided the local growth of the objective function around its
minimizer can be bounded. Particular examples of such problem classes are
nonsmooth Lipschitz functions, Lipschitz-smooth functions, H\"older-smooth
functions, functions with high-order Lipschitz derivative,
quasi-self-concordant functions, and $(L_0,L_1)$-smooth functions. Crucially,
DADA is applicable to both unconstrained and constrained problems, even when
the domain is unbounded, without requiring prior knowledge of the number of
iterations or desired accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Rhythm and Voice Conversion of Dysarthric to Healthy Speech
  for ASR <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl El Hajal, Enno Hermann, Ajinkya Kulkarni, Mathew Magimai. -Doss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) systems are well known to perform poorly
on dysarthric speech. Previous works have addressed this by speaking rate
modification to reduce the mismatch with typical speech. Unfortunately, these
approaches rely on transcribed speech data to estimate speaking rates and
phoneme durations, which might not be available for unseen speakers. Therefore,
we combine unsupervised rhythm and voice conversion methods based on
self-supervised speech representations to map dysarthric to typical speech. We
evaluate the outputs with a large ASR model pre-trained on healthy speech
without further fine-tuning and find that the proposed rhythm conversion
especially improves performance for speakers of the Torgo corpus with more
severe cases of dysarthria. Code and audio samples are available at
https://idiap.github.io/RnV .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025 Satellite Workshop: Workshop on Speech
  Pathology Analysis and DEtection (SPADE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Over-the-Air Multi-Sensor Inference with Neural Networks Using
  Memristor-Based Analog Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Busra Tegin, Muhammad Atif Ali, Tolga M Duman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks provide reliable solutions for many classification and
regression tasks; however, their application in real-time wireless systems with
simple sensor networks is limited due to high energy consumption and
significant bandwidth needs. This study proposes a multi-sensor wireless
inference system with memristor-based analog computing. Given the sensors'
limited computational capabilities, the features from the network's front end
are transmitted to a central device where an $L_p$-norm inspired approximation
of the maximum operation is employed to achieve transformation-invariant
features, enabling efficient over-the-air transmission. We also introduce a
trainable over-the-air sensor fusion method based on $L_p$-norm inspired
combining function that customizes sensor fusion to match the network and
sensor distribution characteristics, enhancing adaptability. To address the
energy constraints of sensors, we utilize memristors, known for their
energy-efficient in-memory computing, enabling analog-domain computations that
reduce energy use and computational overhead in edge computing. This dual
approach of memristors and $L_p$-norm inspired sensor fusion fosters
energy-efficient computational and transmission paradigms and serves as a
practical energy-efficient solution with minimal performance loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Challenges and recommendations for Electronic Health Records data
  extraction and preparation for dynamic prediction modelling in hospitalized
  patients -- a practical guide 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Albu, Shan Gao, Pieter Stijnen, Frank E. Rademakers, Bas C T van Bussel, Taya Collyer, Tina Hernandez-Boussard, Laure Wynants, Ben Van Calster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic predictive modeling using electronic health record (EHR) data has
gained significant attention in recent years. The reliability and
trustworthiness of such models depend heavily on the quality of the underlying
data, which is largely determined by the stages preceding the model
development: data extraction from EHR systems and data preparation. We list
over forty challenges encountered during these stages and provide actionable
recommendations for addressing them. These challenges are organized into four
categories: cohort definition, outcome definition, feature engineering, and
data cleaning. This list is designed to serve as a practical guide for data
extraction engineers and researchers, supporting better practices and improving
the quality and real-world applicability of dynamic prediction models in
clinical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpaceTime: Causal Discovery from Non-Stationary Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarah Mameche, Lénaïg Cornanguer, Urmi Ninad, Jilles Vreeken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding causality is challenging and often complicated by changing
causal relationships over time and across environments. Climate patterns, for
example, shift over time with recurring seasonal trends, while also depending
on geographical characteristics such as ecosystem variability. Existing methods
for discovering causal graphs from time series either assume stationarity, do
not permit both temporal and spatial distribution changes, or are unaware of
locations with the same causal relationships. In this work, we therefore unify
the three tasks of causal graph discovery in the non-stationary multi-context
setting, of reconstructing temporal regimes, and of partitioning datasets and
time intervals into those where invariant causal relationships hold. To
construct a consistent score that forms the basis of our method, we employ the
Minimum Description Length principle. Our resulting algorithm SPACETIME
simultaneously accounts for heterogeneity across space and non-stationarity
over time. Given multiple time series, it discovers regime changepoints and a
temporal causal graph using non-parametric functional modeling and kernelized
discrepancy testing. We also show that our method provides insights into
real-world phenomena such as river-runoff measured at different catchments and
biosphere-atmosphere interactions across ecosystems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Explanations for k-means and Gaussian Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Vardakas, Antonia Karra, Evaggelia Pitoura, Aristidis Likas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactuals have been recognized as an effective approach to explain
classifier decisions. Nevertheless, they have not yet been considered in the
context of clustering. In this work, we propose the use of counterfactuals to
explain clustering solutions. First, we present a general definition for
counterfactuals for model-based clustering that includes plausibility and
feasibility constraints. Then we consider the counterfactual generation problem
for k-means and Gaussian clustering assuming Euclidean distance. Our approach
takes as input the factual, the target cluster, a binary mask indicating
actionable or immutable features and a plausibility factor specifying how far
from the cluster boundary the counterfactual should be placed. In the k-means
clustering case, analytical mathematical formulas are presented for computing
the optimal solution, while in the Gaussian clustering case (assuming full,
diagonal, or spherical covariances) our method requires the numerical solution
of a nonlinear equation with a single parameter only. We demonstrate the
advantages of our approach through illustrative examples and quantitative
experimental comparisons.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amortized Bayesian Mixture Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Šimon Kucharský, Paul Christian Bürkner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finite mixtures are a broad class of models useful in scenarios where
observed data is generated by multiple distinct processes but without explicit
information about the responsible process for each data point. Estimating
Bayesian mixture models is computationally challenging due to issues such as
high-dimensional posterior inference and label switching. Furthermore,
traditional methods such as MCMC are applicable only if the likelihoods for
each mixture component are analytically tractable.
  Amortized Bayesian Inference (ABI) is a simulation-based framework for
estimating Bayesian models using generative neural networks. This allows the
fitting of models without explicit likelihoods, and provides fast inference.
ABI is therefore an attractive framework for estimating mixture models. This
paper introduces a novel extension of ABI tailored to mixture models. We
factorize the posterior into a distribution of the parameters and a
distribution of (categorical) mixture indicators, which allows us to use a
combination of generative neural networks for parameter inference, and
classification networks for mixture membership identification. The proposed
framework accommodates both independent and dependent mixture models, enabling
filtering and smoothing. We validate and demonstrate our approach through
synthetic and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modelling Activity Scheduling Behaviour with Deep Generative Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Shone, Tim Hillel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We model human activity scheduling behaviour using a deep generative machine
learning approach. Activity schedules, which represent the activities and
associated travel behaviours of individuals, are a core component of many
applied models in the transport, energy and epidemiology domains. Our data
driven approach learns human preferences and scheduling logic without the need
for complex interacting combinations of sub-models and custom-rules, this makes
our approach significantly faster and simpler to operate that existing
approaches. We find activity schedule data combines aspects of both continuous
image data and also discrete text data, requiring novel approaches. We
additionally contribute a novel schedule representation and comprehensive
evaluation framework for generated schedules. Evaluation shows our approach is
able to rapidly generate large, diverse and realistic synthetic samples of
activity schedules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Relevance of AWS Chronos: An Evaluation of Standard Methods for Time
  Series Forecasting with Limited Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Baron, Alex Karpinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A systematic comparison of Chronos, a transformer-based time series
forecasting framework, against traditional approaches including ARIMA and
Prophet. We evaluate these models across multiple time horizons and user
categories, with a focus on the impact of historical context length. Our
analysis reveals that while Chronos demonstrates superior performance for
longer-term predictions and maintains accuracy with increased context,
traditional models show significant degradation as context length increases. We
find that prediction quality varies systematically between user classes,
suggesting that underlying behavior patterns always influence model
performance. This study provides a case for deploying Chronos in real-world
applications where limited model tuning is feasible, especially in scenarios
requiring longer prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Graph MLP Mixer for Spatio-Temporal Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Bilal, Luis Carretero Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal forecasting is critical in applications such as traffic
prediction, climate modeling, and environmental monitoring. However, the
prevalence of missing data in real-world sensor networks significantly
complicates this task. In this paper, we introduce the Temporal Graph MLP-Mixer
(T-GMM), a novel architecture designed to address these challenges. The model
combines node-level processing with patch-level subgraph encoding to capture
localized spatial dependencies while leveraging a three-dimensional MLP-Mixer
to handle temporal, spatial, and feature-based dependencies. Experiments on the
AQI, ENGRAD, PV-US and METR-LA datasets demonstrate the model's ability to
effectively forecast even in the presence of significant missing data. While
not surpassing state-of-the-art models in all scenarios, the T-GMM exhibits
strong learning capabilities, particularly in capturing long-range
dependencies. These results highlight its potential for robust, scalable
spatiotemporal forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hypercone Assisted Contour Generation for Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annita Vapsi, Andrés Muñoz, Nancy Thomas, Keshav Ramani, Daniel Borrajo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in the field of out-of-distribution (OOD) detection have
placed great emphasis on learning better representations suited to this task.
While there are distance-based approaches, distributional awareness has seldom
been exploited for better performance. We present HAC$_k$-OOD, a novel OOD
detection method that makes no distributional assumption about the data, but
automatically adapts to its distribution. Specifically, HAC$_k$-OOD constructs
a set of hypercones by maximizing the angular distance to neighbors in a given
data-point's vicinity to approximate the contour within which in-distribution
(ID) data-points lie. Experimental results show state-of-the-art FPR@95 and
AUROC performance on Near-OOD detection and on Far-OOD detection on the
challenging CIFAR-100 benchmark without explicitly training for OOD
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Safeguarding a Classifier from OOD and Adversarial Samples: an
  Extreme Value Theory Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Atienza, Christophe Labreuche, Johanne Cohen, Michele Sebag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel method, Sample-efficient Probabilistic
Detection using Extreme Value Theory (SPADE), which transforms a classifier
into an abstaining classifier, offering provable protection against
out-of-distribution and adversarial samples. The approach is based on a
Generalized Extreme Value (GEV) model of the training distribution in the
classifier's latent space, enabling the formal characterization of OOD samples.
Interestingly, under mild assumptions, the GEV model also allows for formally
characterizing adversarial samples. The abstaining classifier, which rejects
samples based on their assessment by the GEV model, provably avoids OOD and
adversarial samples. The empirical validation of the approach, conducted on
various neural architectures (ResNet, VGG, and Vision Transformer) and medium
and large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its
frugality, stability, and efficiency compared to the state of the art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contributions to the Decision Theoretic Foundations of Machine Learning
  and Robust Statistics under Weakly Structured Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Jansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This habilitation thesis is cumulative and, therefore, is collecting and
connecting research that I (together with several co-authors) have conducted
over the last few years. Thus, the absolute core of the work is formed by the
ten publications listed on page 5 under the name Contributions 1 to 10. The
references to the complete versions of these articles are also found in this
list, making them as easily accessible as possible for readers wishing to dive
deep into the different research projects. The chapters following this thesis,
namely Parts A to C and the concluding remarks, serve to place the articles in
a larger scientific context, to (briefly) explain their respective content on a
less formal level, and to highlight some interesting perspectives for future
research in their respective contexts. Naturally, therefore, the following
presentation has neither the level of detail nor the formal rigor that can
(hopefully) be found in the papers. The purpose of the following text is to
provide the reader an easy and high-level access to this interesting and
important research field as a whole, thereby, advertising it to a broader
audience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Habilitation Thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surrogate-based multiscale analysis of experiments on thermoplastic
  composites under off-axis loading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. A. Maia, I. B. C. M. Rocha, D. Kovačević, F. P. van der Meer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a surrogate-based multiscale approach to model
constant strain-rate and creep experiments on unidirectional thermoplastic
composites under off-axis loading. In previous contributions, these experiments
were modeled through a single-scale micromechanical simulation under the
assumption of macroscopic homogeneity. Although efficient and accurate in many
scenarios, simulations with low-off axis angles showed significant
discrepancies with the experiments. It was hypothesized that the mismatch was
caused by macroscopic inhomogeneity, which would require a multiscale approach
to capture it. However, full-field multiscale simulations remain
computationally prohibitive. To address this issue, we replace the micromodel
with a Physically Recurrent Neural Network (PRNN), a surrogate model that
combines data-driven components with embedded constitutive models to capture
history-dependent behavior naturally. The explainability of the latent space of
this network is also explored in a transfer learning strategy that requires no
re-training. With the surrogate-based simulations, we confirm the hypothesis
raised on the inhomogeneity of the macroscopic strain field and gain insights
into the influence of adjustment of the experimental setup with oblique
end-tabs. Results from the surrogate-based multiscale approach show better
agreement with experiments than the single-scale micromechanical approach over
a wide range of settings, although with limited accuracy on the creep
experiments, where macroscopic test effects were implicitly taken into account
in the material properties calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages. 31 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved learning rates in multi-unit uniform price auctions <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marius Potfer, Dorian Baudry, Hugo Richard, Vianney Perchet, Cheng Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the strategic participation of electricity producers in
electricity day-ahead market, we study the problem of online learning in
repeated multi-unit uniform price auctions focusing on the adversarial opposing
bid setting. The main contribution of this paper is the introduction of a new
modeling of the bid space. Indeed, we prove that a learning algorithm
leveraging the structure of this problem achieves a regret of
$\tilde{O}(K^{4/3}T^{2/3})$ under bandit feedback, improving over the bound of
$\tilde{O}(K^{7/4}T^{3/4})$ previously obtained in the literature. This
improved regret rate is tight up to logarithmic terms. Inspired by electricity
reserve markets, we further introduce a different feedback model under which
all winning bids are revealed. This feedback interpolates between the
full-information and bandit scenarios depending on the auctions' results. We
prove that, under this feedback, the algorithm that we propose achieves regret
$\tilde{O}(K^{5/2}\sqrt{T})$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple but Effective Closed-form Solution for Extreme Multi-label
  Learning <span class="chip">ECIR25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuma Onishi, Katsuhiko Hayashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extreme multi-label learning (XML) is a task of assigning multiple labels
from an extremely large set of labels to each data instance. Many current
high-performance XML models are composed of a lot of hyperparameters, which
complicates the tuning process. Additionally, the models themselves are adapted
specifically to XML, which complicates their reimplementation. To remedy this
problem, we propose a simple method based on ridge regression for XML. The
proposed method not only has a closed-form solution but also is composed of a
single hyperparameter. Since there are no precedents on applying ridge
regression to XML, this paper verified the performance of the method by using
various XML benchmark datasets. Furthermore, we enhanced the prediction of
low-frequency labels in XML, which hold informative content. This prediction is
essential yet challenging because of the limited amount of data. Here, we
employed a simple frequency-based weighting. This approach greatly simplifies
the process compared with existing techniques. Experimental results revealed
that it can achieve levels of performance comparable to, or even exceeding,
those of models with numerous hyperparameters. Additionally, we found that the
frequency-based weighting significantly improved the predictive performance for
low-frequency labels, while requiring almost no changes in implementation. The
source code for the proposed method is available on github at
https://github.com/cars1015/XML-ridge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10pages, Accepted at ECIR25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mean and Variance Estimation Complexity in Arbitrary Distributions via
  Wasserstein Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentio Iverson, Stephen Vavasis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter estimation is a fundamental challenge in machine learning, crucial
for tasks such as neural network weight fitting and Bayesian inference. This
paper focuses on the complexity of estimating translation $\boldsymbol{\mu} \in
\mathbb{R}^l$ and shrinkage $\sigma \in \mathbb{R}_{++}$ parameters for a
distribution of the form $\frac{1}{\sigma^l} f_0 \left( \frac{\boldsymbol{x} -
\boldsymbol{\mu}}{\sigma} \right)$, where $f_0$ is a known density in
$\mathbb{R}^l$ given $n$ samples. We highlight that while the problem is
NP-hard for Maximum Likelihood Estimation (MLE), it is possible to obtain
$\varepsilon$-approximations for arbitrary $\varepsilon > 0$ within
$\text{poly} \left( \frac{1}{\varepsilon} \right)$ time using the Wasserstein
distance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convex Physics Informed Neural Networks for the Monge-Ampère Optimal
  Transport Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Caboussat, Anna Peruso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal transportation of raw material from suppliers to customers is an
issue arising in logistics that is addressed here with a continuous model
relying on optimal transport theory. A physics informed neuralnetwork method is
advocated here for the solution of the corresponding generalized Monge-Amp`ere
equation. Convex neural networks are advocated to enforce the convexity of the
solution to the Monge-Amp\`ere equation and obtain a suitable approximation of
the optimal transport map. A particular focus is set on the enforcement of
transport boundary conditions in the loss function. Numerical experiments
illustrate the solution to the optimal transport problem in several
configurations, and sensitivity analyses are performed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 14 figures. Submitted to Engineering Computations on 26
  September 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Region-wise stacking ensembles for estimating brain-age using MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Antonopoulos, Shammi More, Simon B. Eickhoff, Federico Raimondo, Kaustubh R. Patil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive modeling using structural magnetic resonance imaging (MRI) data is
a prominent approach to study brain-aging. Machine learning algorithms and
feature extraction methods have been employed to improve predictions and
explore healthy and accelerated aging e.g. neurodegenerative and psychiatric
disorders. The high-dimensional MRI data pose challenges to building
generalizable and interpretable models as well as for data privacy. Common
practices are resampling or averaging voxels within predefined parcels, which
reduces anatomical specificity and biological interpretability as voxels within
a region may differently relate to aging. Effectively, naive fusion by
averaging can result in information loss and reduced accuracy. We present a
conceptually novel two-level stacking ensemble (SE) approach. The first level
comprises regional models for predicting individuals' age based on voxel-wise
information, fused by a second-level model yielding final predictions. Eight
data fusion scenarios were explored using as input Gray matter volume (GMV)
estimates from four datasets covering the adult lifespan. Performance, measured
using mean absolute error (MAE), R2, correlation and prediction bias, showed
that SE outperformed the region-wise averages. The best performance was
obtained when first-level regional predictions were obtained as out-of-sample
predictions on the application site with second-level models trained on
independent and site-specific data (MAE=4.75 vs baseline regional mean GMV
MAE=5.68). Performance improved as more datasets were used for training.
First-level predictions showed improved and more robust aging signal providing
new biological insights and enhanced data privacy. Overall, the SE improves
accuracy compared to the baseline while preserving or enhancing data privacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>version1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing UAV Path Planning Efficiency Through Accelerated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseanne Viana, Boris Galkin, Lester Ho, Holger Claussen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs) are increasingly essential in various fields
such as surveillance, reconnaissance, and telecommunications. This study aims
to develop a learning algorithm for the path planning of UAV wireless
communication relays, which can reduce storage requirements and accelerate Deep
Reinforcement Learning (DRL) convergence. Assuming the system possesses terrain
maps of the area and can estimate user locations using localization algorithms
or direct GPS reporting, it can input these parameters into the learning
algorithms to achieve optimized path planning performance. However, higher
resolution terrain maps are necessary to extract topological information such
as terrain height, object distances, and signal blockages. This requirement
increases memory and storage demands on UAVs while also lengthening convergence
times in DRL algorithms. Similarly, defining the telecommunication coverage map
in UAV wireless communication relays using these terrain maps and user position
estimations demands higher memory and storage utilization for the learning path
planning algorithms. Our approach reduces path planning training time by
applying a dimensionality reduction technique based on Principal Component
Analysis (PCA), sample combination, Prioritized Experience Replay (PER), and
the combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE) loss
calculations in the coverage map estimates, thereby enhancing a Twin Delayed
Deep Deterministic Policy Gradient (TD3) algorithm. The proposed solution
reduces the convergence episodes needed for basic training by approximately
four times compared to the traditional TD3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted in https://camad2024.ieee-camad.org/
  conference but it is not available from the conference yet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Prediction Sets with Improved Conditional Coverage using Trust
  Scores 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jivat Neet Kaur, Michael I. Jordan, Ahmed Alaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard conformal prediction offers a marginal guarantee on coverage, but
for prediction sets to be truly useful, they should ideally ensure coverage
conditional on each test point. Unfortunately, it is impossible to achieve
exact, distribution-free conditional coverage in finite samples. In this work,
we propose an alternative conformal prediction algorithm that targets coverage
where it matters most--in instances where a classifier is overconfident in its
incorrect predictions. We start by dissecting miscoverage events in
marginally-valid conformal prediction, and show that miscoverage rates vary
based on the classifier's confidence and its deviation from the Bayes optimal
classifier. Motivated by this insight, we develop a variant of conformal
prediction that targets coverage conditional on a reduced set of two variables:
the classifier's confidence in a prediction and a nonparametric trust score
that measures its deviation from the Bayes classifier. Empirical evaluation on
multiple image datasets shows that our method generally improves conditional
coverage properties compared to standard conformal prediction, including
class-conditional coverage, coverage over arbitrary subgroups, and coverage
over demographic groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Exploration of Stopword Probabilities in Topic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangjiang Xue, Pierre Le Bras, David A. Robb, Mike J. Chantler, Stefano Padilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stopword removal is a critical stage in many Machine Learning methods but
often receives little consideration, it interferes with the model
visualizations and disrupts user confidence. Inappropriately chosen or hastily
omitted stopwords not only lead to suboptimal performance but also
significantly affect the quality of models, thus reducing the willingness of
practitioners and stakeholders to rely on the output visualizations. This paper
proposes a novel extraction method that provides a corpus-specific
probabilistic estimation of stopword likelihood and an interactive
visualization system to support their analysis. We evaluated our approach and
interface using real-world data, a commonly used Machine Learning method (Topic
Modelling), and a comprehensive qualitative experiment probing user confidence.
The results of our work show that our system increases user confidence in the
credibility of topic models by (1) returning reasonable probabilities, (2)
generating an appropriate and representative extension of common stopword
lists, and (3) providing an adjustable threshold for estimating and analyzing
stopwords visually. Finally, we discuss insights, recommendations, and best
practices to support practitioners while improving the output of Machine
Learning methods and topic model visualizations with robust stopword analysis
and removal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Impact of Generative Artificial Intelligence in Education:
  A Thematic Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Kaushik, Sargam Yadav, Andrew Browne, David Lillis, David Williams, Jack Mc Donnell, Peadar Grant, Siobhan Connolly Kernan, Shubham Sharma, Mansi Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in Generative Artificial intelligence (GenAI)
technology have been transformative for the field of education. Large Language
Models (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate
tasks, create content for personalised teaching, and handle repetitive tasks to
allow more time for creative thinking. However, it is important to develop
guidelines, policies, and assessment methods in the education sector to ensure
the responsible integration of these tools. In this article, thematic analysis
has been performed on seven essays obtained from professionals in the education
sector to understand the advantages and pitfalls of using GenAI models such as
ChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been
performed on the essays to extract further insights from the text. The study
found several themes which highlight benefits and drawbacks of GenAI tools, as
well as suggestions to overcome these limitations and ensure that students are
using these tools in a responsible and ethical manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gene Regulatory Network Inference in the Presence of Selection Bias and
  Latent Confounders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongxu Luo, Haoyue Dai, Boyang Sun, Loka Li, Biwei Huang, Petar Stojanov, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gene Regulatory Network Inference (GRNI) aims to identify causal
relationships among genes using gene expression data, providing insights into
regulatory mechanisms. A significant yet often overlooked challenge is
selection bias, a process where only cells meeting specific criteria, such as
gene expression thresholds, survive or are observed, distorting the true joint
distribution of genes and thus biasing GRNI results. Furthermore, gene
expression is influenced by latent confounders, such as non-coding RNAs, which
add complexity to GRNI. To address these challenges, we propose GISL (Gene
Regulatory Network Inference in the presence of Selection bias and Latent
confounders), a novel algorithm to infer true regulatory relationships in the
presence of selection and confounding issues. Leveraging data obtained via
multiple gene perturbation experiments, we show that the true regulatory
relationships, as well as selection processes and latent confounders can be
partially identified without strong parametric models and under mild graphical
assumptions. Experimental results on both synthetic and real-world single-cell
gene expression datasets demonstrate the superiority of GISL over existing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PaSa: An LLM Agent for Comprehensive Academic Paper Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, Weinan E
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PaSa, an advanced Paper Search agent powered by large language
models. PaSa can autonomously make a series of decisions, including invoking
search tools, reading papers, and selecting relevant references, to ultimately
obtain comprehensive and accurate results for complex scholarly queries. We
optimize PaSa using reinforcement learning with a synthetic dataset,
AutoScholarQuery, which includes 35k fine-grained academic queries and
corresponding papers sourced from top-tier AI conference publications.
Additionally, we develop RealScholarQuery, a benchmark collecting real-world
academic queries to assess PaSa performance in more realistic scenarios.
Despite being trained on synthetic data, PaSa significantly outperforms
existing baselines on RealScholarQuery, including Google, Google Scholar,
Google with GPT-4 for paraphrased queries, chatGPT (search-enabled GPT-4o),
GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably,
PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78%
in recall@20 and 39.90% in recall@50. It also exceeds PaSa-GPT-4o by 30.36% in
recall and 4.25% in precision. Model, datasets, and code are available at
https://github.com/bytedance/pasa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotic World Model: A Neural Network Simulator for Robust Policy
  Optimization in Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Li, Andreas Krause, Marco Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning robust and generalizable world models is crucial for enabling
efficient and scalable robotic control in real-world environments. In this
work, we introduce a novel framework for learning world models that accurately
capture complex, partially observable, and stochastic dynamics. The proposed
method employs a dual-autoregressive mechanism and self-supervised training to
achieve reliable long-horizon predictions without relying on domain-specific
inductive biases, ensuring adaptability across diverse robotic tasks. We
further propose a policy optimization framework that leverages world models for
efficient training in imagined environments and seamless deployment in
real-world systems. Through extensive experiments, our approach consistently
outperforms state-of-the-art methods, demonstrating superior autoregressive
prediction accuracy, robustness to noise, and generalization across
manipulation and locomotion tasks. Notably, policies trained with our method
are successfully deployed on ANYmal D hardware in a zero-shot transfer,
achieving robust performance with minimal sim-to-real performance loss. This
work advances model-based reinforcement learning by addressing the challenges
of long-horizon prediction, error accumulation, and sim-to-real transfer. By
providing a scalable and robust framework, the introduced methods pave the way
for adaptive and efficient robotic systems in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jef Jonkers, Luc Duchateau, Glenn Van Wallendael, Sofie Van Hoecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anatomical landmark localization in 2D/3D images is a critical task in
medical imaging. Although many general-purpose tools exist for landmark
localization in classical computer vision tasks, such as pose estimation, they
lack the specialized features and modularity necessary for anatomical landmark
localization applications in the medical domain. Therefore, we introduce
landmarker, a Python package built on PyTorch. The package provides a
comprehensive, flexible toolkit for developing and evaluating landmark
localization algorithms, supporting a range of methodologies, including static
and adaptive heatmap regression. landmarker enhances the accuracy of landmark
identification, streamlines research and development processes, and supports
various image formats and preprocessing pipelines. Its modular design allows
users to customize and extend the toolkit for specific datasets and
applications, accelerating innovation in medical imaging. landmarker addresses
a critical need for precision and customization in landmark localization tasks
not adequately met by existing general-purpose pose estimation tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A recursive Bayesian neural network for constitutive modeling of sands
  under monotonic loading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toiba Noor, Soban Nasir Lone, G. V. Ramana, Rajdip Nayek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In geotechnical engineering, constitutive models play a crucial role in
describing soil behavior under varying loading conditions. Data-driven deep
learning (DL) models offer a promising alternative for developing predictive
constitutive models. When prediction is the primary focus, quantifying the
predictive uncertainty of a trained DL model and communicating this uncertainty
to end users is crucial for informed decision-making.
  This study proposes a recursive Bayesian neural network (rBNN) framework,
which builds upon recursive feedforward neural networks (rFFNNs) by introducing
generalized Bayesian inference for uncertainty quantification. A significant
contribution of this work is the incorporation of a sliding window approach in
rFFNNs, allowing the models to effectively capture temporal dependencies across
load steps. The rBNN extends this framework by treating model parameters as
random variables, with their posterior distributions inferred using generalized
variational inference.
  The proposed framework is validated on two datasets: (i) a numerically
simulated consolidated drained (CD) triaxial dataset employing a hardening soil
model and (ii) an experimental dataset comprising 28 CD triaxial tests on
Baskarp sand. Comparative analyses with LSTM, Bi-LSTM, and GRU models
demonstrate that the deterministic rFFNN achieves superior predictive accuracy,
attributed to its transparent structure and sliding window design. While the
rBNN marginally trails in accuracy for the experimental case, it provides
robust confidence intervals, addressing data sparsity and measurement noise in
experimental conditions. The study underscores the trade-offs between
deterministic and probabilistic approaches and the potential of rBNNs for
uncertainty-aware constitutive modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-level Solar Irradiance Clustering with Season Identification: A
  Comparative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roshni Agrawal, Sivakumar Subramanian, Venkataramana Runkana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solar irradiance clustering can enhance solar power capacity planning and
help improve forecasting models by identifying similar irradiance patterns
influenced by seasonal and weather changes. In this study, we adopt an
efficient two-level clustering approach to automatically identify seasons using
the clear sky irradiance in first level and subsequently to identify daily
cloud level as clear, cloudy and partly cloudy within each season in second
level. In the second level of clustering, three methods are compared, namely,
Daily Irradiance Index (DII or $\beta$), Euclidean Distance (ED), and Dynamic
Time Warping (DTW) distance. The DII is computed as the ratio of time integral
of measured irradiance to time integral of the clear sky irradiance. The
identified clusters were compared quantitatively using established clustering
metrics and qualitatively by comparing the mean irradiance profiles. The
results clearly establish the superiority of the $\beta$-based clustering
approach as the leader, setting a new benchmark for solar irradiance clustering
studies. Moreover, $\beta$-based clustering remains effective even for annual
data unlike the time-series methods which suffer significant performance
degradation. Interestingly, contrary to expectations, ED-based clustering
outperforms the more compute-intensive DTW distance-based clustering. The
method has been rigorously validated using data from two distinct US locations,
demonstrating robust scalability for larger datasets and potential
applicability for other locations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 9 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double descent in quantum machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marie Kempkes, Aroosa Ijaz, Elies Gil-Fuster, Carlos Bravo-Prieto, Jakob Spiegelberg, Evert van Nieuwenburg, Vedran Dunjko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The double descent phenomenon challenges traditional statistical learning
theory by revealing scenarios where larger models do not necessarily lead to
reduced performance on unseen data. While this counterintuitive behavior has
been observed in a variety of classical machine learning models, particularly
modern neural network architectures, it remains elusive within the context of
quantum machine learning. In this work, we analytically demonstrate that
quantum learning models can exhibit double descent behavior by drawing on
insights from linear regression and random matrix theory. Additionally, our
numerical experiments on quantum kernel methods across different real-world
datasets and system sizes further confirm the existence of a test error peak, a
characteristic feature of double descent. Our findings provide evidence that
quantum models can operate in the modern, overparameterized regime without
experiencing overfitting, thereby opening pathways to improved learning
performance beyond traditional statistical learning theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Change Captioning in Remote Sensing: SECOND-CC Dataset and
  MModalCC Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Can Karaca, M. Enes Ozelbas, Saadettin Berber, Orkhan Karimli, Turabi Yildirim, M. Fatih Amasyali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote sensing change captioning (RSICC) aims to describe changes between
bitemporal images in natural language. Existing methods often fail under
challenges like illumination differences, viewpoint changes, blur effects,
leading to inaccuracies, especially in no-change regions. Moreover, the images
acquired at different spatial resolutions and have registration errors tend to
affect the captions. To address these issues, we introduce SECOND-CC, a novel
RSICC dataset featuring high-resolution RGB image pairs, semantic segmentation
maps, and diverse real-world scenarios. SECOND-CC which contains 6,041 pairs of
bitemporal RS images and 30,205 sentences describing the differences between
images. Additionally, we propose MModalCC, a multimodal framework that
integrates semantic and visual data using advanced attention mechanisms,
including Cross-Modal Cross Attention (CMCA) and Multimodal Gated Cross
Attention (MGCA). Detailed ablation studies and attention visualizations
further demonstrate its effectiveness and ability to address RSICC challenges.
Comprehensive experiments show that MModalCC outperforms state-of-the-art RSICC
methods, including RSICCformer, Chg2Cap, and PSNet with +4.6% improvement on
BLEU4 score and +9.6% improvement on CIDEr score. We will make our dataset and
codebase publicly available to facilitate future research at
https://github.com/ChangeCapsInRS/SecondCC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE Transactions on Geoscience
  and Remote Sensing journal for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-D-Piece: Image Tokenizer Meets Quality-Controllable Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keita Miwa, Kento Sasaki, Hidehisa Arai, Tsubasa Takahashi, Yu Yamaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current image tokenization methods require a large number of tokens to
capture the information contained within images. Although the amount of
information varies across images, most image tokenizers only support
fixed-length tokenization, leading to inefficiency in token allocation. In this
study, we introduce One-D-Piece, a discrete image tokenizer designed for
variable-length tokenization, achieving quality-controllable mechanism. To
enable variable compression rate, we introduce a simple but effective
regularization mechanism named "Tail Token Drop" into discrete one-dimensional
image tokenizers. This method encourages critical information to concentrate at
the head of the token sequence, enabling support of variadic tokenization,
while preserving state-of-the-art reconstruction quality. We evaluate our
tokenizer across multiple reconstruction quality metrics and find that it
delivers significantly better perceptual quality than existing
quality-controllable compression methods, including JPEG and WebP, at smaller
byte sizes. Furthermore, we assess our tokenizer on various downstream computer
vision tasks, including image classification, object detection, semantic
segmentation, and depth estimation, confirming its adaptability to numerous
applications compared to other variable-rate methods. Our approach demonstrates
the versatility of variable-length discrete image tokenization, establishing a
new paradigm in both compression efficiency and reconstruction performance.
Finally, we validate the effectiveness of tail token drop via detailed analysis
of tokenizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our Project Page:
  https://turingmotors.github.io/one-d-piece-tokenizer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OMoE: Diversifying Mixture of Low-Rank Adaptation by Orthogonal
  Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyuan Feng, Zhiqiang Pu, Tianyi Hu, Dongmin Li, Xiaolin Ai, Huimu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building mixture-of-experts (MoE) architecture for Low-rank adaptation (LoRA)
is emerging as a potential direction in parameter-efficient fine-tuning (PEFT)
for its modular design and remarkable performance. However, simply stacking the
number of experts cannot guarantee significant improvement. In this work, we
first conduct qualitative analysis to indicate that experts collapse to similar
representations in vanilla MoE, limiting the capacity of modular design and
computational efficiency. Ulteriorly, Our analysis reveals that the performance
of previous MoE variants maybe limited by a lack of diversity among experts.
Motivated by these findings, we propose Orthogonal Mixture-of-Experts (OMoE), a
resource-efficient MoE variant that trains experts in an orthogonal manner to
promote diversity. In OMoE, a Gram-Schmidt process is leveraged to enforce that
the experts' representations lie within the Stiefel manifold. By applying
orthogonal constraints directly to the architecture, OMoE keeps the learning
objective unchanged, without compromising optimality. Our method is simple and
alleviates memory bottlenecks, as it incurs minimal experts compared to vanilla
MoE models. Experiments on diverse commonsense reasoning benchmarks demonstrate
that OMoE can consistently achieve stable and efficient performance improvement
when compared with the state-of-the-art methods while significantly reducing
the number of required experts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating <span class="highlight-title">Large Language Model</span>s through Partially Linear Feed-Forward
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gansen Hu, Zhaoguo Wang, Jinglin Wei, Wei Huang, Haibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate remarkable capabilities but face
deployment challenges due to their massive parameter counts. While existing
compression techniques like pruning can reduce model size, it leads to
significant accuracy degradation under high compression ratios. We present a
novel perspective inspired by constant folding in compiler optimization. Our
approach enables parameter reduction by treating activation functions in LLMs
as linear functions.
  However, recent LLMs use complex non-linear activations like GELU that
prevent direct application of this technique. We propose TARDIS, which enables
optimization of LLMs with non-linear activations by partially approximating
them with linear functions in frequently occurring input ranges. For outlier
inputs, TARDIS employs an online predictor to dynamically fall back to original
computations.
  Our experiments demonstrate that TARDIS achieves 80% parameter reduction in
feed-forward networks, while significantly outperforming state-of-the-art
pruning methods Wanda and RIA with up to 65% higher accuracy. In practical
deployments for a 7B model, TARDIS achieves 1.6x end-to-end inference speedup
when integrated with the vLLM serving system, and 1.4x speedup with the widely
adopted HuggingFace implementation, while incurring only a 10.9% accuracy
trade-off.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking student skills real-time through a continuous-variable dynamic
  Bayesian network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hildo Bijl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Knowledge Tracing is focused on predicting the success rate of a
student for a given skill. Modern methods like Deep Knowledge Tracing provide
accurate estimates given enough data, but being based on neural networks they
struggle to explain how these estimates are formed. More classical methods like
Dynamic Bayesian Networks can do this, but they cannot give data on the
accuracy of their estimates and often struggle to incorporate new observations
in real-time due to their high computational load.
  This paper presents a novel method, Performance Distribution Tracing (PDT),
in which the distribution of the success rate is traced live. It uses a Dynamic
Bayesian Network with continuous random variables as nodes. By tracing the
success rate distribution, there is always data available on the accuracy of
any success rate estimation. In addition, it makes it possible to combine data
from similar/related skills to come up with a more informed estimate of success
rates. This makes it possible to predict exercise success rates, providing both
explainability and an accuracy indication, even when an exercise requires a
combination of different skills to solve. And through the use of the beta
distribution functions as conjugate priors, all distributions are available in
analytical form, allowing efficient online updates upon new observations.
Experiments have shown that the resulting estimates generally feel sufficiently
accurate to end-users such that they accept recommendations based on them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PandaSkill -- Player Performance and Skill Rating in Esports:
  Application to League of Legends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime De Bois, Flora Parmentier, Raphaël Puget, Matthew Tanti, Jordan Peltier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To take the esports scene to the next level, we introduce PandaSkill, a
framework for assessing player performance and skill rating. Traditional rating
systems like Elo and TrueSkill often overlook individual contributions and face
challenges in professional esports due to limited game data and fragmented
competitive scenes. PandaSkill leverages machine learning to estimate in-game
player performance from individual player statistics. Each in-game role is
modeled independently, ensuring a fair comparison between them. Then, using
these performance scores, PandaSkill updates the player skill ratings using the
Bayesian framework OpenSkill in a free-for-all setting. In this setting, skill
ratings are updated solely based on performance scores rather than game
outcomes, hightlighting individual contributions. To address the challenge of
isolated rating pools that hinder cross-regional comparisons, PandaSkill
introduces a dual-rating system that combines players' regional ratings with a
meta-rating representing each region's overall skill level. Applying PandaSkill
to five years of professional League of Legends matches worldwide, we show that
our method produces skill ratings that better predict game outcomes and align
more closely with expert opinions compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Virtual Nodes Improve Long-term Traffic Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Cao, Dingyi Zhuang, Jinhua Zhao, Shenhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective traffic prediction is a cornerstone of intelligent transportation
systems, enabling precise forecasts of traffic flow, speed, and congestion.
While traditional spatio-temporal graph neural networks (ST-GNNs) have achieved
notable success in short-term traffic forecasting, their performance in
long-term predictions remains limited. This challenge arises from
over-squashing problem, where bottlenecks and limited receptive fields restrict
information flow and hinder the modeling of global dependencies. To address
these challenges, this study introduces a novel framework that incorporates
virtual nodes, which are additional nodes added to the graph and connected to
existing nodes, in order to aggregate information across the entire graph
within a single GNN layer. Our proposed model incorporates virtual nodes by
constructing a semi-adaptive adjacency matrix. This matrix integrates
distance-based and adaptive adjacency matrices, allowing the model to leverage
geographical information while also learning task-specific features from data.
Experimental results demonstrate that the inclusion of virtual nodes
significantly enhances long-term prediction accuracy while also improving
layer-wise sensitivity to mitigate the over-squashing problem. Virtual nodes
also offer enhanced explainability by focusing on key intersections and
high-traffic areas, as shown by the visualization of their adjacency matrix
weights on road network heat maps. Our advanced approach enhances the
understanding and management of urban traffic systems, making it particularly
well-suited for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Spatiotemporal Augmentation for Improving Dynamic Graph
  Learning <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Chu, Hanlin Xue, Bingce Wang, Xiaoyang Liu, Weiping Li, Tong Mo, Tuoyu Feng, Zhijie Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graph augmentation is used to improve the performance of dynamic
GNNs. Most methods assume temporal locality, meaning that recent edges are more
influential than earlier edges. However, for temporal changes in edges caused
by random noise, overemphasizing recent edges while neglecting earlier ones may
lead to the model capturing noise. To address this issue, we propose STAA
(SpatioTemporal Activity-Aware Random Walk Diffusion). STAA identifies nodes
likely to have noisy edges in spatiotemporal dimensions. Spatially, it analyzes
critical topological positions through graph wavelet coefficients. Temporally,
it analyzes edge evolution through graph wavelet coefficient change rates.
Then, random walks are used to reduce the weights of noisy edges, deriving a
diffusion matrix containing spatiotemporal information as an augmented
adjacency matrix for dynamic GNN learning. Experiments on multiple datasets
show that STAA outperforms other dynamic graph augmentation methods in node
classification and link prediction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE International Conference on Acoustics, Speech, and Signal
  Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RichSpace: Enriching Text-to-Video <span class="highlight-title">Prompt</span> Space via Text Embedding
  Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuefan Cao, Chengyue Gong, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video generation models have made impressive progress, but they still
struggle with generating videos with complex features. This limitation often
arises from the inability of the text encoder to produce accurate embeddings,
which hinders the video generation model. In this work, we propose a novel
approach to overcome this challenge by selecting the optimal text embedding
through interpolation in the embedding space. We demonstrate that this method
enables the video generation model to produce the desired videos. Additionally,
we introduce a simple algorithm using perpendicular foot embeddings and cosine
similarity to identify the optimal interpolation embedding. Our findings
highlight the importance of accurate text embeddings and offer a pathway for
improving text-to-video generation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aneumo: A Large-Scale Comprehensive Synthetic Dataset of Aneurysm
  Hemodynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xigui Li, Yuanye Zhou, Feiyang Xiao, Xin Guo, Yichi Zhang, Chen Jiang, Jianchao Ge, Xiansheng Wang, Qimeng Wang, Taiwei Zhang, Chensen Lin, Yuan Cheng, Yuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intracranial aneurysm (IA) is a common cerebrovascular disease that is
usually asymptomatic but may cause severe subarachnoid hemorrhage (SAH) if
ruptured. Although clinical practice is usually based on individual factors and
morphological features of the aneurysm, its pathophysiology and hemodynamic
mechanisms remain controversial. To address the limitations of current
research, this study constructed a comprehensive hemodynamic dataset of
intracranial aneurysms. The dataset is based on 466 real aneurysm models, and
10,000 synthetic models were generated by resection and deformation operations,
including 466 aneurysm-free models and 9,534 deformed aneurysm models. The
dataset also provides medical image-like segmentation mask files to support
insightful analysis. In addition, the dataset contains hemodynamic data
measured at eight steady-state flow rates (0.001 to 0.004 kg/s), including
critical parameters such as flow velocity, pressure, and wall shear stress,
providing a valuable resource for investigating aneurysm pathogenesis and
clinical prediction. This dataset will help advance the understanding of the
pathologic features and hemodynamic mechanisms of intracranial aneurysms and
support in-depth research in related fields. Dataset hosted at
https://github.com/Xigui-Li/Aneumo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable artificial intelligence (XAI): from inherent explainability
  to <span class="highlight-title">large language model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuseini Mumuni, Alhassan Mumuni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has continued to achieve tremendous success in
recent times. However, the decision logic of these frameworks is often not
transparent, making it difficult for stakeholders to understand, interpret or
explain their behavior. This limitation hinders trust in machine learning
systems and causes a general reluctance towards their adoption in practical
applications, particularly in mission-critical domains like healthcare and
autonomous driving. Explainable AI (XAI) techniques facilitate the
explainability or interpretability of machine learning models, enabling users
to discern the basis of the decision and possibly avert undesirable behavior.
This comprehensive survey details the advancements of explainable AI methods,
from inherently interpretable models to modern approaches for achieving
interpretability of various black box models, including large language models
(LLMs). Additionally, we review explainable AI techniques that leverage LLM and
vision-language model (VLM) frameworks to automate or improve the
explainability of other machine learning models. The use of LLM and VLM as
interpretability methods particularly enables high-level, semantically
meaningful explanations of model decisions and behavior. Throughout the paper,
we highlight the scientific principles, strengths and weaknesses of
state-of-the-art methods and outline different areas of improvement. Where
appropriate, we also present qualitative and quantitative comparison results of
various methods to show how they compare. Finally, we discuss the key
challenges of XAI and directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIRCHITECT v2: Learning the Hardware Accelerator Design Space through
  Unified Representations <span class="chip">DATE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamin Seo, Akshat Ramachandran, Yu-Chuan Chuang, Anirudh Itagi, Tushar Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Design space exploration (DSE) plays a crucial role in enabling custom
hardware architectures, particularly for emerging applications like AI, where
optimized and specialized designs are essential. With the growing complexity of
deep neural networks (DNNs) and the introduction of advanced foundational
models (FMs), the design space for DNN accelerators is expanding at an
exponential rate. Additionally, this space is highly non-uniform and
non-convex, making it increasingly difficult to navigate and optimize.
Traditional DSE techniques rely on search-based methods, which involve
iterative sampling of the design space to find the optimal solution. However,
this process is both time-consuming and often fails to converge to the global
optima for such design spaces. Recently, AIrchitect v1, the first attempt to
address the limitations of search-based techniques, transformed DSE into a
constant-time classification problem using recommendation networks. In this
work, we propose AIrchitect v2, a more accurate and generalizable
learning-based DSE technique applicable to large-scale design spaces that
overcomes the shortcomings of earlier approaches. Specifically, we devise an
encoder-decoder transformer model that (a) encodes the complex design space
into a uniform intermediate representation using contrastive learning and (b)
leverages a novel unified representation blending the advantages of
classification and regression to effectively explore the large DSE space
without sacrificing accuracy. Experimental results evaluated on 10^5 real DNN
workloads demonstrate that, on average, AIrchitect v2 outperforms existing
techniques by 15% in identifying optimal design points. Furthermore, to
demonstrate the generalizability of our method, we evaluate performance on
unseen model workloads (LLMs) and attain a 1.7x improvement in inference
latency on the identified hardware architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to DATE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiPruner: Balanced Structure Removal in Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J. Pablo Muñoz, Jinjie Yuan, Nilesh Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, state-of-the-art approaches for pruning large pre-trained models
(LPMs) have demonstrated that the training-free removal of non-critical
residual blocks in Transformers is viable for reducing model size, achieving
results that outperform previous training-free pruning approaches. Motivated by
these findings, we extend BlockPruner (Zhong et al., 2024) and propose
MultiPruner, a pruning approach that surpasses recent training-free pruning
methods by adopting a multidimensional, iterative, fine-grained pruning
strategy. In MultiPruner, multidimensional pruning reinstates the structural
balance in block-pruned models by sequentially compressing along three
dimensions: i) residual blocks, ii) channels of multilayer perceptrons (MLP),
and iii) attention heads. This solution enhances zero-shot accuracy on
downstream tasks compared to other techniques while improving model compression
ratios, producing compressed models with fewer computing and memory
requirements. Extensive experiments demonstrate the advantages of the proposed
method across various large pre-trained models. The code and pruning
configurations are available at
https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Client-Centric Federated Adaptive Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhui Sun, Xidong Wu, Heng Huang, Aidong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed learning paradigm where clients
collaboratively train a model while keeping their own data private. With an
increasing scale of clients and models, FL encounters two key challenges,
client drift due to a high degree of statistical/system heterogeneity, and lack
of adaptivity. However, most existing FL research is based on unrealistic
assumptions that virtually ignore system heterogeneity. In this paper, we
propose Client-Centric Federated Adaptive Optimization, which is a class of
novel federated adaptive optimization approaches. We enable several features in
this framework such as arbitrary client participation, asynchronous server
aggregation, and heterogeneous local computing, which are ubiquitous in
real-world FL systems but are missed in most existing works. We provide a
rigorous convergence analysis of our proposed framework for general nonconvex
objectives, which is shown to converge with the best-known rate. Extensive
experiments show that our approaches consistently outperform the baseline by a
large margin across benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Scale Feature Extraction and Fusion Deep Learning Method for
  Classification of Wheat Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajjad Saleem, Adil Hussain, Nabila Majeed, Zahid Akhtar, Kamran Siddique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wheat is an important source of dietary fiber and protein that is negatively
impacted by a number of risks to its growth. The difficulty of identifying and
classifying wheat diseases is discussed with an emphasis on wheat loose smut,
leaf rust, and crown and root rot. Addressing conditions like crown and root
rot, this study introduces an innovative approach that integrates multi-scale
feature extraction with advanced image segmentation techniques to enhance
classification accuracy. The proposed method uses neural network models
Xception, Inception V3, and ResNet 50 to train on a large wheat disease
classification dataset 2020 in conjunction with an ensemble of machine vision
classifiers, including voting and stacking. The study shows that the suggested
methodology has a superior accuracy of 99.75% in the classification of wheat
diseases when compared to current state-of-the-art approaches. A deep learning
ensemble model Xception showed the highest accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HEART: Achieving Timely Multi-Model Training for
  Vehicle-Edge-Cloud-Integrated Hierarchical Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohong Yang, Minghui Liwang, Xianbin Wang, Zhipeng Cheng, Seyyedali Hosseinalipour, Huaiyu Dai, Zhenzhen Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of AI-enabled Internet of Vehicles (IoV) calls for efficient
machine learning (ML) solutions that can handle high vehicular mobility and
decentralized data. This has motivated the emergence of Hierarchical Federated
Learning over vehicle-edge-cloud architectures (VEC-HFL). Nevertheless, one
aspect which is underexplored in the literature on VEC-HFL is that vehicles
often need to execute multiple ML tasks simultaneously, where this multi-model
training environment introduces crucial challenges. First, improper aggregation
rules can lead to model obsolescence and prolonged training times. Second,
vehicular mobility may result in inefficient data utilization by preventing the
vehicles from returning their models to the network edge. Third, achieving a
balanced resource allocation across diverse tasks becomes of paramount
importance as it majorly affects the effectiveness of collaborative training.
We take one of the first steps towards addressing these challenges via
proposing a framework for multi-model training in dynamic VEC-HFL with the goal
of minimizing global training latency while ensuring balanced training across
various tasks-a problem that turns out to be NP-hard. To facilitate timely
model training, we introduce a hybrid synchronous-asynchronous aggregation
rule. Building on this, we present a novel method called Hybrid Evolutionary
And gReedy allocaTion (HEART). The framework operates in two stages: first, it
achieves balanced task scheduling through a hybrid heuristic approach that
combines improved Particle Swarm Optimization (PSO) and Genetic Algorithms
(GA); second, it employs a low-complexity greedy algorithm to determine the
training priority of assigned tasks on vehicles. Experiments on real-world
datasets demonstrate the superiority of HEART over existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Statistical Inference for Sequential Feature Selection after Domain
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duong Tan Loc, Nguyen Thang Loi, Vo Nguyen Le Duy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In high-dimensional regression, feature selection methods, such as sequential
feature selection (SeqFS), are commonly used to identify relevant features.
When data is limited, domain adaptation (DA) becomes crucial for transferring
knowledge from a related source domain to a target domain, improving
generalization performance. Although SeqFS after DA is an important task in
machine learning, none of the existing methods can guarantee the reliability of
its results. In this paper, we propose a novel method for testing the features
selected by SeqFS-DA. The main advantage of the proposed method is its
capability to control the false positive rate (FPR) below a significance level
$\alpha$ (e.g., 0.05). Additionally, a strategic approach is introduced to
enhance the statistical power of the test. Furthermore, we provide extensions
of the proposed method to SeqFS with model selection criteria including AIC,
BIC, and adjusted R-squared. Extensive experiments are conducted on both
synthetic and real-world datasets to validate the theoretical results and
demonstrate the proposed method's superior performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering <span class="highlight-title">Large Language Model</span>s with Feature Guided Activation Additions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Soo, Wesley Teng, Chandrasekaran Balaganesh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective and reliable control over large language model (LLM) behavior is a
significant challenge. While activation steering methods, which add steering
vectors to a model's hidden states, are a promising approach, existing
techniques often lack precision and interpretability in how they influence
model outputs. We introduce Feature Guided Activation Additions (FGAA), a novel
activation steering method that leverages insights from Contrastive Activation
Addition (CAA) and Sparse Autoencoder-Targeted Steering (SAE-TS). By operating
in the latent space of a Sparse Autoencoder (SAE) and employing optimization
techniques to select desired SAE features, FGAA constructs precise steering
vectors that provide better steering effects while maintaining coherence of
steered model outputs. In this regard, evaluations on Gemma-2-2B and Gemma-2-9B
models across various steering tasks demonstrate that FGAA outperforms existing
steering methods of CAA, SAE decoder steering, and SAE-TS. Our results also
highlight important trade-offs between steering scale and general model
capabilities that are consistent across all tested steering methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 maintext pages, 14 appendix pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Study on a Fast Solver for Combined Field Integral Equations of 3D
  Conducting Bodies Based on Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Shan, Xin Zhang, Di Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a graph neural networks (GNNs)-based fast solver
(GraphSolver) for solving combined field integral equations (CFIEs) of 3D
conducting bodies. Rao-Wilton-Glisson (RWG) basis functions are employed to
discretely and accurately represent the geometry of 3D conducting bodies. A
concise and informative graph representation is then constructed by treating
each RWG function as a node in the graph, enabling the flow of current between
nodes. With the transformed graphs, GraphSolver is developed to directly
predict real and imaginary parts of the x, y and z components of the surface
current densities at each node (RWG function). Numerical results demonstrate
the efficacy of GraphSolver in solving CFIEs for 3D conducting bodies with
varying levels of geometric complexity, including basic 3D targets,
missile-shaped targets, and airplane-shaped targets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLIM: Sim-to-Real Legged Instructive Manipulation via Long-Horizon
  Visuomotor Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Yiqing Yang, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a low-cost quadruped manipulation system that solves long-horizon
real-world tasks, trained by reinforcement learning purely in simulation. The
system comprises 1) a hierarchical design of a high-level policy for
visual-mobile manipulation following instructions, and a low-level policy for
quadruped movement and limb-control, 2) a progressive policy expansion approach
for solving the long-horizon task together with a teacher-student framework for
efficient high-level training of the high-level visuomotor policy, and 3) a
suite of techniques for minimizing sim-to-real gaps.
  With budget-friendly but limited reliability and performance hardware, and
just one wrist-mounted RGB camera, the entire system fully trained in
simulation achieves high success rates for long horizon tasks involving search,
move, grasp, and drop-into, with fluid sim-to-real transfer in a wide variety
of indoor and outdoor scenes and lighting conditions.Extensive real-world
evaluations show that on the long horizon mobile manipulation tasks, our system
achieves good performance when transferred to real both in terms of task
success rate and execution efficiency. Finally, we discuss the necessity of our
sim-to-real techniques for legged mobile manipulation, and show their ablation
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SBAMDT: Bayesian Additive Decision Trees with Adaptive Soft
  Semi-multivariate Split Rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stamatina Lamprinakou, Huiyan Sang, Bledar A. Konomi, Ligang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Additive Regression Trees [BART, Chipman et al., 2010] have gained
significant popularity due to their remarkable predictive performance and
ability to quantify uncertainty. However, standard decision tree models rely on
recursive data splits at each decision node, using deterministic decision rules
based on a single univariate feature. This approach limits their ability to
effectively capture complex decision boundaries, particularly in scenarios
involving multiple features, such as spatial domains, or when transitions are
either sharp or smoothly varying. In this paper, we introduce a novel
probabilistic additive decision tree model that employs a soft split rule. This
method enables highly flexible splits that leverage both univariate and
multivariate features, while also respecting the geometric properties of the
feature domain. Notably, the probabilistic split rule adapts dynamically across
decision nodes, allowing the model to account for varying levels of smoothness
in the regression function. We demonstrate the utility of the proposed model
through comparisons with existing tree-based models on synthetic datasets and a
New York City education dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FoundationStereo: Zero-Shot Stereo Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tremendous progress has been made in deep stereo matching to excel on
benchmark datasets through per-domain fine-tuning. However, achieving strong
zero-shot generalization - a hallmark of foundation models in other computer
vision tasks - remains challenging for stereo matching. We introduce
FoundationStereo, a foundation model for stereo depth estimation designed to
achieve strong zero-shot generalization. To this end, we first construct a
large-scale (1M stereo pairs) synthetic training dataset featuring large
diversity and high photorealism, followed by an automatic self-curation
pipeline to remove ambiguous samples. We then design a number of network
architecture components to enhance scalability, including a side-tuning feature
backbone that adapts rich monocular priors from vision foundation models to
mitigate the sim-to-real gap, and long-range context reasoning for effective
cost volume filtering. Together, these components lead to strong robustness and
accuracy across domains, establishing a new standard in zero-shot stereo depth
estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Binary Representation Learning for Knowledge Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahya Badran, Christine Preisach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge tracing (KT) models aim to predict students' future performance
based on their historical interactions. Most existing KT models rely
exclusively on human-defined knowledge concepts (KCs) associated with
exercises. As a result, the effectiveness of these models is highly dependent
on the quality and completeness of the predefined KCs. Human errors in labeling
and the cost of covering all potential underlying KCs can limit model
performance.
  In this paper, we propose a KT model, Sparse Binary Representation KT
(SBRKT), that generates new KC labels, referred to as auxiliary KCs, which can
augment the predefined KCs to address the limitations of relying solely on
human-defined KCs. These are learned through a binary vector representation,
where each bit indicates the presence (one) or absence (zero) of an auxiliary
KC. The resulting discrete representation allows these auxiliary KCs to be
utilized in training any KT model that incorporates KCs. Unlike pre-trained
dense embeddings, which are limited to models designed to accept such vectors,
our discrete representations are compatible with both classical models, such as
Bayesian Knowledge Tracing (BKT), and modern deep learning approaches.
  To generate this discrete representation, SBRKT employs a binarization method
that learns a sparse representation, fully trainable via stochastic gradient
descent. Additionally, SBRKT incorporates a recurrent neural network (RNN) to
capture temporal dynamics and predict future student responses by effectively
combining the auxiliary and predefined KCs. Experimental results demonstrate
that SBRKT outperforms the tested baselines on several datasets and achieves
competitive performance on others. Furthermore, incorporating the learned
auxiliary KCs consistently enhances the performance of BKT across all tested
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fairness-Oriented Reinforcement Learning Approach for the Operation
  and Control of Shared Micromobility Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15780v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15780v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Cederle, Luca Vittorio Piron, Marina Ceccon, Federico Chiariotti, Alessandro Fabris, Marco Fabris, Gian Antonio Susto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Machine Learning grows in popularity across various fields, equity has
become a key focus for the AI community. However, fairness-oriented approaches
are still underexplored in smart mobility. Addressing this gap, our study
investigates the balance between performance optimization and algorithmic
fairness in shared micromobility services providing a novel framework based on
Reinforcement Learning. Exploiting Q-learning, the proposed methodology
achieves equitable outcomes in terms of the Gini index across different areas
characterized by their distance from central hubs. Through vehicle rebalancing,
the provided scheme maximizes operator performance while ensuring fairness
principles for users, reducing iniquity by up to 85% while only increasing
costs by 30% (w.r.t. applying no equity adjustment). A case study with
synthetic data validates our insights and highlights the importance of fairness
in urban micromobility (source code:
https://github.com/mcederle99/FairMSS.git).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, accepted at the 2025 American Control Conference
  (ACC) on January 17th, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Learning Informative Trajectory Embeddings for Imitation,
  Classification and Regression <span class="chip">AAMAS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichang Ge, Changyu Chen, Arunesh Sinha, Pradeep Varakantham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world sequential decision making tasks like autonomous driving,
robotics, and healthcare, learning from observed state-action trajectories is
critical for tasks like imitation, classification, and clustering. For example,
self-driving cars must replicate human driving behaviors, while robots and
healthcare systems benefit from modeling decision sequences, whether or not
they come from expert data. Existing trajectory encoding methods often focus on
specific tasks or rely on reward signals, limiting their ability to generalize
across domains and tasks. Inspired by the success of embedding models like CLIP
and BERT in static domains, we propose a novel method for embedding
state-action trajectories into a latent space that captures the skills and
competencies in the dynamic underlying decision-making processes. This method
operates without the need for reward labels, enabling better generalization
across diverse domains and tasks. Our contributions are threefold: (1) We
introduce a trajectory embedding approach that captures multiple abilities from
state-action data. (2) The learned embeddings exhibit strong representational
power across downstream tasks, including imitation, classification, clustering,
and regression. (3) The embeddings demonstrate unique properties, such as
controlling agent behaviors in IQ-Learn and an additive structure in the latent
space. Experimental results confirm that our method outperforms traditional
approaches, offering more flexible and powerful trajectory representations for
various applications. Our code is available at
https://github.com/Erasmo1015/vte.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAMAS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic gradient descent for streaming linear and rectified linear
  systems with adversarial corruptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Halyun Jeong, Deanna Needell, Elizaveta Rebrova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SGD-exp, a stochastic gradient descent approach for linear and
ReLU regressions under Massart noise (adversarial semi-random corruption model)
for the fully streaming setting. We show novel nearly linear convergence
guarantees of SGD-exp to the true parameter with up to $50\%$ Massart
corruption rate, and with any corruption rate in the case of symmetric
oblivious corruptions. This is the first convergence guarantee result for
robust ReLU regression in the streaming setting, and it shows the improved
convergence rate over previous robust methods for $L_1$ linear regression due
to a choice of an exponentially decaying step size, known for its efficiency in
practice. Our analysis is based on the drift analysis of a discrete stochastic
process, which could also be interesting on its own.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STPOTR: Simultaneous Human Trajectory and Pose Prediction Using a
  Non-Autoregressive <span class="highlight-title">Transformer</span> for Robot Following Ahead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.07600v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.07600v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdavian, Payam Nikdel, Mahdi TaherAhmadi, Mo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we develop a neural network model to predict future human
motion from an observed human motion history. We propose a non-autoregressive
transformer architecture to leverage its parallel nature for easier training
and fast, accurate predictions at test time. The proposed architecture divides
human motion prediction into two parts: 1) the human trajectory, which is the
hip joint 3D position over time and 2) the human pose which is the all other
joints 3D positions over time with respect to a fixed hip joint. We propose to
make the two predictions simultaneously, as the shared representation can
improve the model performance. Therefore, the model consists of two sets of
encoders and decoders. First, a multi-head attention module applied to encoder
outputs improves human trajectory. Second, another multi-head self-attention
module applied to encoder outputs concatenated with decoder outputs facilitates
learning of temporal dependencies. Our model is well-suited for robotic
applications in terms of test accuracy and speed, and compares favorably with
respect to state-of-the-art methods. We demonstrate the real-world
applicability of our work via the Robot Follow-Ahead task, a challenging yet
practical case study for our proposed model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effect of Similarity Measures on Accurate Stability Estimates for
  Local Surrogate Models in Text-based Explainable AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Burger, Charles Walter, Thai Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has investigated the vulnerability of local surrogate methods to
adversarial perturbations on a machine learning (ML) model's inputs, where the
explanation is manipulated while the meaning and structure of the original
input remains similar under the complex model. Although weaknesses across many
methods have been shown to exist, the reasons behind why remain little
explored. Central to the concept of adversarial attacks on explainable AI (XAI)
is the similarity measure used to calculate how one explanation differs from
another. A poor choice of similarity measure can lead to erroneous conclusions
on the efficacy of an XAI method. Too sensitive a measure results in
exaggerated vulnerability, while too coarse understates its weakness. We
investigate a variety of similarity measures designed for text-based ranked
lists, including Kendall's Tau, Spearman's Footrule, and Rank-biased Overlap to
determine how substantial changes in the type of measure or threshold of
success affect the conclusions generated from common adversarial attack
processes. Certain measures are found to be overly sensitive, resulting in
erroneous estimates of stability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 Tables (Minor edits for clarity and grammar)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Rank Irreducible Cartesian Tensor Decomposition and Bases of
  Equivariant Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18263v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18263v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Shao, Yikang Li, Zhouchen Lin, Qinghua Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Irreducible Cartesian tensors (ICTs) play a crucial role in the design of
equivariant graph neural networks, as well as in theoretical chemistry and
chemical physics. Meanwhile, the design space of available linear operations on
tensors that preserve symmetry presents a significant challenge. The ICT
decomposition and a basis of this equivariant space are difficult to obtain for
high-rank tensors. After decades of research, Bonvicini (2024) recently
achieves an explicit ICT decomposition for $n=5$ with factorial time/space
complexity. In this work we, for the first time, obtains decomposition matrices
for ICTs up to rank $n=9$ with reduced and affordable complexity, by
constructing what we call path matrices. The path matrices are obtained via
performing chain-like contractions with Clebsch-Gordan matrices following the
parentage scheme. We prove and leverage that the concatenation of path matrices
is an orthonormal change-of-basis matrix between the Cartesian tensor product
space and the spherical direct sum spaces. Furthermore, we identify a complete
orthogonal basis for the equivariant space, rather than a spanning set
(Pearce-Crump, 2023), through this path matrices technique. To the best of our
knowledge, this is also the first analytic, rather than numerical, method for
theoretically obtaining arbitrary rank orthogonal ICT decomposition matrices
and orthogonal equivariant bases. We further extend our result to the arbitrary
tensor product and direct sum spaces, enabling free design between different
spaces while keeping symmetry. The Python code is available at
https://github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases, where
the $n=6,\dots,9$ ICT decomposition matrices are obtained in 1s, 3s, 11s, and
4m32s on 28-cores Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GHz, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Multi-hop Traffic Pressure for Heterogeneous Traffic
  Perimeter Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaocan Li, Xiaoyu Wang, Ilia Smirnov, Scott Sanner, Baher Abdulhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perimeter control (PC) prevents loss of traffic network capacity due to
congestion in urban areas. Homogeneous PC allows all access points to a
protected region to have identical permitted inflow. However, homogeneous PC
performs poorly when the congestion in the protected region is heterogeneous
(e.g., imbalanced demand) since the homogeneous PC does not consider specific
traffic conditions around each perimeter intersection. When the protected
region has spatially heterogeneous congestion, one needs to modulate the
perimeter inflow rate to be higher near low-density regions and vice versa for
high-density regions. A na\"ive approach is to leverage 1-hop traffic pressure
to measure traffic condition around perimeter intersections, but such metric is
too spatially myopic for PC. To address this issue, we formulate multi-hop
downstream pressure grounded on Markov chain theory, which ``looks deeper''
into the protected region beyond perimeter intersections. In addition, we
formulate a two-stage hierarchical control scheme that can leverage this novel
multi-hop pressure to redistribute the total permitted inflow provided by a
pre-trained deep reinforcement learning homogeneous control policy.
Experimental results show that our heterogeneous PC approaches leveraging
multi-hop pressure significantly outperform homogeneous PC in scenarios where
the origin-destination flows are highly imbalanced with high spatial
heterogeneity. Moveover, our approach is shown to be robust against turning
ratio uncertainties by a sensitivity analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages main body, 13 figures, journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Types of AI Existential Risk: Decisive and Accumulative 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07836v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07836v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atoosa Kasirzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional discourse on existential risks (x-risks) from AI typically
focuses on abrupt, dire events caused by advanced AI systems, particularly
those that might achieve or surpass human-level intelligence. These events have
severe consequences that either lead to human extinction or irreversibly
cripple human civilization to a point beyond recovery. This discourse, however,
often neglects the serious possibility of AI x-risks manifesting incrementally
through a series of smaller yet interconnected disruptions, gradually crossing
critical thresholds over time. This paper contrasts the conventional "decisive
AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the
former envisions an overt AI takeover pathway, characterized by scenarios like
uncontrollable superintelligence, the latter suggests a different causal
pathway to existential catastrophes. This involves a gradual accumulation of
critical AI-induced threats such as severe vulnerabilities and systemic erosion
of economic and political structures. The accumulative hypothesis suggests a
boiling frog scenario where incremental AI risks slowly converge, undermining
societal resilience until a triggering event results in irreversible collapse.
Through systems analysis, this paper examines the distinct assumptions
differentiating these two hypotheses. It is then argued that the accumulative
view can reconcile seemingly incompatible perspectives on AI risks. The
implications of differentiating between these causal pathways -- the decisive
and the accumulative -- for the governance of AI as well as long-term AI safety
are discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal article for Philosophical Studies</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Counterfactual Uncertainty Quantification of Factual Estimand of
  Efficacy from Before-and-After Treatment Repeated Measures Randomized
  Controlled Trials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09635v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09635v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingya Wang, Yang Han, Yushi Liu, Szu-Yu Tang, Jason C. Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ideal estimand for comparing treatment $Rx$ with a control $C$ is the
$\textit{counterfactual}$ efficacy $Rx:C$, the expected differential outcome
between $Rx$ and $C$ if each patient were given $\textit{both}$. One hundred
years ago, Neyman (1923a) proved unbiased $\textit{point estimation}$ of
counterfactual efficacy from designed $\textit{factual}$ experiments is
achievable. But he left the determination of how much might the counterfactual
variance of this estimate be smaller than the factual variance as an open
challenge. This article shows $\textit{counterfactual}$ uncertainty
quantification (CUQ), quantifying uncertainty for factual point estimates but
in a counterfactual setting, is achievable for Randomized Controlled Trials
(RCTs) with Before-and-After treatment Repeated Measures which are common in
many therapeutic areas. We achieve CUQ whose variability is typically smaller
than factual UQ by creating a new statistical modeling principle called ETZ.
  We urge caution in using predictors with measurement error which violates
standard regression assumption and can cause $\textit{attenuation}$ in
estimating treatment effects. Fortunately, we prove that, for traditional
medicine in general, and for targeted therapy with efficacy defined as averaged
over the population, counterfactual point estimation is unbiased. However, for
both Real Human and Digital Twins approaches, predicting treatment effect in
$\textit{subgroups}$ may have attenuation bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Machine Learning for Remaining Useful Life Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc-André Zöller, Fabian Mauthe, Peter Zeiler, Marius Lindauer, Marco F. Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Being able to predict the remaining useful life (RUL) of an engineering
system is an important task in prognostics and health management. Recently,
data-driven approaches to RUL predictions are becoming prevalent over
model-based approaches since no underlying physical knowledge of the
engineering system is required. Yet, this just replaces required expertise of
the underlying physics with machine learning (ML) expertise, which is often
also not available. Automated machine learning (AutoML) promises to build
end-to-end ML pipelines automatically enabling domain experts without ML
expertise to create their own models. This paper introduces AutoRUL, an
AutoML-driven end-to-end approach for automatic RUL predictions. AutoRUL
combines fine-tuned standard regression methods to an ensemble with high
predictive power. By evaluating the proposed method on eight real-world and
synthetic datasets against state-of-the-art hand-crafted models, we show that
AutoML provides a viable alternative to hand-crafted data-driven RUL
predictions. Consequently, creating RUL predictions can be made more accessible
for domain experts using AutoML by eliminating ML expertise from data-driven
model construction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript accepted at IEEE SMC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing reliability in prediction intervals using point forecasters:
  Heteroscedastic Quantile Regression and Width-Adaptive Conformal Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Sebastián, Carlos E. González-Guillén, Jesús Juan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing prediction intervals for time series forecasting is challenging,
particularly when practitioners rely solely on point forecasts. While previous
research has focused on creating increasingly efficient intervals, we argue
that standard measures alone are inadequate. Beyond efficiency, prediction
intervals must adapt their width based on the difficulty of the prediction
while preserving coverage regardless of complexity. To address these issues, we
propose combining Heteroscedastic Quantile Regression (HQR) with Width-Adaptive
Conformal Inference (WACI). This integrated procedure guarantees theoretical
coverage and enables interval widths to vary with predictive uncertainty. We
assess its performance using both a synthetic example and a real world
Electricity Price Forecasting scenario. Our results show that this combined
approach meets or surpasses typical benchmarks for validity and efficiency,
while also fulfilling important yet often overlooked practical requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can machine learning unlock new insights into high-frequency trading? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08101v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08101v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. Ibikunle, B. Moews, D. Muravyev, K. Rzayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We design and train machine learning models to capture the nonlinear
interactions between financial market dynamics and high-frequency trading (HFT)
activity. In doing so, we introduce new metrics to identify liquidity-demanding
and -supplying HFT strategies. Both types of HFT strategies increase activity
in response to information events and decrease it when trading speed is
restricted, with liquidity-supplying strategies demonstrating greater
responsiveness. Liquidity-demanding HFT is positively linked with latency
arbitrage opportunities, whereas liquidity-supplying HFT is negatively related,
aligning with theoretical expectations. Our metrics have implications for
understanding the information production process in financial markets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>66 pages, 6 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Large Language Model</span> is Secretly a Protein Sequence Optimizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinkai Wang, Jiaxing He, Yuanqi Du, Xiaohui Chen, Jianan Canal Li, Li-Ping Liu, Xiaolin Xu, Soha Hassoun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the protein sequence engineering problem, which aims to find
protein sequences with high fitness levels, starting from a given wild-type
sequence. Directed evolution has been a dominating paradigm in this field which
has an iterative process to generate variants and select via experimental
feedback. We demonstrate large language models (LLMs), despite being trained on
massive texts, are secretly protein sequence optimizers. With a directed
evolutionary method, LLM can perform protein engineering through Pareto and
experiment-budget constrained optimization, demonstrating success on both
synthetic and experimental fitness landscapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging Diversity and Uncertainty in Active learning with
  <span class="highlight-title">Self-Supervised</span> <span class="highlight-title">Pre-Train</span>ing <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Doucet, Benjamin Estermann, Till Aczel, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the integration of diversity-based and uncertainty-based
sampling strategies in active learning, particularly within the context of
self-supervised pre-trained models. We introduce a straightforward heuristic
called TCM that mitigates the cold start problem while maintaining strong
performance across various data levels. By initially applying TypiClust for
diversity sampling and subsequently transitioning to uncertainty sampling with
Margin, our approach effectively combines the strengths of both strategies. Our
experiments demonstrate that TCM consistently outperforms existing methods
across various datasets in both low and high data regimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024 Workshop on Practical Machine Learning for Low
  Resource Settings (PML4LRS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Quantization for Matrix Multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Ordentlich, Yury Polyanskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work in machine learning community proposed multiple methods for
performing lossy compression (quantization) of large matrices. This
quantization is important for accelerating matrix multiplication (main
component of large language models), which is often bottlenecked by the speed
of loading these matrices from memory. Unlike classical vector quantization and
rate-distortion theory, the goal of these new compression algorithms is to be
able to approximate not the matrices themselves, but their matrix product.
Specifically, given a pair of real matrices $A,B$ an encoder (compressor) is
applied to each of them independently producing descriptions with $R$ bits per
entry. These representations subsequently are used by the decoder to estimate
matrix product $A^\top B$. In this work, we provide a non-asymptotic lower
bound on the mean squared error of this approximation (as a function of rate
$R$) for the case of matrices $A,B$ with iid Gaussian entries. Algorithmically,
we construct a universal quantizer based on nested lattices with an explicit
guarantee of approximation error for any (non-random) pair of matrices $A$, $B$
in terms of only Frobenius norms $\|\bar{A}\|_F, \|\bar{B}\|_F$ and
$\|\bar{A}^\top \bar{B}\|_F$, where $\bar{A},\bar{B}$ are versions of $A,B$
with zero-centered columns, respectively. For iid Gaussian matrices our
quantizer achieves the lower bound and is, thus, asymptotically optimal. A
practical low-complexity version of our quantizer achieves performance quite
close to optimal. In addition, we derive rate-distortion function for matrix
multiplication of iid Gaussian matrices, which exhibits an interesting
phase-transition at $R\approx 0.906$ bit/entry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DPCL-Diff: The Temporal <span class="highlight-title">Knowledge Graph</span> Reasoning Based on Graph Node
  Diffusion Model with Dual-Domain Periodic Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Cao, Lisheng Wang, Luobin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal knowledge graph (TKG) reasoning that infers future missing facts is
an essential and challenging task. Predicting future events typically relies on
closely related historical facts, yielding more accurate results for repetitive
or periodic events. However, for future events with sparse historical
interactions, the effectiveness of this method, which focuses on leveraging
high-frequency historical information, diminishes. Recently, the capabilities
of diffusion models in image generation have opened new opportunities for TKG
reasoning. Therefore, we propose a graph node diffusion model with dual-domain
periodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff)
introduces noise into sparsely related events to simulate new events,
generating high-quality data that better conforms to the actual distribution.
This generative mechanism significantly enhances the model's ability to reason
about new events. Additionally, the dual-domain periodic contrastive learning
(DPCL) maps periodic and non-periodic event entities to Poincar\'e and
Euclidean spaces, leveraging their characteristics to distinguish similar
periodic events effectively. Experimental results on four public datasets
demonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG
models in event prediction, demonstrating our approach's effectiveness. This
study also investigates the combined effectiveness of GNDiff and DPCL in TKG
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jailbreaking as a Reward Misspecification Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14393v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14393v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihui Xie, Jiahui Gao, Lei Li, Zhenguo Li, Qi Liu, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of large language models (LLMs) has raised concerns
about their safety and reliability, particularly regarding their vulnerability
to adversarial attacks. In this paper, we propose a novel perspective that
attributes this vulnerability to reward misspecification during the alignment
process. This misspecification occurs when the reward function fails to
accurately capture the intended behavior, leading to misaligned model outputs.
We introduce a metric ReGap to quantify the extent of reward misspecification
and demonstrate its effectiveness and robustness in detecting harmful backdoor
prompts. Building upon these insights, we present ReMiss, a system for
automated red teaming that generates adversarial prompts in a
reward-misspecified space. ReMiss achieves state-of-the-art attack success
rates on the AdvBench benchmark against various target aligned LLMs while
preserving the human readability of the generated prompts. Furthermore, these
attacks on open-source models demonstrate high transferability to closed-source
models like GPT-4o and out-of-distribution tasks from HarmBench. Detailed
analysis highlights the unique advantages of the proposed reward
misspecification objective compared to previous methods, offering new insights
for improving LLM safety and robustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting drug-disease association prediction for drug repositioning via
  dual-feature extraction and cross-dual-domain decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11812v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11812v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enqiang Zhu, Xiang Li, Chanjuan Liu, Nikhil R. Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraction of biomedical data has significant academic and practical
value in contemporary biomedical sciences. In recent years, drug repositioning,
a cost-effective strategy for drug development by discovering new indications
for approved drugs, has gained increasing attention. However, many existing
drug repositioning methods focus on mining information from adjacent nodes in
biomedical networks without considering the potential inter-relationships
between the feature spaces of drugs and diseases. This can lead to inaccurate
encoding, resulting in biased mined drug-disease association information. To
address this limitation, we propose a new model called Dual-Feature Drug
Repurposing Neural Network (DFDRNN). DFDRNN allows the mining of two features
(similarity and association) from the drug-disease biomedical networks to
encode drugs and diseases. A self-attention mechanism is utilized to extract
neighbor feature information. It incorporates two dual-feature extraction
modules: the single-domain dual-feature extraction (SDDFE) module for
extracting features within a single domain (drugs or diseases) and the
cross-domain dual-feature extraction (CDDFE) module for extracting features
across domains. By utilizing these modules, we ensure more appropriate encoding
of drugs and diseases. A cross-dual-domain decoder is also designed to predict
drug-disease associations in both domains. Our proposed DFDRNN model
outperforms six state-of-the-art methods on four benchmark datasets, achieving
an average AUROC of 0.946 and an average AUPR of 0.597. Case studies on two
diseases show that the proposed DFDRNN model can be applied in real-world
scenarios, demonstrating its significant potential in drug repositioning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bandit on the Hunt: Dynamic Crawling for Cyber Threat Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Kuehn, Dilara Nadermahmoodi, Markus Bayer, Christian Reuter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public information contains valuable Cyber Threat Intelligence (CTI) that is
used to prevent attacks in the future. Ideally, the learnings from previous
attacks help to mitigate all those that follow. While there are standards for
sharing this information, much of it is shared in non-standardized news
articles or blog posts. It is a time-consuming task to monitor online sources
for threats and even then, one can never be sure, to use the right sources.
Current research propose extractors of Indicators of Compromise from known
sources, while the identification of new sources is rarely considered. This
paper proposes a focused crawler focused on the CTI domain based on multi-armed
bandit ( MAB) and different crawling strategies. It uses SBERT to identify
relevant documents, while dynamically adapt its crawling path. We propose a
system called ThreatCrawl, which achieve a harvest rate of over 25% and is able
to expand its used seed by over 300%, while retaining focus on the topic at
hand. In addition, this crawler identified previously unknown but highly
relevant overview pages, datasets, and domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Redundant Is the <span class="highlight-title">Transformer</span> Stack in Speech Representation Models? <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teresa Dorszewski, Albert Kjøller Jacobsen, Lenka Tětková, Lars Kai Hansen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech representation models, particularly those leveraging
transformer architectures, have demonstrated remarkable performance across
various tasks such as speech recognition, speaker identification, and emotion
detection. Recent studies on transformer models revealed a high redundancy
between layers and the potential for significant pruning, which we will
investigate here for transformer-based speech representation models. We perform
a detailed analysis of layer similarity in speech representation models using
three similarity metrics: cosine similarity, centered kernel alignment, and
mutual nearest-neighbor alignment. Our findings reveal a block-like structure
of high similarity, suggesting two main processing steps and significant
redundancy of layers. We demonstrate the effectiveness of pruning
transformer-based speech representation models without the need for
post-training, achieving up to 40% reduction in transformer layers while
maintaining over 95% of the model's predictive capacity. Furthermore, we employ
a knowledge distillation method to substitute the entire transformer stack with
mimicking layers, reducing the network size 95-98% and the inference time by up
to 94%. This substantial decrease in computational load occurs without
considerable performance loss, suggesting that the transformer stack is almost
completely redundant for downstream applications of speech representation
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICASSP 2025 (excluding appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Ming Liu, Ming-Chih Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning and large language models (LLMs) have
facilitated the deployment of the mixture-of-experts (MoE) mechanism in the
stock investment domain. While these models have demonstrated promising trading
performance, they are often unimodal, neglecting the wealth of information
available in other modalities, such as textual data. Moreover, the traditional
neural network-based router selection mechanism fails to consider contextual
and real-world nuances, resulting in suboptimal expert selection. To address
these limitations, we propose LLMoE, a novel framework that employs LLMs as the
router within the MoE architecture. Specifically, we replace the conventional
neural network-based router with LLMs, leveraging their extensive world
knowledge and reasoning capabilities to select experts based on historical
price data and stock news. This approach provides a more effective and
interpretable selection mechanism. Our experiments on multimodal real-world
stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models
and other deep neural network approaches. Additionally, the flexible
architecture of LLMoE allows for easy adaptation to various downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025 Workshop on AI for Social Impact - Bridging
  Innovations in Finance, Social Media, and Crime Prevention</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-stage Deep Learning Artifact Reduction for Pallel-beam Computed
  Tomography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00494v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00494v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Shi, Daniel M. Pelt, K. Joost Batenburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computed Tomography (CT) using synchrotron radiation is a powerful technique
that, compared to lab-CT techniques, boosts high spatial and temporal
resolution while also providing access to a range of contrast-formation
mechanisms. The acquired projection data is typically processed by a
computational pipeline composed of multiple stages. Artifacts introduced during
data acquisition can propagate through the pipeline, and degrade image quality
in the reconstructed images. Recently, deep learning has shown significant
promise in enhancing image quality for images representing scientific data.
This success has driven increasing adoption of deep learning techniques in CT
imaging. Various approaches have been proposed to incorporate deep learning
into computational pipelines, but each has limitations in addressing artifacts
effectively and efficiently in synchrotron CT, either in properly addressing
the specific artifacts, or in computational efficiency.
  Recognizing these challenges, we introduce a novel method that incorporates
separate deep learning models at each stage of the tomography
pipeline-projection, sinogram, and reconstruction-to address specific artifacts
locally in a data-driven way. Our approach includes bypass connections that
feed both the outputs from previous stages and raw data to subsequent stages,
minimizing the risk of error propagation. Extensive evaluations on both
simulated and real-world datasets illustrate that our approach effectively
reduces artifacts and outperforms comparison methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annealed Multiple Choice Learning: Overcoming limitations of
  Winner-takes-all with annealing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15580v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15580v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Perera, Victor Letzelter, Théo Mariotte, Adrien Cortés, Mickael Chen, Slim Essid, Gaël Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Annealed Multiple Choice Learning (aMCL) which combines
simulated annealing with MCL. MCL is a learning framework handling ambiguous
tasks by predicting a small set of plausible hypotheses. These hypotheses are
trained using the Winner-takes-all (WTA) scheme, which promotes the diversity
of the predictions. However, this scheme may converge toward an arbitrarily
suboptimal local minimum, due to the greedy nature of WTA. We overcome this
limitation using annealing, which enhances the exploration of the hypothesis
space during training. We leverage insights from statistical physics and
information theory to provide a detailed description of the model training
trajectory. Additionally, we validate our algorithm by extensive experiments on
synthetic datasets, on the standard UCI benchmark, and on speech separation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM360 K2: Building a 65B 360-Open-Source <span class="highlight-title">Large Language Model</span> from
  Scratch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07124v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07124v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengzhong Liu, Bowen Tan, Hongyi Wang, Willie Neiswanger, Tianhua Tao, Haonan Li, Fajri Koto, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Liqun Ma, Liping Tang, Nikhil Ranjan, Yonghao Zhuang, Guowei He, Renxi Wang, Mingkai Deng, Robin Algayres, Yuanzhi Li, Zhiqiang Shen, Preslav Nakov, Eric Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We detail the training of the LLM360 K2-65B model, scaling up our 360-degree
OPEN SOURCE approach to the largest and most powerful models under project
LLM360. While open-source LLMs continue to advance, the answer to "How are the
largest LLMs trained?" remains unclear within the community. The implementation
details for such high-capacity models are often protected due to business
considerations associated with their high cost. This lack of transparency
prevents LLM researchers from leveraging valuable insights from prior
experience, e.g., "What are the best practices for addressing loss spikes?" The
LLM360 K2 project addresses this gap by providing full transparency and access
to resources accumulated during the training of LLMs at the largest scale. This
report highlights key elements of the K2 project, including our first model, K2
DIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals
LLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the
implementation steps and present a longitudinal analysis of K2 DIAMOND's
capabilities throughout its training process. We also outline ongoing projects
such as TXT360, setting the stage for future models in the series. By offering
previously unavailable resources, the K2 project also resonates with the
360-degree OPEN SOURCE principles of transparency, reproducibility, and
accessibility, which we believe are vital in the era of resource-intensive AI
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix
  Sharing and Throughput-oriented Token Batching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) increasingly play an important role in a wide
range of information processing and management tasks. Many of these tasks are
performed in large batches or even offline, and the performance indictor for
which is throughput. These tasks usually show the characteristic of prefix
sharing, where different prompt input can partially show the common prefix.
However, the existing LLM inference engines tend to optimize the streaming
requests and show limitations of supporting the large batched tasks with the
prefix sharing characteristic. The existing solutions use the LRU-based cache
to reuse the KV context of common prefix between requests. The KV context that
are about to be reused may prematurely evicted with the implicit cache
management. Besides, the streaming oriented systems do not leverage the
request-batch information and can not mix the decoding tokens with the prefill
chunks to the best for the batched scenarios, and thus fails to saturate the
GPU. We propose BatchLLM to address the above problems. BatchLLM explicitly
identifies the common prefixes globally. The requests sharing the same prefix
will be scheduled together to reuse the KV context the best. BatchLLM reorders
the requests and schedules the requests with larger ratio of decoding first to
better mix the decoding tokens with the latter prefill chunks, and applies
memory-centric token batching to enlarge the token-batch sizes, which helps to
increase the GPU utilization. Finally, BatchLLM optimizes the prefix-shared
Attention kernel with horizontal fusion to reduce tail effect and kernel launch
overhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang
by 1.3$\times$ to 10.8$\times$ on a set of microbenchmarks and a typical
industry workload under different hardware environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language
  Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibaut Thonet, Jos Rozen, Laurent Besacier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on Large Language Models (LLMs) has recently witnessed an increasing
interest in extending the models' context size to better capture dependencies
within long documents. While benchmarks have been proposed to assess long-range
abilities, existing efforts primarily considered generic tasks that are not
necessarily aligned with real-world applications. In contrast, we propose a new
benchmark for long-context LLMs focused on a practical meeting assistant
scenario in which the long contexts consist of transcripts obtained by
automatic speech recognition, presenting unique challenges for LLMs due to the
inherent noisiness and oral nature of such data. Our benchmark, ELITR-Bench,
augments the existing ELITR corpus by adding 271 manually crafted questions
with their ground-truth answers, as well as noisy versions of meeting
transcripts altered to target different Word Error Rate levels. Our experiments
with 12 long-context LLMs on ELITR-Bench confirm the progress made across
successive generations of both proprietary and open models, and point out their
discrepancies in terms of robustness to transcript noise. We also provide a
thorough analysis of our GPT-4-based evaluation, including insights from a
crowdsourcing study. Our findings indicate that while GPT-4's scores align with
human judges, its ability to distinguish beyond three score levels may be
limited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating analytical variability in fMRI results with style transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03703v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03703v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elodie Germani, Camille Maumet, Elisa Fromont
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to improve the reproducibility of neuroimaging
results by converting statistic maps across different functional MRI pipelines.
We make the assumption that pipelines used to compute fMRI statistic maps can
be considered as a style component and we propose to use different generative
models, among which, Generative Adversarial Networks (GAN) and Diffusion Models
(DM) to convert statistic maps across different pipelines. We explore the
performance of multiple GAN frameworks, and design a new DM framework for
unsupervised multi-domain styletransfer. We constrain the generation of 3D fMRI
statistic maps using the latent space of an auxiliary classifier that
distinguishes statistic maps from different pipelines and extend traditional
sampling techniques used in DM to improve the transition performance. Our
experiments demonstrate that our proposed methods aresuccessful: pipelines can
indeed be transferred as a style component, providing animportant source of
data augmentation for future medical studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating lensed quasars discovery and modeling with physics-informed
  variational autoen<span class="highlight-title">code</span>rs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Irham T. Andika, Stefan Schuldt, Sherry H. Suyu, Satadru Bag, Raoul Cañameras, Alejandra Melo, Claudio Grillo, James H. H. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Strongly lensed quasars provide valuable insights into the rate of cosmic
expansion, the distribution of dark matter in foreground deflectors, and the
characteristics of quasar hosts. However, detecting them in astronomical images
is difficult due to the prevalence of non-lensing objects. To address this
challenge, we developed a generative deep learning model called VariLens, built
upon a physics-informed variational autoencoder. This model seamlessly
integrates three essential modules: image reconstruction, object
classification, and lens modeling, offering a fast and comprehensive approach
to strong lens analysis. VariLens is capable of rapidly determining both (1)
the probability that an object is a lens system and (2) key parameters of a
singular isothermal ellipsoid (SIE) mass model -- including the Einstein radius
($\theta_\mathrm{E}$), lens center, and ellipticity -- in just milliseconds
using a single CPU. A direct comparison of VariLens estimates with traditional
lens modeling for 20 known lensed quasars within the Subaru Hyper Suprime-Cam
(HSC) footprint shows good agreement, with both results consistent within
$2\sigma$ for systems with $\theta_\mathrm{E}<3$ arcsecs. To identify new
lensed quasar candidates, we begin with an initial sample of approximately 80
million sources, combining HSC data with multiwavelength information from
various surveys. After applying a photometric preselection aimed at locating
$z>1.5$ sources, the number of candidates is reduced to 710,966. Subsequently,
VariLens highlights 13,831 sources, each showing a high likelihood of being a
lens. A visual assessment of these objects results in 42 promising candidates
that await spectroscopic confirmation. These results underscore the potential
of automated deep learning pipelines to efficiently detect and model strong
lenses in large datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the Astronomy & Astrophysics journal and updated to
  reflect the revised version. The paper consists of 17 main pages, 14 figures,
  and 5 tables. We welcome feedback and comments from readers!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VECT-GAN: A variationally en<span class="highlight-title">code</span>d generative model for overcoming data
  scarcity in pharmaceutical science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Abdalla, Marrisa Taub, Eleanor Hilton, Priya Akkaraju, Alexander Milanovic, Mine Orlu, Abdul W. Basit, Michael T Cook, Tapabrata Chakraborti, David Shorthouse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in pharmaceutical research has led to reliance on
labour-intensive trial-and-error approaches for development rather than
data-driven methods. While Machine Learning offers a solution, existing
datasets are often small and noisy, limiting their utility. To address this, we
developed a Variationally Encoded Conditional Tabular Generative Adversarial
Network (VECT-GAN), a novel generative model specifically designed for
augmenting small, noisy datasets. We introduce a pipeline where data is
augmented before regression model development and demonstrate that this
consistently and significantly improves performance over other state-of-the-art
tabular generative models. We apply this pipeline across six pharmaceutical
datasets, and highlight its real-world applicability by developing novel
polymers with medically desirable mucoadhesive properties, which we made and
experimentally characterised. Additionally, we pre-train the model on the
ChEMBL database of drug-like molecules, leveraging knowledge distillation to
enhance its generalisability, making it readily available for use on
pharmaceutical datasets containing small molecules, an extremely common
pharmaceutical task. We demonstrate the power of synthetic data for
regularising small tabular datasets, highlighting its potential to become
standard practice in pharmaceutical model development, and make our method,
including VECT-GAN pre-trained on ChEMBL available as a pip package.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 primary figures, 3 supplementary figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IterL2Norm: Fast Iterative L2-Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ChangMin Ye, Yonguk Sim, Youngchae Kim, SeongMin Jin, Doo Seok Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large language models are a memory-bound model whose
operation is based on a large amount of data that are marginally reused. Thus,
the data movement between a host and accelerator likely dictates the total
wall-clock time. Layer normalization is one of the key workloads in the
transformer model, following each of multi-head attention and feed-forward
network blocks. To reduce data movement, layer normalization needs to be
performed on the same chip as the matrix-matrix multiplication engine. To this
end, we introduce an iterative L2-normalization method for 1D input
(IterL2Norm), ensuring fast convergence to the steady-state solution within
five iteration steps and high precision, outperforming the fast inverse square
root algorithm in six out of nine cases for FP32 and five out of nine for
BFloat16 across the embedding lengths used in the OPT models. Implemented in
32/28nm CMOS, the IterL2Norm macro normalizes $d$-dimensional vectors, where
$64 \leq d \leq 1024$, with a latency of 116-227 cycles at 100MHz/1.05V.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Design, Automation & Test in Europe Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Graph Representations and Graph Neural Networks for
  Multivariate Time Series Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wennuo Yang, Shiling Wu, Yuzhi Zhou, Cheng Luo, Xilin He, Weicheng Xie, Linlin Shen, Siyang Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate Time Series Classification (MTSC) enables the analysis if
complex temporal data, and thus serves as a cornerstone in various real-world
applications, ranging from healthcare to finance. Since the relationship among
variables in MTS usually contain crucial cues, a large number of graph-based
MTSC approaches have been proposed, as the graph topology and edges can
explicitly represent relationships among variables (channels), where not only
various MTS graph representation learning strategies but also different Graph
Neural Networks (GNNs) have been explored. Despite such progresses, there is no
comprehensive study that fairly benchmarks and investigates the performances of
existing widely-used graph representation learning strategies/GNN classifiers
in the application of different MTSC tasks. In this paper, we present the first
benchmark which systematically investigates the effectiveness of the
widely-used three node feature definition strategies, four edge feature
learning strategies and five GNN architecture, resulting in 60 different
variants for graph-based MTSC. These variants are developed and evaluated with
a standardized data pipeline and training/validation/testing strategy on 26
widely-used suspensor MTSC datasets. Our experiments highlight that node
features significantly influence MTSC performance, while the visualization of
edge features illustrates why adaptive edge learning outperforms other edge
feature learning methods. The code of the proposed benchmark is publicly
available at
\url{https://github.com/CVI-yangwn/Benchmark-GNN-for-Multivariate-Time-Series-Classification}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Median (GM) Matching for Robust Data Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Acharya, Inderjit S Dhillon, Sujay Sanghavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale data collections in the wild, are invariably noisy. Thus
developing data pruning strategies that remain robust even in the presence of
corruption is critical in practice. In this work, we propose Geometric Median
($\gm$) Matching -- a herding style greedy algorithm that yields a $k$-subset
such that the mean of the subset approximates the geometric median of the
(potentially) noisy dataset. Theoretically, we show that $\gm$ Matching enjoys
an improved $\gO(1/k)$ scaling over $\gO(1/\sqrt{k})$ scaling of uniform
sampling; while achieving {\bf optimal breakdown point} of {\bf 1/2} even under
{\bf arbitrary} corruption. Extensive experiments across several popular deep
learning benchmarks indicate that $\gm$ Matching consistently improves over
prior state-of-the-art; the gains become more profound at high rates of
corruption and aggressive pruning rates; making $\gm$ Matching a strong
baseline for future research in robust data pruning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural networks for insurance pricing with frequency and severity data:
  a benchmark study from data preprocessing to technical tariff 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12671v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12671v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Freek Holvoet, Katrien Antonio, Roel Henckaerts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Insurers usually turn to generalized linear models for modeling claim
frequency and severity data. Due to their success in other fields, machine
learning techniques are gaining popularity within the actuarial toolbox. Our
paper contributes to the literature on frequency-severity insurance pricing
with machine learning via deep learning structures. We present a benchmark
study on four insurance data sets with frequency and severity targets in the
presence of multiple types of input features. We compare in detail the
performance of: a generalized linear model on binned input data, a
gradient-boosted tree model, a feed-forward neural network (FFNN), and the
combined actuarial neural network (CANN). The CANNs combine a baseline
prediction established with a GLM and GBM, respectively, with a neural network
correction. We explain the data preprocessing steps with specific focus on the
multiple types of input features typically present in tabular insurance data
sets, such as postal codes, numeric and categorical covariates. Autoencoders
are used to embed the categorical variables into the neural network, and we
explore their potential advantages in a frequency-severity setting. Model
performance is evaluated not only on out-of-sample deviance but also using
statistical and calibration performance criteria and managerial tools to get
more nuanced insights. Finally, we construct global surrogate models for the
neural nets' frequency and severity models. These surrogates enable the
translation of the essential insights captured by the FFNNs or CANNs to GLMs.
As such, a technical tariff table results that can easily be deployed in
practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Secure Multiplication: Hiding Information in the
  Rubble of Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16105v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16105v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viveck R. Cadambe, Ateet Devulapalli, Haewon Jeong, Flavio P. Calmon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of private distributed multi-party multiplication. It
is well-established that Shamir secret-sharing coding strategies can enable
perfect information-theoretic privacy in distributed computation via the
celebrated algorithm of Ben Or, Goldwasser and Wigderson (the "BGW algorithm").
However, perfect privacy and accuracy require an honest majority, that is, $N
\geq 2t+1$ compute nodes are required to ensure privacy against any $t$
colluding adversarial nodes. By allowing for some controlled amount of
information leakage and approximate multiplication instead of exact
multiplication, we study coding schemes for the setting where the number of
honest nodes can be a minority, that is $N< 2t+1.$ We develop a tight
characterization privacy-accuracy trade-off for cases where $N < 2t+1$ by
measuring information leakage using {differential} privacy instead of perfect
privacy, and using the mean squared error metric for accuracy. A novel
technical aspect is an intricately layered noise distribution that merges ideas
from differential privacy and Shamir secret-sharing at different layers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of papers presented in IEEE ISIT 2022, IEEE ISIT
  2023 and TPDP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RELIEF: Reinforcement Learning Empowered Graph Feature <span class="highlight-title">Prompt</span> Tuning <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03195v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03195v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiapeng Zhu, Zichen Ding, Jianxiang Yu, Jiaqi Tan, Xiang Li, Weining Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of the "pre-train, prompt" paradigm has recently extended its
generalization ability and data efficiency to graph representation learning,
following its achievements in Natural Language Processing (NLP). Initial graph
prompt tuning approaches tailored specialized prompting functions for Graph
Neural Network (GNN) models pre-trained with specific strategies, such as edge
prediction, thus limiting their applicability. In contrast, another pioneering
line of research has explored universal prompting via adding prompts to the
input graph's feature space, thereby removing the reliance on specific
pre-training strategies. However, the necessity to add feature prompts to all
nodes remains an open question. Motivated by findings from prompt tuning
research in the NLP domain, which suggest that highly capable pre-trained
models need less conditioning signal to achieve desired behaviors, we advocate
for strategically incorporating necessary and lightweight feature prompts to
certain graph nodes to enhance downstream task performance. This introduces a
combinatorial optimization problem, requiring a policy to decide 1) which nodes
to prompt and 2) what specific feature prompts to attach. We then address the
problem by framing the prompt incorporation process as a sequential
decision-making problem and propose our method, RELIEF, which employs
Reinforcement Learning (RL) to optimize it. At each step, the RL agent selects
a node (discrete action) and determines the prompt content (continuous action),
aiming to maximize cumulative performance gain. Extensive experiments on graph
and node-level tasks with various pre-training strategies in few-shot scenarios
demonstrate that our RELIEF outperforms fine-tuning and other prompt-based
approaches in classification performance and data efficiency. The code is
available at https://github.com/JasonZhujp/RELIEF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGKDD 2025 (camera-ready version). Due to the space
  limitation, please refer to the V2 version for more details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Elucidating the Design Space of Dataset Condensation <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13733v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13733v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shitong Shao, Zikai Zhou, Huanran Chen, Zhiqiang Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset condensation, a concept within data-centric learning, efficiently
transfers critical attributes from an original dataset to a synthetic version,
maintaining both diversity and realism. This approach significantly improves
model training efficiency and is adaptable across multiple application areas.
Previous methods in dataset condensation have faced challenges: some incur high
computational costs which limit scalability to larger datasets (e.g., MTT,
DREAM, and TESLA), while others are restricted to less optimal design spaces,
which could hinder potential improvements, especially in smaller datasets
(e.g., SRe2L, G-VBSM, and RDED). To address these limitations, we propose a
comprehensive design framework that includes specific, effective strategies
like implementing soft category-aware matching and adjusting the learning rate
schedule. These strategies are grounded in empirical evidence and theoretical
backing. Our resulting approach, Elucidate Dataset Condensation (EDC),
establishes a benchmark for both small and large-scale dataset condensation. In
our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on
ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a
compression ratio of 0.78%. This performance exceeds those of SRe2L, G-VBSM,
and RDED by margins of 27.3%, 17.2%, and 6.6%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce AceMath, a suite of frontier math models that
excel in solving complex math problems, along with highly effective reward
models capable of evaluating generated solutions and reliably identifying the
correct ones. To develop the instruction-tuned math models, we propose a
supervised fine-tuning (SFT) process that first achieves competitive
performance across general domains, followed by targeted fine-tuning for the
math domain using a carefully curated set of prompts and synthetically
generated responses. The resulting model, AceMath-72B-Instruct greatly
outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop
math-specialized reward model, we first construct AceMath-RewardBench, a
comprehensive and robust benchmark for evaluating math reward models across
diverse problems and difficulty levels. After that, we present a systematic
approach to build our math reward models. The resulting model, AceMath-72B-RM,
consistently outperforms state-of-the-art reward models. Furthermore, when
combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest
average rm@8 score across the math reasoning benchmarks. We release model
weights, training data, and evaluation benchmarks at:
https://research.nvidia.com/labs/adlr/acemath
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing small projectors and multiple views for efficient vision
  <span class="highlight-title">pretrain</span>ing <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Krishna Agrawal, Arna Ghosh, Shagun Sodhani, Adam Oberman, Blake Richards
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in self-supervised (SSL) visual representation learning has
led to the development of several different proposed frameworks that rely on
augmentations of images but use different loss functions. However, there are
few theoretically grounded principles to guide practice, so practical
implementation of each SSL framework requires several heuristics to achieve
competitive performance. In this work, we build on recent analytical results to
design practical recommendations for competitive and efficient SSL that are
grounded in theory. Specifically, recent theory tells us that existing SSL
frameworks are minimizing the same idealized loss, which is to learn features
that best match the data similarity kernel defined by the augmentations used.
We show how this idealized loss can be reformulated to a functionally
equivalent loss that is more efficient to compute. We study the implicit bias
of using gradient descent to minimize our reformulated loss function and find
that using a stronger orthogonalization constraint with a reduced projector
dimensionality should yield good representations. Furthermore, the theory tells
us that approximating the reformulated loss should be improved by increasing
the number of augmentations, and as such using multiple augmentations should
lead to improved convergence. We empirically verify our findings on CIFAR, STL
and Imagenet datasets, wherein we demonstrate an improved linear readout
performance when training a ResNet-backbone using our theoretically grounded
recommendations. Remarkably, we also demonstrate that by leveraging these
insights, we can reduce the pretraining dataset size by up to 2$\times$ while
maintaining downstream accuracy simply by using more data augmentations. Taken
together, our work provides theoretically grounded recommendations that can be
used to improve SSL convergence and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Plug-and-Play HIO Approach for Phase Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cagatay Isil, Figen S. Oktem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the phase retrieval problem, the aim is the recovery of an unknown image
from intensity-only measurements such as Fourier intensity. Although there are
several solution approaches, solving this problem is challenging due to its
nonlinear and ill-posed nature. Recently, learning-based approaches have
emerged as powerful alternatives to the analytical methods for several inverse
problems. In the context of phase retrieval, a novel plug-and-play approach
that exploits learning-based prior and efficient update steps has been
presented at the Computational Optical Sensing and Imaging topical meeting,
with demonstrated state-of-the-art performance. The key idea was to incorporate
learning-based prior to the Gerchberg-Saxton type algorithms through
plug-and-play regularization. In this paper, we present the mathematical
development of the method including the derivation of its analytical update
steps based on half-quadratic splitting and comparatively evaluate its
performance through extensive simulations on a large test dataset. The results
show the effectiveness of the method in terms of both image quality,
computational efficiency, and robustness to initialization and noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tree-structured Markov random fields with Poisson marginal distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Côté, Hélène Cossette, Etienne Marceau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new family of tree-structured Markov random fields for a vector of discrete
counting random variables is introduced. According to the characteristics of
the family, the marginal distributions of the Markov random fields are all
Poisson with the same mean, and are untied from the strength or structure of
their built-in dependence. This key feature is uncommon for Markov random
fields and most convenient for applications purposes. The specific properties
of this new family confer a straightforward sampling procedure and analytic
expressions for the joint probability mass function and the joint probability
generating function of the vector of counting random variables, thus granting
computational methods that scale well to vectors of high dimension. We study
the distribution of the sum of random variables constituting a Markov random
field from the proposed family, analyze a random variable's individual
contribution to that sum through expected allocations, and establish stochastic
orderings to assess a wide understanding of their behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruction-Guided Fusion of Multi-Layer Visual Features in Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08443v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08443v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Li, Yi Zheng, Haotian Chen, Xiaolei Chen, Yuxuan Liang, Chenghang Lai, Bin Li, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have achieved remarkable success in a
wide range of multimodal tasks by integrating pre-trained vision encoders and
large language models. However, current LVLMs primarily rely on visual features
extracted from the final layers of the vision encoder, overlooking the
complementary information available in shallower layers. While recent
approaches have explored the use of multilayer visual features in LVLMs, they
tend to be task-agnostic and fail to examine the dependencies of hierarchical
visual features on specific tasks. To address these gaps, we systematically
investigate the contributions of visual features from different encoder layers
using 18 benchmarks spanning 6 task categories. Our findings reveal that
multilayer features provide complementary strengths with varying task
dependencies, and uniform fusion leads to suboptimal performance. Building on
these insights, we propose the instruction-guided vision aggregator, a module
that dynamically integrates multi-layer visual features based on textual
instructions, without increasing the number of visual tokens. Extensive
evaluations demonstrate the superior performance of our method. Additionally,
an in-depth analysis of the aggregator's behavior highlights the dominance of
mid-to-high-level features in semantic-rich tasks and the critical role of
low-level features in fine-grained perception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TraceFL: Interpretability-Driven Debugging in Federated Learning via
  Neuron Provenance <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13632v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13632v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waris Gill, Ali Anwar, Muhammad Ali Gulzar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Federated Learning, clients train models on local data and send updates to
a central server, which aggregates them into a global model using a fusion
algorithm. This collaborative yet privacy-preserving training comes at a cost.
FL developers face significant challenges in attributing global model
predictions to specific clients. Localizing responsible clients is a crucial
step towards (a) excluding clients primarily responsible for incorrect
predictions and (b) encouraging clients who contributed high-quality models to
continue participating in the future. Existing ML debugging approaches are
inherently inapplicable as they are designed for single-model, centralized
training.
  We introduce TraceFL, a fine-grained neuron provenance capturing mechanism
that identifies clients responsible for a global model's prediction by tracking
the flow of information from individual clients to the global model. Since
inference on different inputs activates a different set of neurons of the
global model, TraceFL dynamically quantifies the significance of the global
model's neurons in a given prediction, identifying the most crucial neurons in
the global model. It then maps them to the corresponding neurons in every
participating client to determine each client's contribution, ultimately
localizing the responsible client. We evaluate TraceFL on six datasets,
including two real-world medical imaging datasets and four neural networks,
including advanced models such as GPT. TraceFL achieves 99% accuracy in
localizing the responsible client in FL tasks spanning both image and text
classification tasks. At a time when state-of-the-artML debugging approaches
are mostly domain-specific (e.g., image classification only), TraceFL is the
first technique to enable highly accurate automated reasoning across a wide
range of FL applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2025 IEEE/ACM 47th International Conference on Software
  Engineering (ICSE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Dynamical Systems by Leveraging Data from Similar Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Xin, Lintao Ye, George Chiu, Shreyas Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning the dynamics of a linear system when one
has access to data generated by an auxiliary system that shares similar (but
not identical) dynamics, in addition to data from the true system. We use a
weighted least squares approach, and provide finite sample error bounds of the
learned model as a function of the number of samples and various system
parameters from the two systems as well as the weight assigned to the auxiliary
data. We show that the auxiliary data can help to reduce the intrinsic system
identification error due to noise, at the price of adding a portion of error
that is due to the differences between the two system models. We further
provide a data-dependent bound that is computable when some prior knowledge
about the systems, such as upper bounds on noise levels and model difference,
is available. This bound can also be used to determine the weight that should
be assigned to the auxiliary data during the model training stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Spatial Complexity of Optical Computing and How to Reduce It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yandong Li, Francesco Monticone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Similar to algorithms, which consume time and memory to run, hardware
requires resources to function. For devices processing physical waves,
implementing operations needs sufficient "space," as dictated by wave physics.
How much space is needed to perform a certain function is a fundamental
question in optics, with recent research addressing it for given mathematical
operations, but not for more general computing tasks, e.g., classification.
Inspired by computational complexity theory, we study the "spatial complexity"
of optical computing systems in terms of scaling laws - specifically, how their
physical dimensions must scale as the dimension of the mathematical operation
increases - and propose a new paradigm for designing optical computing systems:
space-efficient neuromorphic optics, based on structural sparsity constraints
and neural pruning methods motivated by wave physics (notably, the concept of
"overlapping nonlocality"). On two mainstream platforms, free-space optics and
on-chip integrated photonics, our methods demonstrate substantial size
reductions (to 1%-10% the size of conventional designs) with minimal compromise
on performance. Our theoretical and computational results reveal a trend of
diminishing returns on accuracy as structure dimensions increase, providing a
new perspective for interpreting and approaching the ultimate limits of optical
computing - a balanced trade-off between device size and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistent estimation of generative model representations in the data
  kernel perspective space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aranyak Acharyya, Michael W. Trosset, Carey E. Priebe, Hayden S. Helm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models, such as large language models and text-to-image diffusion
models, produce relevant information when presented a query. Different models
may produce different information when presented the same query. As the
landscape of generative models evolves, it is important to develop techniques
to study and analyze differences in model behaviour. In this paper we present
novel theoretical results for embedding-based representations of generative
models in the context of a set of queries. In particular, we establish
sufficient conditions for the consistent estimation of the model embeddings in
situations where the query set and the number of models grow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can AI-Generated Text be Reliably Detected? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11156v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11156v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) perform impressively well in various
applications. However, the potential for misuse of these models in activities
such as plagiarism, generating fake news, and spamming has raised concern about
their responsible use. Consequently, the reliable detection of AI-generated
text has become a critical area of research. AI text detectors have shown to be
effective under their specific settings. In this paper, we stress-test the
robustness of these AI text detectors in the presence of an attacker. We
introduce recursive paraphrasing attack to stress test a wide range of
detection schemes, including the ones using the watermarking as well as neural
network-based detectors, zero shot classifiers, and retrieval-based detectors.
Our experiments conducted on passages, each approximately 300 tokens long,
reveal the varying sensitivities of these detectors to our attacks. Our
findings indicate that while our recursive paraphrasing method can
significantly reduce detection rates, it only slightly degrades text quality in
many cases, highlighting potential vulnerabilities in current detection systems
in the presence of an attacker. Additionally, we investigate the susceptibility
of watermarked LLMs to spoofing attacks aimed at misclassifying human-written
text as AI-generated. We demonstrate that an attacker can infer hidden AI text
signatures without white-box access to the detection method, potentially
leading to reputational risks for LLM developers. Finally, we provide a
theoretical framework connecting the AUROC of the best possible detector to the
Total Variation distance between human and AI text distributions. This analysis
offers insights into the fundamental challenges of reliable detection as
language models continue to advance. Our code is publicly available at
https://github.com/vinusankars/Reliability-of-AI-text-detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing User Interest based on Stream Clustering and Memory Networks
  in Large-Scale Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13238v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13238v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Liu, Nian Wang, Cong Xu, Ming Zhao, Bin Wang, Yi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RSs) provide personalized recommendation service based
on user interest, which are widely used in various platforms. However, there
are lots of users with sparse interest due to lacking consumption behaviors,
which leads to poor recommendation results for them. This problem is widespread
in large-scale RSs and is particularly difficult to address. To solve this
problem, we propose a novel solution named User Interest Enhancement (UIE)
which enhances user interest including user profile and user history behavior
sequences using the enhancement vectors and personalized enhancement vector
generated based on stream clustering and memory networks from different
perspectives. UIE not only remarkably improves model performance on the users
with sparse interest but also significantly enhance model performance on other
users. UIE is an end-to-end solution which is easy to be implemented based on
ranking model. Moreover, we expand our solution and apply similar methods to
long-tail items, which also achieves excellent improvement. Furthermore, we
conduct extensive offline and online experiments in a large-scale industrial
RS. The results demonstrate that our model outperforms other models remarkably,
especially for the users with sparse interest. Until now, UIE has been fully
deployed in multiple large-scale RSs and achieved remarkable improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning with Human Judgement: The Role of Pairwise Preference in Large
  Language Model Evaluators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16950v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16950v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated promising capabilities as
automatic evaluators in assessing the quality of generated natural language.
However, LLMs still exhibit biases in evaluation and often struggle to generate
coherent evaluations that align with human assessments. In this work, we first
conduct a systematic study of the misalignment between LLM evaluators and human
evaluation, revealing that existing calibration methods aimed at mitigating
biases of LLMs are insufficient for effectively aligning LLM evaluators.
Inspired by the use of preference data in RLHF, we formulate the evaluation as
a ranking problem and introduce Pairwise-preference Search (PAIRS), an
uncertainty-guided search-based rank aggregation method that employs LLMs to
conduct pairwise comparisons locally and efficiently ranks candidate texts
globally. PAIRS achieves state-of-the-art performance on representative
evaluation tasks in long-form generations and demonstrates significant
improvements over direct scoring. Furthermore, we provide insights into the
role of pairwise preference in quantifying the transitivity of LLMs and
demonstrate how PAIRS benefits from calibration using debiased pairwise
evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by COLM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Matrix Multiplications for Lookup Table-Quantized LLMs <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10960v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10960v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Guo, William Brandon, Radostin Cholakov, Jonathan Ragan-Kelley, Eric P. Xing, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of large language models (LLMs) is often constrained by memory
bandwidth, where the primary bottleneck is the cost of transferring model
parameters from the GPU's global memory to its registers. When coupled with
custom kernels that fuse the dequantization and matmul operations, weight-only
quantization can thus enable faster inference by reducing the amount of memory
movement. However, developing high-performance kernels for weight-quantized
LLMs presents substantial challenges, especially when the weights are
compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,
lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup
table engine for LUT-quantized LLMs, which uses offline restructuring of the
quantized weight matrix to minimize bit manipulations associated with
unpacking, and vectorization and duplication of the lookup table to mitigate
shared memory bandwidth constraints. At batch sizes < 32 and quantization group
size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster
than existing GEMM kernels. As an application of FLUTE, we explore a simple
extension to lookup table-based NormalFloat quantization and apply it to
quantize LLaMA3 to various configurations, obtaining competitive quantization
performance against strong baselines while obtaining an end-to-end throughput
increase of 1.5 to 2 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2024 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial Clustering of Citizen Science Data Improves Downstream Species
  Distribution Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nahian Ahmed, Mark Roth, Tyler A. Hallman, W. Douglas Robinson, Rebecca A. Hutchinson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citizen science biodiversity data present great opportunities for ecology and
conservation across vast spatial and temporal scales. However, the
opportunistic nature of these data lacks the sampling structure required by
modeling methodologies that address a pervasive challenge in ecological data
collection: imperfect detection, i.e., the likelihood of under-observing
species on field surveys. Occupancy modeling is an example of an approach that
accounts for imperfect detection by explicitly modeling the observation process
separately from the biological process of habitat selection. This produces
species distribution models that speak to the pattern of the species on a
landscape after accounting for imperfect detection in the data, rather than the
pattern of species observations corrupted by errors. To achieve this benefit,
occupancy models require multiple surveys of a site across which the site's
status (i.e., occupied or not) is assumed constant. Since citizen science data
are not collected under the required repeated-visit protocol, observations may
be grouped into sites post hoc. Existing approaches for constructing sites
discard some observations and/or consider only geographic distance and not
environmental similarity. In this study, we compare ten approaches for site
construction in terms of their impact on downstream species distribution models
for 31 bird species in Oregon, using observations recorded in the eBird
database. We find that occupancy models built on sites constructed by spatial
clustering algorithms perform better than existing alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Sequence and Structure Generation for Realistic Antibody
  Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05982v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05982v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nayoung Kim, Minsu Kim, Sungsoo Ahn, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep learning has made rapid progress in antibody design, which
plays a key role in the advancement of therapeutics. A dominant paradigm is to
train a model to jointly generate the antibody sequence and the structure as a
candidate. However, the joint generation requires the model to generate both
the discrete amino acid categories and the continuous 3D coordinates; this
limits the space of possible architectures and may lead to suboptimal
performance. In response, we propose an antibody sequence-structure decoupling
(ASSD) framework, which separates sequence generation and structure prediction.
Although our approach is simple, our idea allows the use of powerful neural
architectures and demonstrates notable performance improvements. We also find
that the widely used non-autoregressive generators promote sequences with
overly repeating tokens. Such sequences are both out-of-distribution and prone
to undesirable developability properties that can trigger harmful immune
responses in patients. To resolve this, we introduce a composition-based
objective that allows an efficient trade-off between high performance and low
token repetition. ASSD shows improved performance in various antibody design
experiments, while the composition-based objective successfully mitigates token
repetition of non-autoregressive models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Adaptive Calibration and Optimal Design <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14440v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14440v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Oliveira, Dino Sejdinovic, David Howard, Edwin V. Bonilla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The process of calibrating computer models of natural phenomena is essential
for applications in the physical sciences, where plenty of domain knowledge can
be embedded into simulations and then calibrated against real observations.
Current machine learning approaches, however, mostly rely on rerunning
simulations over a fixed set of designs available in the observed data,
potentially neglecting informative correlations across the design space and
requiring a large amount of simulations. Instead, we consider the calibration
process from the perspective of Bayesian adaptive experimental design and
propose a data-efficient algorithm to run maximally informative simulations
within a batch-sequential process. At each round, the algorithm jointly
estimates the parameters of the posterior distribution and optimal designs by
maximising a variational lower bound of the expected information gain. The
simulator is modelled as a sample from a Gaussian process, which allows us to
correlate simulations and observed data with the unknown calibration
parameters. We show the benefits of our method when compared to related
approaches across synthetic and real-data problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 final revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keeping LLMs Aligned After <span class="highlight-title">Fine-tuning</span>: The Crucial Role of <span class="highlight-title">Prompt</span>
  Templates <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18540v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18540v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public LLMs such as the Llama 2-Chat underwent alignment training and were
considered safe. Recently Qi et al. [2024] reported that even benign
fine-tuning on seemingly safe datasets can give rise to unsafe behaviors in the
models. The current paper is about methods and best practices to mitigate such
loss of alignment. We focus on the setting where a public model is fine-tuned
before serving users for specific usage, where the model should improve on the
downstream task while maintaining alignment. Through extensive experiments on
several chat models (Meta's Llama 2-Chat, Mistral AI's Mistral 7B Instruct
v0.2, and OpenAI's GPT-3.5 Turbo), this paper uncovers that the prompt
templates used during fine-tuning and inference play a crucial role in
preserving safety alignment, and proposes the ``Pure Tuning, Safe Testing''
(PTST) strategy -- fine-tune models without a safety prompt, but include it at
test time. This seemingly counterintuitive strategy incorporates an intended
distribution shift to encourage alignment preservation. Fine-tuning experiments
on GSM8K, ChatDoctor, and OpenOrca show that PTST significantly reduces the
rise of unsafe behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLPF: Reinforcement Learning from Prediction Feedback for User
  Summarization with LLMs <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxing Wu, Lin Ning, Luyang Liu, Harrison Lee, Neo Wu, Chao Wang, Sushant Prakash, Shawn O'Banion, Bradley Green, Jun Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM-powered personalization agent systems employ Large Language Models (LLMs)
to predict users' behavior from their past activities. However, their
effectiveness often hinges on the ability to effectively leverage extensive,
long user historical data due to its inherent noise and length of such data.
Existing pretrained LLMs may generate summaries that are concise but lack the
necessary context for downstream tasks, hindering their utility in
personalization systems. To address these challenges, we introduce
Reinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to
generate concise, human-readable user summaries that are optimized for
downstream task performance. By maximizing the usefulness of the generated
summaries, RLPF effectively distills extensive user history data while
preserving essential information for downstream tasks. Our empirical evaluation
demonstrates significant improvements in both extrinsic downstream task utility
and intrinsic summary quality, surpassing baseline methods by up to 22% on
downstream task performance and achieving an up to 84.59% win rate on
Factuality, Abstractiveness, and Readability. RLPF also achieves a remarkable
74% reduction in context length while improving performance on 16 out of 19
unseen tasks and/or datasets, showcasing its generalizability. This approach
offers a promising solution for enhancing LLM personalization by effectively
transforming long, noisy user histories into informative and human-readable
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Study of Multi-Agent Deep Reinforcement Learning for Safe
  and Robust Autonomous Highway Ramp Entry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14593v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14593v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larry Schester, Luis E. Ortiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicles today can drive themselves on highways and driverless robotaxis
operate in major cities, with more sophisticated levels of autonomous driving
expected to be available and become more common in the future. Yet, technically
speaking, so-called "Level 5" (L5) operation, corresponding to full autonomy,
has not been achieved. For that to happen, functions such as fully autonomous
highway ramp entry must be available, and provide provably safe, and reliably
robust behavior to enable full autonomy. We present a systematic study of a
highway ramp function that controls the vehicles forward-moving actions to
minimize collisions with the stream of highway traffic into which a merging
(ego) vehicle enters. We take a game-theoretic multi-agent (MA) approach to
this problem and study the use of controllers based on deep reinforcement
learning (DRL). The virtual environment of the MA DRL uses self-play with
simulated data where merging vehicles safely learn to control longitudinal
position during a taper-type merge. The work presented in this paper extends
existing work by studying the interaction of more than two vehicles (agents)
and does so by systematically expanding the road scene with additional traffic
and ego vehicles. While previous work on the two-vehicle setting established
that collision-free controllers are theoretically impossible in fully
decentralized, non-coordinated environments, we empirically show that
controllers learned using our approach are nearly ideal when measured against
idealized optimal controllers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures; added support ack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Complete Characterization of Learnability for Stochastic Noisy Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09597v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09597v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steve Hanneke, Kun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the stochastic noisy bandit problem with an unknown reward function
$f^*$ in a known function class $\mathcal{F}$. Formally, a model $M$ maps arms
$\pi$ to a probability distribution $M(\pi)$ of reward. A model class
$\mathcal{M}$ is a collection of models. For each model $M$, define its mean
reward function $f^M(\pi)=\mathbb{E}_{r \sim M(\pi)}[r]$. In the bandit
learning problem, we proceed in rounds, pulling one arm $\pi$ each round and
observing a reward sampled from $M(\pi)$. With knowledge of $\mathcal{M}$,
supposing that the true model $M\in \mathcal{M}$, the objective is to identify
an arm $\hat{\pi}$ of near-maximal mean reward $f^M(\hat{\pi})$ with high
probability in a bounded number of rounds. If this is possible, then the model
class is said to be learnable.
  Importantly, a result of \cite{hanneke2023bandit} shows there exist model
classes for which learnability is undecidable. However, the model class they
consider features deterministic rewards, and they raise the question of whether
learnability is decidable for classes containing sufficiently noisy models. For
the first time, we answer this question in the positive by giving a complete
characterization of learnability for model classes with arbitrary noise. In
addition to that, we also describe the full spectrum of possible optimal query
complexities. Further, we prove adaptivity is sometimes necessary to achieve
the optimal query complexity. Last, we revisit an important complexity measure
for interactive decision making, the Decision-Estimation-Coefficient
\citep{foster2021statistical,foster2023tight}, and propose a new variant of the
DEC which also characterizes learnability in this setting.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-16T00:00:00Z">2025-01-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">62</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Language Barriers in Healthcare: A Study on Arabic LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nada Saadi, Tathagata Raha, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, Praveen K Kanithi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the challenges of developing large language models
(LLMs) proficient in both multilingual understanding and medical knowledge. We
demonstrate that simply translating medical data does not guarantee strong
performance on clinical tasks in the target language. Our experiments reveal
that the optimal language mix in training data varies significantly across
different medical tasks. We find that larger models with carefully calibrated
language ratios achieve superior performance on native-language clinical tasks.
Furthermore, our results suggest that relying solely on fine-tuning may not be
the most effective approach for incorporating new language knowledge into LLMs.
Instead, data and computationally intensive pretraining methods may still be
necessary to achieve optimal performance in multilingual medical settings.
These findings provide valuable guidance for building effective and inclusive
medical AI systems for diverse linguistic communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qwen it detect machine-generated text? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teodor-George Marchitan, Claudiu Creanga, Liviu P. Dinu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the approach of the Unibuc - NLP team in tackling the
Coling 2025 GenAI Workshop, Task 1: Binary Multilingual Machine-Generated Text
Detection. We explored both masked language models and causal models. For
Subtask A, our best model achieved first-place out of 36 teams when looking at
F1 Micro (Auxiliary Score) of 0.8333, and second-place when looking at F1 Macro
(Main Score) of 0.8301
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Generalization in Chain of Thought Reasoning for Smaller
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxwell J. Yin, Dingyi Jiang, Yongbing Chen, Boyu Wang, Charles Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning in smaller language models is a challenging
natural language process problem yet highly desirable in many real-life
applications. Existing CoT knowledge distillation methods often suffer from
overly conservative memorization in smaller LLMs, leading to low generalization
confidence. As fully preserving the CoT ability of teacher model is impossible,
we hypothesize that adversarial CoT fine-tuning is crucial for developing
smaller LLM with robust CoT generalization. To this end, we propose
\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled
fine-tuning framework that integrates diverse CoT domains. Specifically, PRADA
pioneers two CoT improvements in smaller LLM: (1) Recovering the
domain-invariant feature insight which typically lost during distillation with
domain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT
prompt engineering by employing domain-adversarial approaches. We theoretically
demonstrate the effectiveness of our approach and empirically show that it
significantly outperforms the state of the arts in a wide range of tasks.
Moreover, our empirical findings reveal that the smaller LLM, when leveraging
PRADA, aligns closely with domain knowledge, thereby improving the
explainability of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conversational Text Extraction with <span class="highlight-title">Large Language Model</span>s Using
  Retrieval-Augmented Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soham Roy, Mitul Goswami, Nisharg Nargund, Suneeta Mohanty, Prasant Kumar Pattnaik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a system leveraging Large Language Models (LLMs) to
extract text and enhance user interaction with PDF documents via a
conversational interface. Utilizing Retrieval-Augmented Generation (RAG), the
system provides informative responses to user inquiries while highlighting
relevant passages within the PDF. Upon user upload, the system processes the
PDF, employing sentence embeddings to create a document-specific vector store.
This vector store enables efficient retrieval of pertinent sections in response
to user queries. The LLM then engages in a conversational exchange, using the
retrieved information to extract text and generate comprehensive, contextually
aware answers. While our approach demonstrates competitive ROUGE values
compared to existing state-of-the-art techniques for text extraction and
summarization, we acknowledge that further qualitative evaluation is necessary
to fully assess its effectiveness in real-world applications. The proposed
system gives competitive ROUGE values as compared to existing state-of-the-art
techniques for text extraction and summarization, thus offering a valuable tool
for researchers, students, and anyone seeking to efficiently extract knowledge
and gain insights from documents through an intuitive question-answering
interface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computing Optimization-Based <span class="highlight-title">Prompt</span> Injections Against Closed-Weights
  Models By Misusing a <span class="highlight-title">Fine-Tuning</span> API 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Labunets, Nishit V. Pandya, Ashish Hooda, Xiaohan Fu, Earlence Fernandes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We surface a new threat to closed-weight Large Language Models (LLMs) that
enables an attacker to compute optimization-based prompt injections.
Specifically, we characterize how an attacker can leverage the loss-like
information returned from the remote fine-tuning interface to guide the search
for adversarial prompts. The fine-tuning interface is hosted by an LLM vendor
and allows developers to fine-tune LLMs for their tasks, thus providing
utility, but also exposes enough information for an attacker to compute
adversarial prompts. Through an experimental analysis, we characterize the
loss-like values returned by the Gemini fine-tuning API and demonstrate that
they provide a useful signal for discrete optimization of adversarial prompts
using a greedy search algorithm. Using the PurpleLlama prompt injection
benchmark, we demonstrate attack success rates between 65% and 82% on Google's
Gemini family of LLMs. These attacks exploit the classic utility-security
tradeoff - the fine-tuning interface provides a useful feature for developers
but also exposes the LLMs to powerful attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniThink: Expanding Knowledge Boundaries in Machine Writing through
  Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine writing with large language models often relies on
retrieval-augmented generation. However, these approaches remain confined
within the boundaries of the model's predefined scope, limiting the generation
of content with rich information. Specifically, vanilla-retrieved information
tends to lack depth, utility, and suffers from redundancy, which negatively
impacts the quality of generated articles, leading to shallow, repetitive, and
unoriginal outputs. To address these issues, we propose OmniThink, a machine
writing framework that emulates the human-like process of iterative expansion
and reflection. The core idea behind OmniThink is to simulate the cognitive
behavior of learners as they progressively deepen their knowledge of the
topics. Experimental results demonstrate that OmniThink improves the knowledge
density of generated articles without compromising metrics such as coherence
and depth. Human evaluations and expert feedback further highlight the
potential of OmniThink to address real-world challenges in the generation of
long-form articles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Lexicon-Based Text Embeddings with <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Lei, Tao Shen, Yu Cao, Andrew Yates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large language models (LLMs) have demonstrated exceptional performance
on general-purpose text embedding tasks. While dense embeddings have dominated
related research, we introduce the first Lexicon-based EmbeddiNgS (LENS)
leveraging LLMs that achieve competitive performance on these tasks. Regarding
the inherent tokenization redundancy issue and unidirectional attention
limitations in traditional causal LLMs, LENS consolidates the vocabulary space
through token embedding clustering, and investigates bidirectional attention
and various pooling strategies. Specifically, LENS simplifies lexicon matching
by assigning each dimension to a specific token cluster, where semantically
similar tokens are grouped together, and unlocking the full potential of LLMs
through bidirectional attention. Extensive experiments demonstrate that LENS
outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),
delivering compact feature representations that match the sizes of dense
counterparts. Notably, combining LENSE with dense embeddings achieves
state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suggesting <span class="highlight-title">Code</span> Edits in Interactive Machine Learning Notebooks Using
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bihui Jin, Jiayue Wang, Pengyu Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning developers frequently use interactive computational
notebooks, such as Jupyter notebooks, to host code for data processing and
model training. Jupyter notebooks provide a convenient tool for writing machine
learning pipelines and interactively observing outputs, however, maintaining
Jupyter notebooks, e.g., to add new features or fix bugs, can be challenging
due to the length and complexity of the notebooks. Moreover, there is no
existing benchmark related to developer edits on Jupyter notebooks. To address
this, we present the first dataset of 48,398 Jupyter notebook edits derived
from 20,095 revisions of 792 machine learning repositories on GitHub, and
perform the first study of the using LLMs to predict code edits in Jupyter
notebooks. Our dataset captures granular details of cell-level and line-level
modifications, offering a foundation for understanding real-world maintenance
patterns in machine learning workflows. We observed that the edits on Jupyter
notebooks are highly localized, with changes averaging only 166 lines of code
in repositories. While larger models outperform smaller counterparts in code
editing, all models have low accuracy on our dataset even after finetuning,
demonstrating the complexity of real-world machine learning maintenance tasks.
Our findings emphasize the critical role of contextual information in improving
model performance and point toward promising avenues for advancing large
language models' capabilities in engineering machine learning code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention based Bidirectional GRU hybrid model for inappropriate content
  detection in Urdu language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezzah Shoukat, Rabia Irfan, Iqra Basharat, Muhammad Ali Tahir, Sameen Shaukat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increased use of the internet and social networks for online
discussions, the spread of toxic and inappropriate content on social networking
sites has also increased. Several studies have been conducted in different
languages. However, there is less work done for South Asian languages for
inappropriate content identification using deep learning techniques. In Urdu
language, the spellings are not unique, and people write different common
spellings for the same word, while mixing it other languages, like English in
the text makes it more challenging, and limited research work is available to
process such language with the finest algorithms. The use of attention layer
with a deep learning model can help handling the long-term dependencies and
increase its efficiency . To explore the effects of the attention layer, this
study proposes attention-based Bidirectional GRU hybrid model for identifying
inappropriate content in Urdu Unicode text language. Four different baseline
deep learning models; LSTM, Bi-LSTM, GRU, and TCN, are used to compare the
performance of the proposed model. The results of these models were compared
based on evaluation metrics, dataset size, and impact of the word embedding
layer. The pre-trained Urdu word2Vec embeddings were utilized for our case. Our
proposed model BiGRU-A outperformed all other baseline models by yielding 84\%
accuracy without using pre-trained word2Vec layer. From our experiments, we
have established that the attention layer improves the model's efficiency, and
pre-trained word2Vec embedding does not work well with an inappropriate content
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Insights from 12 Machine Learning Models in Extracting
  Economic Ideology from Political Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihed Ncib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study conducts a systematic assessment of the capabilities of 12 machine
learning models and model variations in detecting economic ideology. As an
evaluation benchmark, I use manifesto data spanning six elections in the United
Kingdom and pre-annotated by expert and crowd coders. The analysis assesses the
performance of several generative, fine-tuned, and zero-shot models at the
granular and aggregate levels. The results show that generative models such as
GPT-4o and Gemini 1.5 Flash consistently outperform other models against all
benchmarks. However, they pose issues of accessibility and resource
availability. Fine-tuning yielded competitive performance and offers a reliable
alternative through domain-specific optimization. But its dependency on
training data severely limits scalability. Zero-shot models consistently face
difficulties with identifying signals of economic ideology, often resulting in
negative associations with human coding. Using general knowledge for the
domain-specific task of ideology scaling proved to be unreliable. Other key
findings include considerable within-party variation, fine-tuning benefiting
from larger training data, and zero-shot's sensitivity to prompt content. The
assessments include the strengths and limitations of each model and derive
best-practices for automated analyses of political content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptation of Foundation LLMs for e-Commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Herold, Michael Kozielski, Tala Bazazo, Pavel Petrushkov, Hadi Hashemi, Patrycja Cieplicka, Dominika Basaj, Shahram Khadivi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the e-Llama models: 8 billion and 70 billion parameter large
language models that are adapted towards the e-commerce domain. These models
are meant as foundation models with deep knowledge about e-commerce, that form
a base for instruction- and fine-tuning. The e-Llama models are obtained by
continuously pretraining the Llama 3.1 base models on 1 trillion tokens of
domain-specific data.
  We discuss our approach and motivate our choice of hyperparameters with a
series of ablation studies. To quantify how well the models have been adapted
to the e-commerce domain, we define and implement a set of multilingual,
e-commerce specific evaluation tasks.
  We show that, when carefully choosing the training setup, the Llama 3.1
models can be adapted towards the new domain without sacrificing significant
performance on general domain tasks. We also explore the possibility of merging
the adapted model and the base model for a better control of the performance
trade-off between domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Heap: A Contamination-Free Multilingual <span class="highlight-title">Code</span> Dataset for Evaluating
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Katzy, Razvan Mihai Popescu, Arie van Deursen, Maliheh Izadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent rise in the popularity of large language models has spurred the
development of extensive code datasets needed to train them. This has left
limited code available for collection and use in the downstream investigation
of specific behaviors, or evaluation of large language models without suffering
from data contamination. To address this problem, we release The Heap, a large
multilingual dataset covering 57 programming languages that has been
deduplicated with respect to other open datasets of code, enabling researchers
to conduct fair evaluations of large language models without significant data
cleaning overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-Print. Accepted to FORGE 2025 Dataset Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through
  Category-Bounding <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Kirmayr, Lukas Stappen, Phillip Schneider, Florian Matthes, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's assistant landscape, personalisation enhances interactions,
fosters long-term relationships, and deepens engagement. However, many systems
struggle with retaining user preferences, leading to repetitive user requests
and disengagement. Furthermore, the unregulated and opaque extraction of user
preferences in industry applications raises significant concerns about privacy
and trust, especially in regions with stringent regulations like Europe. In
response to these challenges, we propose a long-term memory system for voice
assistants, structured around predefined categories. This approach leverages
Large Language Models to efficiently extract, store, and retrieve preferences
within these categories, ensuring both personalisation and transparency. We
also introduce a synthetic multi-turn, multi-session conversation dataset
(CarMem), grounded in real industry data, tailored to an in-car voice assistant
setting. Benchmarked on the dataset, our system achieves an F1-score of .78 to
.95 in preference extraction, depending on category granularity. Our
maintenance strategy reduces redundant preferences by 95% and contradictory
ones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,
the results demonstrate the system's suitability for industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the International Conference on
  Computational Linguistics (COLING 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sentiment Analysis in Twitter Social Network Centered on
  Cryptocurrencies Using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahid Amiri, Mahmood Ahmadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryptocurrency is a digital currency that uses blockchain technology with
secure encryption. Due to the decentralization of these currencies, traditional
monetary systems and the capital market of each they, can influence a society.
Therefore, due to the importance of the issue, the need to understand public
opinion and analyze people's opinions in this regard increases. To understand
the opinions and views of people about different topics, you can take help from
social networks because they are a rich source of opinions. The Twitter social
network is one of the main platforms where users discuss various topics,
therefore, in the shortest time and with the lowest cost, the opinion of the
community can be measured on this social network. Twitter Sentiment Analysis
(TSA) is a field that analyzes the sentiment expressed in tweets. Considering
that most of TSA's research efforts on cryptocurrencies are focused on English
language, the purpose of this paper is to investigate the opinions of Iranian
users on the Twitter social network about cryptocurrencies and provide the best
model for classifying tweets based on sentiment. In the case of automatic
analysis of tweets, managers and officials in the field of economy can gain
knowledge from the general public's point of view about this issue and use the
information obtained in order to properly manage this phenomenon. For this
purpose, in this paper, in order to build emotion classification models,
natural language processing techniques such as bag of words (BOW) and FastText
for text vectorization and classical machine learning algorithms including KNN,
SVM and Adaboost learning methods Deep including LSTM and BERT model were used
for classification, and finally BERT linguistic model had the best accuracy
with 83.50%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages and 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Scarcity to Capability: Empowering Fake News Detection in
  Low-Resource Languages with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrithik Majumdar Shibu, Shrestha Datta, Md. Sumon Miah, Nasrullah Sami, Mahruba Sharmin Chowdhury, Md. Saiful Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid spread of fake news presents a significant global challenge,
particularly in low-resource languages like Bangla, which lack adequate
datasets and detection tools. Although manual fact-checking is accurate, it is
expensive and slow to prevent the dissemination of fake news. Addressing this
gap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news
detection. This version includes 11,700 additional, meticulously curated fake
news articles validated from credible sources, creating a proportional dataset
of 47,000 authentic and 13,000 fake news items across 13 categories. In
addition, we created a manually curated independent test set of 460 fake and
540 authentic news items for rigorous evaluation. We invest efforts in
collecting fake news from credible sources and manually verified while
preserving the linguistic richness. We develop a benchmark system utilizing
transformer-based architectures, including fine-tuned Bidirectional Encoder
Representations from Transformers variants (F1-87\%) and Large Language Models
with Quantized Low-Rank Approximation (F1-89\%), that significantly outperforms
traditional methods. BanFakeNews-2.0 offers a valuable resource to advance
research and application in fake news detection for low-resourced languages. We
publicly release our dataset and model on Github to foster research in this
direction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stylomech: Unveiling Authorship via Computational Stylometry in English
  and Romanized Sinhala 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nabeelah Faumi, Adeepa Gunathilake, Benura Wickramanayake, Deelaka Dias, TGDK Sumanathilaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of Web 2.0, the development in social technology coupled with
global communication systematically brought positive and negative impacts to
society. Copyright claims and Author identification are deemed crucial as there
has been a considerable amount of increase in content violation owing to the
lack of proper ethics in society. The Author's attribution in both English and
Romanized Sinhala became a major requirement in the last few decades. As an
area largely unexplored, particularly within the context of Romanized Sinhala,
the research contributes significantly to the field of computational
linguistics. The proposed author attribution system offers a unique approach,
allowing for the comparison of only two sets of text: suspect author and
anonymous text, a departure from traditional methodologies which often rely on
larger corpora. This work focuses on using the numerical representation of
various pairs of the same and different authors allowing for, the model to
train on these representations as opposed to text, this allows for it to apply
to a multitude of authors and contexts, given that the suspected author text,
and the anonymous text are of reasonable quality. By expanding the scope of
authorship attribution to encompass diverse linguistic contexts, the work
contributes to fostering trust and accountability in digital communication,
especially in Sri Lanka. This research presents a pioneering approach to author
attribution in both English and Romanized Sinhala, addressing a critical need
for content verification and intellectual property rights enforcement in the
digital age.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 figure, 1 image</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence Estimation for Error Detection in Text-to-SQL Systems <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Somov, Elena Tutubalina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL enables users to interact with databases through natural
language, simplifying the retrieval and synthesis of information. Despite the
success of large language models (LLMs) in converting natural language
questions into SQL queries, their broader adoption is limited by two main
challenges: achieving robust generalization across diverse queries and ensuring
interpretative confidence in their predictions. To tackle these issues, our
research investigates the integration of selective classifiers into Text-to-SQL
systems. We analyse the trade-off between coverage and risk using entropy based
confidence estimation with selective classifiers and assess its impact on the
overall performance of Text-to-SQL models. Additionally, we explore the models'
initial calibration and improve it with calibration techniques for better model
alignment between confidence and accuracy. Our experimental results show that
encoder-decoder T5 is better calibrated than in-context-learning GPT 4 and
decoder-only Llama 3, thus the designated external entropy-based selective
classifier has better performance. The study also reveal that, in terms of
error detection, selective classifier with a higher probability detects errors
associated with irrelevant questions rather than incorrect query generations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures, to be published in AAAI 2025 Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting a <span class="highlight-title">Large Language Model</span> with a Combination of Text and Visual
  Data for Conversational Visualization of Global Geospatial Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Mena, Alexandre Kouyoumdjian, Lonni Besançon, Michael Gleicher, Ivan Viola, Anders Ynnerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for augmenting a Large Language Model (LLM) with a
combination of text and visual data to enable accurate question answering in
visualization of scientific data, making conversational visualization possible.
LLMs struggle with tasks like visual data interaction, as they lack contextual
visual information. We address this problem by merging a text description of a
visualization and dataset with snapshots of the visualization. We extract their
essential features into a structured text file, highly compact, yet descriptive
enough to appropriately augment the LLM with contextual information, without
any fine-tuning. This approach can be applied to any visualization that is
already finally rendered, as long as it is associated with some textual
description.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIER: A Novel Metric for Evaluating What Matters in <span class="highlight-title">Code</span>-Switching <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enes Yavuz Ugan, Ngoc-Quan Pham, Leonard Bärmann, Alex Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-switching, the alternation of languages within a single discourse,
presents a significant challenge for Automatic Speech Recognition. Despite the
unique nature of the task, performance is commonly measured with established
metrics such as Word-Error-Rate (WER). However, in this paper, we question
whether these general metrics accurately assess performance on code-switching.
Specifically, using both Connectionist-Temporal-Classification and
Encoder-Decoder models, we show fine-tuning on non-code-switched data from both
matrix and embedded language improves classical metrics on code-switching test
sets, although actual code-switched words worsen (as expected). Therefore, we
propose Point-of-Interest Error Rate (PIER), a variant of WER that focuses only
on specific words of interest. We instantiate PIER on code-switched utterances
and show that this more accurately describes the code-switching performance,
showing huge room for improvement in future work. This focused evaluation
allows for a more precise assessment of model performance, particularly in
challenging aspects such as inter-word and intra-word code-switching.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Inquiry-Diagnosis Relationship with Advanced Patient
  Simulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaocheng Liu, Quan Tu, Wen Ye, Yu Xiao, Zhishou Zhang, Hengfu Cui, Yalun Zhu, Qiang Ju, Shizheng Li, Jian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online medical consultation (OMC) restricts doctors to gathering patient
information solely through inquiries, making the already complex sequential
decision-making process of diagnosis even more challenging. Recently, the rapid
advancement of large language models has demonstrated a significant potential
to transform OMC. However, most studies have primarily focused on improving
diagnostic accuracy under conditions of relatively sufficient information,
while paying limited attention to the "inquiry" phase of the consultation
process. This lack of focus has left the relationship between "inquiry" and
"diagnosis" insufficiently explored. In this paper, we first extract real
patient interaction strategies from authentic doctor-patient conversations and
use these strategies to guide the training of a patient simulator that closely
mirrors real-world behavior. By inputting medical records into our patient
simulator to simulate patient responses, we conduct extensive experiments to
explore the relationship between "inquiry" and "diagnosis" in the consultation
process. Experimental results demonstrate that inquiry and diagnosis adhere to
the Liebig's law: poor inquiry quality limits the effectiveness of diagnosis,
regardless of diagnostic capability, and vice versa. Furthermore, the
experiments reveal significant differences in the inquiry performance of
various models. To investigate this phenomenon, we categorize the inquiry
process into four types: (1) chief complaint inquiry; (2) specification of
known symptoms; (3) inquiry about accompanying symptoms; and (4) gathering
family or medical history. We analyze the distribution of inquiries across the
four types for different models to explore the reasons behind their significant
performance differences. We plan to open-source the weights and related code of
our patient simulator at https://github.com/LIO-H-ZEN/PatientSimulator.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple Choice Questions: Reasoning Makes <span class="highlight-title">Large Language Model</span>s (LLMs)
  More Self-Confident Even When They Are Wrong 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tairan Fu, Javier Conde, Gonzalo Martínez, María Grandury, Pedro Reviriego
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most widely used methods to evaluate LLMs are Multiple Choice
Question (MCQ) tests. MCQ benchmarks enable the testing of LLM knowledge on
almost any topic at scale as the results can be processed automatically. To
help the LLM answer, a few examples called few shots can be included in the
prompt. Moreover, the LLM can be asked to answer the question directly with the
selected option or to first provide the reasoning and then the selected answer,
which is known as chain of thought. In addition to checking whether the
selected answer is correct, the evaluation can look at the LLM-estimated
probability of its response as an indication of the confidence of the LLM in
the response. In this paper, we study how the LLM confidence in its answer
depends on whether the model has been asked to answer directly or to provide
the reasoning before answering. The results of the evaluation of questions on a
wide range of topics in seven different models show that LLMs are more
confident in their answers when they provide reasoning before the answer. This
occurs regardless of whether the selected answer is correct. Our hypothesis is
that this behavior is due to the reasoning that modifies the probability of the
selected answer, as the LLM predicts the answer based on the input question and
the reasoning that supports the selection made. Therefore, LLM estimated
probabilities seem to have intrinsic limitations that should be understood in
order to use them in evaluation procedures. Interestingly, the same behavior
has been observed in humans, for whom explaining an answer increases confidence
in its correctness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Graph-Based Dependency Parsing with Arc Vectorization and
  Attention-Based Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Floquet, Joseph Le Roux, Nadi Tomeh, Thierry Charnois
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel architecture for graph-based dependency parsing that
explicitly constructs vectors, from which both arcs and labels are scored. Our
method addresses key limitations of the standard two-pipeline approach by
unifying arc scoring and labeling into a single network, reducing scalability
issues caused by the information bottleneck and lack of parameter sharing.
Additionally, our architecture overcomes limited arc interactions with
transformer layers to efficiently simulate higher-order dependencies.
Experiments on PTB and UD show that our model outperforms state-of-the-art
parsers in both accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving the unsolvable: Translating case law in Hong Kong 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        King-kui Sin, Xi Xuan, Chunyu Kit, Clara Ho-yan Chan, Honic Ho-kin Ip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenges translating case law under Hong Kong's
bilingual legal system. It highlights the initial success of translating all
written statutes into Chinese before the 1997 handover, a task mandated by the
Basic Law. The effort involved significant collaboration among legal,
linguistic, and translation experts, resulting in a comprehensive and
culturally appropriate bilingual legal system. However, translating case law
remains a significant challenge due to the sheer volume and continuous growth
of judicial decisions. The paper critiques the governments and judiciarys
sporadic and uncoordinated efforts to translate case law, contrasting it with
the thorough approach previously taken for statute translation. Although the
government acknowledges the importance of legal bilingualism, it lacks a
sustainable strategy for translating case law. The Judiciarys position that
translating all judgments is unnecessary, unrealistic, and not cost-effectiveis
analyzed and critiqued for its impact on legal transparency and public trust. A
proposed solution involves leveraging machine translation technology through a
human-machine interactive translation platform, which undergoes two major
transitions. Initially based on a neural model, the platform transitions to
using a large language model for improved translation accuracy. Furthermore, it
evolves from a single-agent system to a multi-agent system, incorporating
Translator, Annotator, and Proofreader agents. This multi-agent approach,
supported by a grant, aims to facilitate efficient, high-quality translation of
judicial judgments by integrating advanced artificial intelligence and
continuous feedback mechanisms, thus better meeting the needs of a bilingual
legal system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Responsible LLMs: Inherent Risk, Malicious Use, and
  Mitigation Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huandong Wang, Wenjie Fu, Yingzhou Tang, Zhilong Chen, Yuxi Huang, Jinghua Piao, Chen Gao, Fengli Xu, Tao Jiang, Yong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) present significant potential for
supporting numerous real-world applications and delivering positive social
impacts, they still face significant challenges in terms of the inherent risk
of privacy leakage, hallucinated outputs, and value misalignment, and can be
maliciously used for generating toxic content and unethical purposes after been
jailbroken. Therefore, in this survey, we present a comprehensive review of
recent advancements aimed at mitigating these issues, organized across the four
phases of LLM development and usage: data collecting and pre-training,
fine-tuning and alignment, prompting and reasoning, and post-processing and
auditing. We elaborate on the recent advances for enhancing the performance of
LLMs in terms of privacy protection, hallucination reduction, value alignment,
toxicity elimination, and jailbreak defenses. In contrast to previous surveys
that focus on a single dimension of responsible LLMs, this survey presents a
unified framework that encompasses these diverse dimensions, providing a
comprehensive view of enhancing LLMs to better serve real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoCBT: An Autonomous Multi-agent Framework for Cognitive Behavioral
  Therapy in Psychological Counseling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ancheng Xu, Di Yang, Renhao Li, Jingwei Zhu, Minghuan Tan, Min Yang, Wanxin Qiu, Mingchen Ma, Haihong Wu, Bingyu Li, Feng Sha, Chengming Li, Xiping Hu, Qiang Qu, Derek F. Wong, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional in-person psychological counseling remains primarily niche, often
chosen by individuals with psychological issues, while online automated
counseling offers a potential solution for those hesitant to seek help due to
feelings of shame. Cognitive Behavioral Therapy (CBT) is an essential and
widely used approach in psychological counseling. The advent of large language
models (LLMs) and agent technology enables automatic CBT diagnosis and
treatment. However, current LLM-based CBT systems use agents with a fixed
structure, limiting their self-optimization capabilities, or providing hollow,
unhelpful suggestions due to redundant response patterns. In this work, we
utilize Quora-like and YiXinLi single-round consultation models to build a
general agent framework that generates high-quality responses for single-turn
psychological consultation scenarios. We use a bilingual dataset to evaluate
the quality of single-response consultations generated by each framework. Then,
we incorporate dynamic routing and supervisory mechanisms inspired by real
psychological counseling to construct a CBT-oriented autonomous multi-agent
framework, demonstrating its general applicability. Experimental results
indicate that AutoCBT can provide higher-quality automated psychological
counseling services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Models Do Not Understand Negation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many practical vision-language applications require models that understand
negation, e.g., when using natural language to retrieve images which contain
certain objects but not others. Despite advancements in vision-language models
(VLMs) through large-scale training, their ability to comprehend negation
remains underexplored. This study addresses the question: how well do current
VLMs understand negation? We introduce NegBench, a new benchmark designed to
evaluate negation understanding across 18 task variations and 79k examples
spanning image, video, and medical datasets. The benchmark consists of two core
tasks designed to evaluate negation understanding in diverse multimodal
settings: Retrieval with Negation and Multiple Choice Questions with Negated
Captions. Our evaluation reveals that modern VLMs struggle significantly with
negation, often performing at chance level. To address these shortcomings, we
explore a data-centric approach wherein we finetune CLIP models on large-scale
synthetic datasets containing millions of negated captions. We show that this
approach can result in a 10% increase in recall on negated queries and a 40%
boost in accuracy on multiple-choice questions with negated captions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://negbench.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mGeNTE: A Multilingual Resource for Gender-Neutral Language and
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrice Savoldi, Eleonora Cupin, Manjinder Thind, Anne Lauscher, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gender-neutral language reflects societal and linguistic shifts towards
greater inclusivity by avoiding the implication that one gender is the norm
over others. This is particularly relevant for grammatical gender languages,
which heavily encode the gender of terms for human referents and over-relies on
masculine forms, even when gender is unspecified or irrelevant. Language
technologies are known to mirror these inequalities, being affected by a male
bias and perpetuating stereotypical associations when translating into
languages with extensive gendered morphology. In such cases, gender-neutral
language can help avoid undue binary assumptions. However, despite its
importance for creating fairer multi- and cross-lingual technologies, inclusive
language research remains scarce and insufficiently supported in current
resources. To address this gap, we present the multilingual mGeNTe dataset.
Derived from the bilingual GeNTE (Piergentili et al., 2023), mGeNTE extends the
original corpus to include the English-Italian/German/Spanish language pairs.
Since each language pair is English-aligned with gendered and neutral sentences
in the target languages, mGeNTE enables research in both automatic
Gender-Neutral Translation (GNT) and language modelling for three grammatical
gender languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating LLM Abilities to Understand Tabular Electronic Health
  Records: A Comprehensive Study of Patient Data Extraction and Retrieval <span class="chip">ECIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesus Lovon, Martin Mouysset, Jo Oleiwan, Jose G. Moreno, Christine Damase-Michel, Lynda Tamine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Record (EHR) tables pose unique challenges among which is
the presence of hidden contextual dependencies between medical features with a
high level of data dimensionality and sparsity. This study presents the first
investigation into the abilities of LLMs to comprehend EHRs for patient data
extraction and retrieval. We conduct extensive experiments using the MIMICSQL
dataset to explore the impact of the prompt structure, instruction, context,
and demonstration, of two backbone LLMs, Llama2 and Meditron, based on task
performance. Through quantitative and qualitative analyses, our findings show
that optimal feature selection and serialization methods can enhance task
performance by up to 26.79% compared to naive approaches. Similarly, in-context
learning setups with relevant example selection improve data extraction
performance by 5.95%. Based on our study findings, we propose guidelines that
we believe would help the design of LLM-based models to support health search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published as full paper in the Proceedings of the European
  Conference on Information Retrieval (ECIR) 2025. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChartInsighter: An Approach for Mitigating Hallucination in Time-series
  Chart Summary Generation with A Benchmark Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fen Wang, Bomiao Wang, Xueli Shu, Zhen Liu, Zekai Shao, Chao Liu, Siming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective chart summary can significantly reduce the time and effort decision
makers spend interpreting charts, enabling precise and efficient communication
of data insights. Previous studies have faced challenges in generating accurate
and semantically rich summaries of time-series data charts. In this paper, we
identify summary elements and common hallucination types in the generation of
time-series chart summaries, which serve as our guidelines for automatic
generation. We introduce ChartInsighter, which automatically generates chart
summaries of time-series data, effectively reducing hallucinations in chart
summary generation. Specifically, we assign multiple agents to generate the
initial chart summary and collaborate iteratively, during which they invoke
external data analysis modules to extract insights and compile them into a
coherent summary. Additionally, we implement a self-consistency test method to
validate and correct our summary. We create a high-quality benchmark of charts
and summaries, with hallucination types annotated on a sentence-by-sentence
basis, facilitating the evaluation of the effectiveness of reducing
hallucinations. Our evaluations using our benchmark show that our method
surpasses state-of-the-art models, and that our summary hallucination rate is
the lowest, which effectively reduces various hallucinations and improves
summary quality. The benchmark is available at
https://github.com/wangfen01/ChartInsighter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Algorithm for Semantic Network Generation from Texts of Low Resource
  Languages Such as Kiswahili 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barack Wamkaya Wanjawa, Lawrence Muchemi, Evans Miriti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processing low-resource languages, such as Kiswahili, using machine learning
is difficult due to lack of adequate training data. However, such low-resource
languages are still important for human communication and are already in daily
use and users need practical machine processing tasks such as summarization,
disambiguation and even question answering (QA). One method of processing such
languages, while bypassing the need for training data, is the use semantic
networks. Some low resource languages, such as Kiswahili, are of the
subject-verb-object (SVO) structure, and similarly semantic networks are a
triple of subject-predicate-object, hence SVO parts of speech tags can map into
a semantic network triple. An algorithm to process raw natural language text
and map it into a semantic network is therefore necessary and desirable in
structuring low resource languages texts. This algorithm tested on the
Kiswahili QA task with upto 78.6% exact match.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures, published in Open Journal for Information
  Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shape-Based Single Object Classification Using Ensemble Method
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nur Shazwani Kamarudin, Mokhairi Makhtar, Syadiah Nor Wan Shamsuddin, Syed Abdullah Fadzli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, more and more images are available. Annotation and retrieval of the
images pose classification problems, where each class is defined as the group
of database images labelled with a common semantic label. Various systems have
been proposed for content-based retrieval, as well as for image classification
and indexing. In this paper, a hierarchical classification framework has been
proposed for bridging the semantic gap effectively and achieving multi-category
image classification. A well known pre-processing and post-processing method
was used and applied to three problems; image segmentation, object
identification and image classification. The method was applied to classify
single object images from Amazon and Google datasets. The classification was
tested for four different classifiers; BayesNetwork (BN), Random Forest (RF),
Bagging and Vote. The estimated classification accuracies ranged from 20% to
99% (using 10-fold cross validation). The Bagging classifier presents the best
performance, followed by the Random Forest classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study of In-Context-Learning-Based Text-to-SQL Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Shen, Chengcheng Wan, Ruoyi Qiao, Jiazhen Zou, Hang Xu, Yuchen Shao, Yueling Zhang, Weikai Miao, Geguang Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been adopted to perform text-to-SQL tasks,
utilizing their in-context learning (ICL) capability to translate natural
language questions into structured query language (SQL). However, such a
technique faces correctness problems and requires efficient repairing
solutions. In this paper, we conduct the first comprehensive study of
text-to-SQL errors. Our study covers four representative ICL-based techniques,
five basic repairing methods, two benchmarks, and two LLM settings. We find
that text-to-SQL errors are widespread and summarize 29 error types of 7
categories. We also find that existing repairing attempts have limited
correctness improvement at the cost of high computational overhead with many
mis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL
error detection and repairing framework. The evaluation demonstrates that
MapleRepair outperforms existing solutions by repairing 13.8% more queries with
neglectable mis-repairs and 67.4% less overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Mental Health Content on Social Media and Its Effect
  Towards Suicidal Ideation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohaiminul Islam Bhuiyan, Nur Shazwani Kamarudin, Nur Hafieza Ismail
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This review underscores the critical need for effective strategies to
identify and support individuals with suicidal ideation, exploiting
technological innovations in ML and DL to further suicide prevention efforts.
The study details the application of these technologies in analyzing vast
amounts of unstructured social media data to detect linguistic patterns,
keywords, phrases, tones, and contextual cues associated with suicidal
thoughts. It explores various ML and DL models like SVMs, CNNs, LSTM, neural
networks, and their effectiveness in interpreting complex data patterns and
emotional nuances within text data. The review discusses the potential of these
technologies to serve as a life-saving tool by identifying at-risk individuals
through their digital traces. Furthermore, it evaluates the real-world
effectiveness, limitations, and ethical considerations of employing these
technologies for suicide prevention, stressing the importance of responsible
development and usage. The study aims to fill critical knowledge gaps by
analyzing recent studies, methodologies, tools, and techniques in this field.
It highlights the importance of synthesizing current literature to inform
practical tools and suicide prevention efforts, guiding innovation in reliable,
ethical systems for early intervention. This research synthesis evaluates the
intersection of technology and mental health, advocating for the ethical and
responsible application of ML, DL, and NLP to offer life-saving potential
worldwide while addressing challenges like generalizability, biases, privacy,
and the need for further research to ensure these technologies do not
exacerbate existing inequities and harms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive
  Vision-Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harrison Fuller, Fernando Gabriela Garcia, Victor Flores
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot learning in medical image classification presents a significant
challenge due to the limited availability of annotated data and the complex
nature of medical imagery. In this work, we propose Adaptive Vision-Language
Fine-tuning with Hierarchical Contrastive Alignment (HiCA), a novel framework
that leverages the capabilities of Large Vision-Language Models (LVLMs) for
medical image analysis. HiCA introduces a two-stage fine-tuning strategy,
combining domain-specific pretraining and hierarchical contrastive learning to
align visual and textual representations at multiple levels. We evaluate our
approach on two benchmark datasets, Chest X-ray and Breast Ultrasound,
achieving state-of-the-art performance in both few-shot and zero-shot settings.
Further analyses demonstrate the robustness, generalizability, and
interpretability of our method, with substantial improvements in performance
compared to existing baselines. Our work highlights the potential of
hierarchical contrastive strategies in adapting LVLMs to the unique challenges
of medical imaging tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Retrieve or Not to Retrieve? Uncertainty Detection for Dynamic
  Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaustubh D. Dhole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation equips large language models with the
capability to retrieve external knowledge, thereby mitigating hallucinations by
incorporating information beyond the model's intrinsic abilities. However, most
prior works have focused on invoking retrieval deterministically, which makes
it unsuitable for tasks such as long-form question answering. Instead,
dynamically performing retrieval by invoking it only when the underlying LLM
lacks the required knowledge can be more efficient. In this context, we delve
deeper into the question, "To Retrieve or Not to Retrieve?" by exploring
multiple uncertainty detection methods. We evaluate these methods for the task
of long-form question answering, employing dynamic retrieval, and present our
comparisons. Our findings suggest that uncertainty detection metrics, such as
Degree Matrix Jaccard and Eccentricity, can reduce the number of retrieval
calls by almost half, with only a slight reduction in question-answering
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perspective Transition of <span class="highlight-title">Large Language Model</span>s for Solving Subjective
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Wang, Yuanchi Zhang, Ziyue Wang, Yuzhuang Xu, Fuwen Luo, Yile Wang, Peng Li, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have revolutionized the field of natural
language processing, enabling remarkable progress in various tasks. Different
from objective tasks such as commonsense reasoning and arithmetic
question-answering, the performance of LLMs on subjective tasks is still
limited, where the perspective on the specific problem plays crucial roles for
better interpreting the context and giving proper response. For example, in
certain scenarios, LLMs may perform better when answering from an expert role
perspective, potentially eliciting their relevant domain knowledge. In
contrast, in some scenarios, LLMs may provide more accurate responses when
answering from a third-person standpoint, enabling a more comprehensive
understanding of the problem and potentially mitigating inherent biases. In
this paper, we propose Reasoning through Perspective Transition (RPT), a method
based on in-context learning that enables LLMs to dynamically select among
direct, role, and third-person perspectives for the best way to solve
corresponding subjective problem. Through extensive experiments on totally 12
subjective tasks by using both closed-source and open-source LLMs including
GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single
fixed perspective based methods such as chain-of-thought prompting and expert
prompting, highlights the intricate ways that LLMs can adapt their perspectives
to provide nuanced and contextually appropriate responses for different
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delayed Fusion: Integrating <span class="highlight-title">Large Language Model</span>s into First-Pass
  Decoding in End-to-end Speech Recognition <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takaaki Hori, Martin Kocour, Adnan Haider, Erik McDermott, Xiaodan Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an efficient decoding approach for end-to-end automatic
speech recognition (E2E-ASR) with large language models (LLMs). Although
shallow fusion is the most common approach to incorporate language models into
E2E-ASR decoding, we face two practical problems with LLMs. (1) LLM inference
is computationally costly. (2) There may be a vocabulary mismatch between the
ASR model and the LLM. To resolve this mismatch, we need to retrain the ASR
model and/or the LLM, which is at best time-consuming and in many cases not
feasible. We propose "delayed fusion," which applies LLM scores to ASR
hypotheses with a delay during decoding and enables easier use of pre-trained
LLMs in ASR tasks. This method can reduce not only the number of hypotheses
scored by the LLM but also the number of LLM inference calls. It also allows
re-tokenizion of ASR hypotheses during decoding if ASR and LLM employ different
tokenizations. We demonstrate that delayed fusion provides improved decoding
speed and accuracy compared to shallow fusion and N-best rescoring using the
LibriHeavy ASR corpus and three public LLMs, OpenLLaMA 3B & 7B and Mistral 7B.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Foundations of <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Xiao, Jingbo Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is a book about large language models. As indicated by the title, it
primarily focuses on foundational concepts rather than comprehensive coverage
of all cutting-edge technologies. The book is structured into four main
chapters, each exploring a key area: pre-training, generative models, prompting
techniques, and alignment methods. It is intended for college students,
professionals, and practitioners in natural language processing and related
fields, and can serve as a reference for anyone interested in large language
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Graph Contrastive Learning Framework for Short Text
  Classification <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao Liu, Fausto Giunchiglia, Lan Huang, Ximing Li, Xiaoyue Feng, Renchu Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short text classification has gained significant attention in the information
age due to its prevalence and real-world applications. Recent advancements in
graph learning combined with contrastive learning have shown promising results
in addressing the challenges of semantic sparsity and limited labeled data in
short text classification. However, existing models have certain limitations.
They rely on explicit data augmentation techniques to generate contrastive
views, resulting in semantic corruption and noise. Additionally, these models
only focus on learning the intrinsic consistency between the generated views,
neglecting valuable discriminative information from other potential views. To
address these issues, we propose a Simple graph contrastive learning framework
for Short Text Classification (SimSTC). Our approach involves performing graph
learning on multiple text-related component graphs to obtain multi-view text
embeddings. Subsequently, we directly apply contrastive learning on these
embeddings. Notably, our method eliminates the need for data augmentation
operations to generate contrastive views while still leveraging the benefits of
multi-view contrastive learning. Despite its simplicity, our model achieves
outstanding performance, surpassing large language models on various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Short Text Classification with Multi-Source Information
  Exploration and Dual-Level Contrastive Learning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao Liu, Mengyu Li, Wei Pang, Fausto Giunchiglia, Lan Huang, Xiaoyue Feng, Renchu Guan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short text classification, as a research subtopic in natural language
processing, is more challenging due to its semantic sparsity and insufficient
labeled samples in practical scenarios. We propose a novel model named
MI-DELIGHT for short text classification in this work. Specifically, it first
performs multi-source information (i.e., statistical information, linguistic
information, and factual information) exploration to alleviate the sparsity
issues. Then, the graph learning approach is adopted to learn the
representation of short texts, which are presented in graph forms. Moreover, we
introduce a dual-level (i.e., instance-level and cluster-level) contrastive
learning auxiliary task to effectively capture different-grained contrastive
information within massive unlabeled data. Meanwhile, previous models merely
perform the main task and auxiliary tasks in parallel, without considering the
relationship among tasks. Therefore, we introduce a hierarchical architecture
to explicitly model the correlations between tasks. We conduct extensive
experiments across various benchmark datasets, demonstrating that MI-DELIGHT
significantly surpasses previous competitive models. It even outperforms
popular large language models on several datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FineMedLM-o1: Enhancing the Medical Reasoning Ability of LLM from
  Supervised <span class="highlight-title">Fine-Tuning</span> to Test-Time Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhou Yu, Tianhao Cheng, Ying Cheng, Rui Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have shown promise in
medical applications such as disease diagnosis and treatment planning. However,
most existing medical LLMs struggle with the advanced reasoning required for
complex clinical scenarios, such as differential diagnosis or personalized
treatment suggestions. We proposed FineMedLM-o1, which leverages high-quality
synthetic medical data and long-form reasoning data for Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and
deep reasoning capabilities. Additionally, we introduced Test-Time Training
(TTT) in the medical domain for the first time, facilitating domain adaptation
and ensuring reliable, accurate reasoning. Experimental results demonstrate
that FineMedLM-o1 achieves a 23% average performance improvement over prior
models on key medical benchmarks. Furthermore, the introduction of TTT provides
an additional 14% performance boost, highlighting its effectiveness in
enhancing medical reasoning capabilities. To support this process, we also
proposed a novel method for synthesizing medical dialogue. Compared to other
open-source datasets, our dataset stands out as superior in both quality and
complexity. The project and data will be released on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entailed Between the Lines: Incorporating Implication into NLI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Havaldar, Hamidreza Alvari, John Palowitch, Mohammad Javad Hosseini, Senaka Buthpitiya, Alex Fabrikant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much of human communication depends on implication, conveying meaning beyond
literal words to express a wider range of thoughts, intentions, and feelings.
For models to better understand and facilitate human communication, they must
be responsive to the text's implicit meaning. We focus on Natural Language
Inference (NLI), a core tool for many language tasks, and find that
state-of-the-art NLI models and datasets struggle to recognize a range of cases
where entailment is implied, rather than explicit from the text. We formalize
implied entailment as an extension of the NLI task and introduce the Implied
NLI dataset (INLI) to help today's LLMs both recognize a broader variety of
implied entailments and to distinguish between implicit and explicit
entailment. We show how LLMs fine-tuned on INLI understand implied entailment
and can generalize this understanding across datasets and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Brain Activity with Advanced <span class="highlight-title">Transformer</span> Models: Exploring the
  Role of Punctuation in Semantic Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zenon Lamprou, Frank Polick, Yashar Moshfeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research examines the congruence between neural activity and advanced
transformer models, emphasizing the semantic significance of punctuation in
text understanding. Utilizing an innovative approach originally proposed by
Toneva and Wehbe, we evaluate four advanced transformer models RoBERTa,
DistiliBERT, ALBERT, and ELECTRA against neural activity data. Our findings
indicate that RoBERTa exhibits the closest alignment with neural activity,
surpassing BERT in accuracy. Furthermore, we investigate the impact of
punctuation removal on model performance and neural alignment, revealing that
BERT's accuracy enhances in the absence of punctuation. This study contributes
to the comprehension of how neural networks represent language and the
influence of punctuation on semantic processing within the human brain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFactor GNNs: Revisiting Factorisation-based Models from a
  Message-Passing Perspective <span class="chip">NeurIPS
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09980v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09980v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Chen, Pushkar Mishra, Luca Franceschi, Pasquale Minervini, Pontus Stenetorp, Sebastian Riedel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factorisation-based Models (FMs), such as DistMult, have enjoyed enduring
success for Knowledge Graph Completion (KGC) tasks, often outperforming Graph
Neural Networks (GNNs). However, unlike GNNs, FMs struggle to incorporate node
features and generalise to unseen nodes in inductive settings. Our work bridges
the gap between FMs and GNNs by proposing ReFactor GNNs. This new architecture
draws upon both modelling paradigms, which previously were largely thought of
as disjoint. Concretely, using a message-passing formalism, we show how FMs can
be cast as GNNs by reformulating the gradient descent procedure as
message-passing operations, which forms the basis of our ReFactor GNNs. Across
a multitude of well-established KGC benchmarks, our ReFactor GNNs achieve
comparable transductive performance to FMs, and state-of-the-art inductive
performance while using an order of magnitude fewer parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36th Conference on Neural Information Processing Systems (NeurIPS
  2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PolInterviews -- A Dataset of German Politician Public Broadcast
  Interviews 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04484v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04484v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Birkenmaier, Laureen Sieber, Felix Bergstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel dataset of public broadcast interviews featuring
high-ranking German politicians. The interviews were sourced from YouTube,
transcribed, processed for speaker identification, and stored in a tidy and
open format. The dataset comprises 99 interviews with 33 different German
politicians across five major interview formats, containing a total of 28,146
sentences. As the first of its kind, this dataset offers valuable opportunities
for research on various aspects of political communication in the (German)
political contexts, such as agenda-setting, interviewer dynamics, or
politicians' self-presentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Crafting Customisable Characters with LLMs: Introducing SimsChat, a
  Persona-Driven Role-Playing Agent Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17962v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17962v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Yang, Dong Liu, Chenghao Xiao, Kun Zhao, Chen Tang, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate remarkable ability to comprehend
instructions and generate human-like text, enabling sophisticated agent
simulation beyond basic behavior replication. However, the potential for
creating freely customisable characters remains underexplored. We introduce the
Customisable Conversation Agent Framework, which employs LLMs to simulate
real-world characters through personalised characteristic feature injection,
enabling diverse character creation according to user preferences. We propose
the SimsConv dataset, comprising 68 customised characters and 13,971 multi-turn
role-playing dialogues across 1,360 real-world scenes. Characters are initially
customised using pre-defined elements (career, aspiration, traits, skills),
then expanded through personal and social profiles. Building on this, we
present SimsChat, a freely customisable role-playing agent incorporating
various realistic settings and topic-specified character interactions.
Experimental results on both SimsConv and WikiRoleEval datasets demonstrate
SimsChat's superior performance in maintaining character consistency, knowledge
accuracy, and appropriate question rejection compared to existing models. Our
framework provides valuable insights for developing more accurate and
customisable human simulacra. Our data and code are publicly available at
https://github.com/Bernard-Yang/SimsChat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> aiX<span class="highlight-title">code</span>r-7B: A Lightweight and Effective <span class="highlight-title">Large Language Model</span> for <span class="highlight-title">Code</span>
  Processing <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13187v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13187v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Jiang, Jia Li, He Zong, Huanyu Liu, Hao Zhu, Shukai Hu, Erlu Li, Jiazheng Ding, Yu Han, Wei Ning, Gen Wang, Yihong Dong, Kechi Zhang, <span class="highlight-author">Ge Li</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been widely used in code completion, and
researchers are focusing on scaling up LLMs to improve their accuracy. However,
larger LLMs have lower inference efficiency, affecting developers' experience
and productivity. In this paper, we propose a lightweight and effective LLM for
code completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B
achieves higher code completion accuracy while having smaller scales (i.e., 7
billion parameters). We attribute the superiority of aiXcoder-7B to three key
factors: (1) Multi-objective training. We employ three training objectives, one
of which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers
the syntax structures in code and effectively improves the performance of LLMs
for code. (2) Diverse data sampling strategies. They consider inter-file
relationships and enhance the capability of LLMs in understanding cross-file
contexts. (3) Extensive high-quality data. We establish a rigorous data
collection pipeline and consume a total of 1.2 trillion unique tokens for
training aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a
broad distribution of code. We evaluate aiXcoder-7B in five popular code
completion benchmarks and a new benchmark collected by this paper. The results
show that aiXcoder-7B outperforms the latest six LLMs with similar sizes and
even surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B),
positioning aiXcoder-7B as a lightweight and effective LLM for academia and
industry. Finally, we summarize three valuable insights for helping
practitioners train the next generations of LLMs for code. aiXcoder-7B has been
open-souced and gained significant attention. Until January 2025, aiXcoder-7B
has received 2,226 GitHub Stars.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>(1) Accepted by the 47th International Conference on Software
  Engineering (ICSE 2025). (2) aiXcoder-7B is available at
  https://github.com/aixcoder-plugin/aiXcoder-7B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio<span class="highlight-title">BERT</span>: Audio Knowledge Augmented Language Model <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjong Ok, Suho Yoo, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have identified that language models, pretrained on text-only
datasets, often lack elementary visual knowledge, \textit{e.g.,} colors of
everyday objects. Motivated by this observation, we ask whether a similar
shortcoming exists in terms of the \textit{auditory} knowledge. To answer this
question, we construct a new dataset called AuditoryBench, which consists of
two novel tasks for evaluating auditory knowledge. Based on our analysis using
the benchmark, we find that language models also suffer from a severe lack of
auditory knowledge. To address this limitation, we propose AudioBERT, a novel
method to augment the auditory knowledge of BERT through a retrieval-based
approach. First, we detect auditory knowledge spans in prompts to query our
retrieval model efficiently. Then, we inject audio knowledge into BERT and
switch on low-rank adaptation for effective adaptation when audio knowledge is
required. Our experiments demonstrate that AudioBERT is quite effective,
achieving superior performance on the AuditoryBench. The dataset and code are
available at \bulurl{https://github.com/HJ-Ok/AudioBERT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Focus On This, Not That! Steering LLMs With Adaptive Feature
  Specification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom A. Lamb, Adam Davies, Alasdair Paren, Philip H. S. Torr, Francesco Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of Instruction Tuning (IT) in training large language
models (LLMs) to perform arbitrary user-specified tasks, these models often
still leverage spurious or biased features learned from their training data,
leading to undesired behaviours when deploying them in new contexts. In this
work, we introduce Focus Instruction Tuning (FIT), which trains LLMs to
condition their responses by focusing on specific features whilst ignoring
others, leading to different behaviours based on what features are specified.
Across several experimental settings, we show that focus-tuned models can be
adaptively steered by focusing on different features at inference-time: for
instance, robustness can be improved by focusing on task-causal features and
ignoring spurious features, and social bias can be mitigated by ignoring
demographic categories. Furthermore, FIT can steer behaviour in new contexts,
generalising under distribution shift and to new unseen features at inference
time, and thereby facilitating more robust, fair, and controllable LLM
applications in real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Fine-Tuned Retrieval-Augmented Generation with Long-Context
  Support: For 3GPP Standards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Erak, Nouf Alabbasi, Omar Alhussein, Ismail Lotfi, Amr Hussein, Sami Muhaidat, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show that large language models (LLMs) struggle with technical
standards in telecommunications. We propose a fine-tuned retrieval-augmented
generation (RAG) system based on the Phi-2 small language model (SLM) to serve
as an oracle for communication networks. Our developed system leverages
forward-looking semantic chunking to adaptively determine parsing breakpoints
based on embedding similarity, enabling effective processing of diverse
document formats. To handle the challenge of multiple similar contexts in
technical standards, we employ a re-ranking algorithm to prioritize the most
relevant retrieved chunks. Recognizing the limitations of Phi-2's small context
window, we implement a recent technique, namely SelfExtend, to expand the
context window during inference, which not only boosts the performance but also
can accommodate a wider range of user queries and design requirements from
customers to specialized technicians. For fine-tuning, we utilize the low-rank
adaptation (LoRA) technique to enhance computational efficiency during training
and enable effective fine-tuning on small datasets. Our comprehensive
experiments demonstrate substantial improvements over existing
question-answering approaches in the telecom domain, achieving performance that
exceeds larger language models such as GPT-4 (which is about 880 times larger
in size). This work presents a novel approach to leveraging SLMs for
communication networks, offering a balance of efficiency and performance. This
work can serve as a foundation towards agentic language models for networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to Proc. IEEE Globecom</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAGBench: Explainable Benchmark for Retrieval-Augmented Generation
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Friel, Masha Belyi, Atindriyo Sanyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has become a standard architectural
pattern for incorporating domain-specific knowledge into user-facing chat
applications powered by Large Language Models (LLMs). RAG systems are
characterized by (1) a document retriever that queries a domain-specific corpus
for context information relevant to an input query, and (2) an LLM that
generates a response based on the provided query and context. However,
comprehensive evaluation of RAG systems remains a challenge due to the lack of
unified evaluation criteria and annotated datasets. In response, we introduce
RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k
examples. It covers five unique industry-specific domains and various RAG task
types. RAGBench examples are sourced from industry corpora such as user
manuals, making it particularly relevant for industry applications. Further, we
formalize the TRACe evaluation framework: a set of explainable and actionable
RAG evaluation metrics applicable across all RAG domains. We release the
labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench.
RAGBench explainable labels facilitate holistic evaluation of RAG systems,
enabling actionable feedback for continuous improvement of production
applications. Thorough extensive benchmarking, we find that LLM-based RAG
evaluation methods struggle to compete with a finetuned RoBERTa model on the
RAG evaluation task. We identify areas where existing approaches fall short and
propose the adoption of RAGBench with TRACe towards advancing the state of RAG
evaluation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond
  Words <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, Zhizheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech encompasses a wealth of information, including but not limited to
content, paralinguistic, and environmental information. This comprehensive
nature of speech significantly impacts communication and is crucial for
human-computer interaction. Chat-Oriented Large Language Models (LLMs), known
for their general-purpose assistance capabilities, have evolved to handle
multi-modal inputs, including speech. Although these models can be adept at
recognizing and analyzing speech, they often fall short of generating
appropriate responses. We argue that this is due to the lack of principles on
task definition and model development, which requires open-source datasets and
metrics suitable for model evaluation. To bridge the gap, we present SD-Eval, a
benchmark dataset aimed at multidimensional evaluation of spoken dialogue
understanding and generation. SD-Eval focuses on paralinguistic and
environmental information and includes 7,303 utterances, amounting to 8.76
hours of speech data. The data is aggregated from eight public datasets,
representing four perspectives: emotion, accent, age, and background sound. To
assess the SD-Eval benchmark dataset, we implement three different models and
construct a training set following a process similar to that of SD-Eval. The
training set contains 1,052.72 hours of speech data and 724.4k utterances. We
also conduct a comprehensive evaluation using objective evaluation methods
(e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the
generated responses. Models conditioned with paralinguistic and environmental
information outperform their counterparts in both objective and subjective
measures. Moreover, experiments demonstrate that LLM-based metrics show a
higher correlation with human evaluation compared to traditional metrics. We
open-source SD-Eval at https://github.com/amphionspace/SD-Eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discriminative Representation learning via Attention-Enhanced
  Contrastive Learning for Short Text Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03584v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03584v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has gained significant attention in short text
clustering, yet it has an inherent drawback of mistakenly identifying samples
from the same category as negatives and then separating them in the feature
space (false negative separation), which hinders the generation of superior
representations. To generate more discriminative representations for efficient
clustering, we propose a novel short text clustering method, called
Discriminative Representation learning via \textbf{A}ttention-\textbf{E}nhanced
\textbf{C}ontrastive \textbf{L}earning for Short Text Clustering
(\textbf{AECL}). The \textbf{AECL} consists of two modules which are the
pseudo-label generation module and the contrastive learning module. Both
modules build a sample-level attention mechanism to capture similarity
relationships between samples and aggregate cross-sample features to generate
consistent representations. Then, the former module uses the more
discriminative consistent representation to produce reliable supervision
information for assist clustering, while the latter module explores similarity
relationships and consistent representations optimize the construction of
positive samples to perform similarity-guided contrastive learning, effectively
addressing the false negative separation issue. Experimental results
demonstrate that the proposed \textbf{AECL} outperforms state-of-the-art
methods. If the paper is accepted, we will open-source the code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language models (LMs) have sparked growing interest in
developing LM agents. While fully autonomous agents could excel in many
scenarios, numerous use cases inherently require them to collaborate with
humans due to humans' latent preferences, domain expertise, or need for
control. To facilitate the study of human-agent collaboration, we present
Collaborative Gym (Co-Gym), a general framework enabling asynchronous,
tripartite interaction among agents, humans, and task environments. We
instantiate Co-Gym with three representative tasks in both simulated and
real-world conditions, and propose an evaluation framework that assesses both
the collaboration outcomes and processes. Our findings reveal that
collaborative agents consistently outperform their fully autonomous
counterparts in task performance within those delivered cases, achieving win
rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related
Work when evaluated by real users. However, our study also highlights
significant challenges in developing collaborative agents, requiring
advancements in core aspects of intelligence -- communication capabilities,
situational awareness, and balancing autonomy and human control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERaLiON-TextLLM: Cross-Lingual Understanding of <span class="highlight-title">Large Language Model</span>s
  in Chinese, Indonesian, Malay, and Singlish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Huang, Tarun Kumar Vangani, Minh Duc Pham, Xunlong Zou, Bin Wang, Zhengyuan Liu, Ai Ti Aw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual large language models (MLLMs) have shown impressive capabilities
across a variety of languages. However, efficacy can differ greatly between
different language families, especially for those with limited linguistic
resources. This report presents MERaLiON-TextLLM, a series of open-source
language models specifically tailored to improve understanding and generation
in Chinese, Indonesian, Malay, and Singlish. The initial released model is
built on Llama-3-8B-Base and refined through a meticulously crafted process of
continued pre-training and weight merging. Our approach achieves performance
improvements across benchmarks in these languages, exceeding the capabilities
of the official Llama-3 models. We provide the model checkpoints as a resource
to support further research and development in cross-lingual language
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do LLMs Really Think Step-by-step In Implicit Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15862v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15862v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been well-known that Chain-of-Thought can remarkably enhance LLMs'
performance on complex tasks. However, because it also introduces slower
inference speeds and higher computational costs, many researches have attempted
to use implicit CoT, which does not need LLMs to explicitly generate the
intermediate steps. However, the invisible reasoning process leaves us a doubt
that, can implicit CoT really be equal to explicit CoT? Therefore, in this
study, we address this question through experiments. We probe the information
of intermediate steps from the model's hidden states when it is either trained
or prompted to perform implicit CoT. The results surprisingly indicate that
when prompted, LLMs hardly think about intermediate steps, suggesting they may
just rely on experience rather than strict step-by-step reasoning. But when
trained, they indeed calculate intermediate steps. Moreover, in both
situations, we find the effect of using implicit CoT is susceptible to the
format of the problem, reaffirming the current deficiency of implicit CoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is in
  https://github.com/yuyijiong/if_step_by_step_implicit_CoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERaLiON-AudioLLM: Bridging Audio and Language with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09818v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09818v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxu He, Zhuohan Liu, Shuo Sun, Bin Wang, Wenyu Zhang, Xunlong Zou, Nancy F. Chen, Ai Ti Aw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MERaLiON-AudioLLM (Multimodal Empathetic Reasoning and Learning
in One Network), the first speech-text model tailored for Singapore's
multilingual and multicultural landscape. Developed under the National Large
Language Models Funding Initiative, Singapore, MERaLiON-AudioLLM integrates
advanced speech and text processing to address the diverse linguistic nuances
of local accents and dialects, enhancing accessibility and usability in
complex, multilingual environments. Our results demonstrate improvements in
both speech recognition and task-specific understanding, positioning
MERaLiON-AudioLLM as a pioneering solution for region specific AI applications.
We envision this release to set a precedent for future models designed to
address localised linguistic and cultural contexts in a global framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/MERaLiON/MERaLiON-AudioLLM-Whisper-SEA-LION</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrisisSense-LLM: Instruction Fine-Tuned <span class="highlight-title">Large Language Model</span> for
  Multi-label Social Media Text Classification in Disaster Informatics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Yin, Chengkai Liu, Ali Mostafavi, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of crisis/disaster informatics, social media is increasingly
being used for improving situational awareness to inform response and relief
efforts. Efficient and accurate text classification tools have been a focal
area of investigation in crisis informatics. However, current methods mostly
rely on single-label text classification models, which fails to capture
different insights embedded in dynamic and multifaceted disaster-related social
media data. This study introduces a novel approach to disaster text
classification by enhancing a pre-trained Large Language Model (LLM) through
instruction fine-tuning targeted for multi-label classification of
disaster-related tweets. Our methodology involves creating a comprehensive
instruction dataset from disaster-related tweets, which is then used to
fine-tune an open-source LLM, thereby embedding it with disaster-specific
knowledge. This fine-tuned model can classify multiple aspects of
disaster-related information simultaneously, such as the type of event,
informativeness, and involvement of human aid, significantly improving the
utility of social media data for situational awareness in disasters. The
results demonstrate that this approach enhances the categorization of critical
information from social media posts, thereby facilitating a more effective
deployment for situational awareness during emergencies. This research paves
the way for more advanced, adaptable, and robust disaster management tools,
leveraging the capabilities of LLMs to improve real-time situational awareness
and response strategies in disaster scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Relevant source code and data is available:
  https://github.com/KaiYin97/CrsisLLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Framework for Inference-time Scaling and Steering of Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, Rajesh Ranganath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models produce impressive results in modalities ranging from images
and video to protein design and text. However, generating samples with
user-specified properties remains a challenge. Recent research proposes
fine-tuning models to maximize rewards that capture desired properties, but
these methods require expensive training and are prone to mode collapse. In
this work, we propose Feynman Kac (FK) steering, an inference-time framework
for steering diffusion models with reward functions. FK steering works by
sampling a system of multiple interacting diffusion processes, called
particles, and resampling particles at intermediate steps based on scores
computed using functions called potentials. Potentials are defined using
rewards for intermediate states and are selected such that a high value
indicates that the particle will yield a high-reward sample. We explore various
choices of potentials, intermediate rewards, and samplers. We evaluate FK
steering on text-to-image and text diffusion models. For steering text-to-image
models with a human preference reward, we find that FK steering a 0.8B
parameter model outperforms a 2.6B parameter fine-tuned model on prompt
fidelity, with faster sampling and no training. For steering text diffusion
models with rewards for text quality and specific text attributes, we find that
FK steering generates lower perplexity, more linguistically acceptable outputs
and enables gradient-free control of attributes like toxicity. Our results
demonstrate that inference-time scaling and steering of diffusion models, even
with off-the-shelf rewards, can provide significant sample quality gains and
controllability benefits. Code is available at
https://github.com/zacharyhorvitz/Fk-Diffusion-Steering .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span>ing Attitudinal Alignment Between <span class="highlight-title">Large Language Model</span>s Vs. Humans
  Towards 17 Sustainable Development Goals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyang Wu, Ying Xu, Tingsong Xiao, Yunze Xiao, Yitong Li, Tianyang Wang, Yichi Zhang, Shanghai Zhong, Yuwei Zhang, Wei Lu, Yifan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as potent tools for advancing the
United Nations' Sustainable Development Goals (SDGs). However, the attitudinal
disparities between LLMs and humans towards these goals can pose significant
challenges. This study conducts a comprehensive review and analysis of the
existing literature on the attitudes of LLMs towards the 17 SDGs, emphasizing
the comparison between their attitudes and support for each goal and those of
humans. We examine the potential disparities, primarily focusing on aspects
such as understanding and emotions, cultural and regional differences, task
objective variations, and factors considered in the decision-making process.
These disparities arise from the underrepresentation and imbalance in LLM
training data, historical biases, quality issues, lack of contextual
understanding, and skewed ethical values reflected. The study also investigates
the risks and harms that may arise from neglecting the attitudes of LLMs
towards the SDGs, including the exacerbation of social inequalities, racial
discrimination, environmental destruction, and resource wastage. To address
these challenges, we propose strategies and recommendations to guide and
regulate the application of LLMs, ensuring their alignment with the principles
and goals of the SDGs, and therefore creating a more just, inclusive, and
sustainable future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PeFoMed: Parameter Efficient <span class="highlight-title">Fine-tuning</span> of Multimodal Large Language
  Models for Medical Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02797v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02797v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong He, Pengfei Li, Gang Liu, Genrong He, Zhaolin Chen, Shenjun Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) represent an evolutionary expansion
in the capabilities of traditional large language models, enabling them to
tackle challenges that surpass the scope of purely text-based applications. It
leverages the knowledge previously encoded within these language models,
thereby enhancing their applicability and functionality in the reign of
multimodal contexts. Recent works investigate the adaptation of MLLMs as a
universal solution to address medical multi-modal problems as a generative
task. In this paper, we propose a parameter efficient framework for fine-tuning
MLLMs, specifically validated on medical visual question answering (Med-VQA)
and medical report generation (MRG) tasks, using public benchmark datasets. We
also introduce an evaluation metric using the 5-point Likert scale and its
weighted average value to measure the quality of the generated reports for MRG
tasks, where the scale ratings are labelled by both humans manually and the
GPT-4 model. We further assess the consistency of performance metrics across
traditional measures, GPT-4, and human ratings for both VQA and MRG tasks. The
results indicate that semantic similarity assessments using GPT-4 align closely
with human annotators and provide greater stability, yet they reveal a
discrepancy when compared to conventional lexical similarity measurements. This
questions the reliability of lexical similarity metrics for evaluating the
performance of generative models in Med-VQA and report generation tasks.
Besides, our fine-tuned model significantly outperforms GPT-4v. This indicates
that without additional fine-tuning, multi-modal models like GPT-4v do not
perform effectively on medical imaging tasks. The code will be available here:
https://github.com/jinlHe/PeFoMed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures
  and Languages <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew Ali Ayele, Víctor Gutiérrez-Basulto, Yazmín Ibáñez-García, Hwaran Lee, Shamsuddeen Hassan Muhammad, Kiwoong Park, Anar Sabuhi Rzayev, Nina White, Seid Muhie Yimam, Mohammad Taher Pilehvar, Nedjma Ousidhoum, Jose Camacho-Collados, Alice Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often lack culture-specific knowledge of daily
life, especially across diverse regions and non-English languages. Existing
benchmarks for evaluating LLMs' cultural sensitivities are limited to a single
language or collected from online sources such as Wikipedia, which do not
reflect the mundane everyday lifestyles of diverse regions. That is,
information about the food people eat for their birthday celebrations, spices
they typically use, musical instruments youngsters play, or the sports they
practice in school is common cultural knowledge but uncommon in easily
collected online sources, especially for underrepresented cultures. To address
this issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate
LLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises
52.6k question-answer pairs from 16 countries/regions, in 13 different
languages, including low-resource ones such as Amharic, Assamese, Azerbaijani,
Hausa, and Sundanese. We construct the benchmark to include two formats of
questions: short-answer and multiple-choice. We show that LLMs perform better
for cultures that are highly represented online, with a maximum 57.34%
difference in GPT-4, the best-performing model, in the short-answer format. For
cultures represented by mid-to-high-resource languages, LLMs perform better in
their local languages, but for cultures represented by low-resource languages,
LLMs perform better in English than the local languages. We make our dataset
publicly available at: https://github.com/nlee0212/BLEnD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2024 Datasets & Benchmark Track</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">36</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASTRA: A Scene-aware <span class="highlight-title">TRAnsformer</span>-based model for trajectory prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Izzeddin Teeti, Aniket Thomas, Munish Monga, Sachin Kumar, Uddeshya Singh, Andrew Bradley, Biplab Banerjee, Fabio Cuzzolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ASTRA (A} Scene-aware TRAnsformer-based model for trajectory
prediction), a light-weight pedestrian trajectory forecasting model that
integrates the scene context, spatial dynamics, social inter-agent interactions
and temporal progressions for precise forecasting. We utilised a U-Net-based
feature extractor, via its latent vector representation, to capture scene
representations and a graph-aware transformer encoder for capturing social
interactions. These components are integrated to learn an agent-scene aware
embedding, enabling the model to learn spatial dynamics and forecast the future
trajectory of pedestrians. The model is designed to produce both deterministic
and stochastic outcomes, with the stochastic predictions being generated by
incorporating a Conditional Variational Auto-Encoder (CVAE). ASTRA also
proposes a simple yet effective weighted penalty loss function, which helps to
yield predictions that outperform a wide array of state-of-the-art
deterministic and generative models. ASTRA demonstrates an average improvement
of 27%/10% in deterministic/stochastic settings on the ETH-UCY dataset, and 26%
improvement on the PIE dataset, respectively, along with seven times fewer
parameters than the existing state-of-the-art model (see Figure 1).
Additionally, the model's versatility allows it to generalize across different
perspectives, such as Bird's Eye View (BEV) and Ego-Vehicle View (EVV).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Explainability to Interpretability: Interpretable Policies in
  Reinforcement Learning Via Model Explanation <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peilang Li, Umer Siddique, Yongcan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (RL) has shown remarkable success in complex
domains, however, the inherent black box nature of deep neural network policies
raises significant challenges in understanding and trusting the decision-making
processes. While existing explainable RL methods provide local insights, they
fail to deliver a global understanding of the model, particularly in
high-stakes applications. To overcome this limitation, we propose a novel
model-agnostic approach that bridges the gap between explainability and
interpretability by leveraging Shapley values to transform complex deep RL
policies into transparent representations. The proposed approach offers two key
contributions: a novel approach employing Shapley values to policy
interpretation beyond local explanations and a general framework applicable to
off-policy and on-policy algorithms. We evaluate our approach with three
existing deep RL algorithms and validate its performance in two classic control
environments. The results demonstrate that our approach not only preserves the
original models' performance but also generates more stable interpretable
policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Deployable AI (DAI) Workshop at the Thirty-Ninth AAAI
  Conference on Artificial Intelligence (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified
  Intermediate Representation <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Berian, Daniel Brignac, JhihYang Wu, Natnael Daba, Abhijit Mahalanobis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geospatial imaging leverages data from diverse sensing modalities-such as EO,
SAR, and LiDAR, ranging from ground-level drones to satellite views. These
heterogeneous inputs offer significant opportunities for scene understanding
but present challenges in interpreting geometry accurately, particularly in the
absence of precise ground truth data. To address this, we propose
CrossModalityDiffusion, a modular framework designed to generate images across
different modalities and viewpoints without prior knowledge of scene geometry.
CrossModalityDiffusion employs modality-specific encoders that take multiple
input images and produce geometry-aware feature volumes that encode scene
structure relative to their input camera positions. The space where the feature
volumes are placed acts as a common ground for unifying input modalities. These
feature volumes are overlapped and rendered into feature images from novel
perspectives using volumetric rendering techniques. The rendered feature images
are used as conditioning inputs for a modality-specific diffusion model,
enabling the synthesis of novel images for the desired output modality. In this
paper, we show that jointly training different modules ensures consistent
geometric understanding across all modalities within the framework. We validate
CrossModalityDiffusion's capabilities on the synthetic ShapeNet cars dataset,
demonstrating its effectiveness in generating accurate and consistent novel
views across multiple imaging modalities and perspectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 2025 WACV workshop GeoCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Language Barriers in Healthcare: A Study on Arabic LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nada Saadi, Tathagata Raha, Clément Christophe, Marco AF Pimentel, Ronnie Rajan, Praveen K Kanithi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the challenges of developing large language models
(LLMs) proficient in both multilingual understanding and medical knowledge. We
demonstrate that simply translating medical data does not guarantee strong
performance on clinical tasks in the target language. Our experiments reveal
that the optimal language mix in training data varies significantly across
different medical tasks. We find that larger models with carefully calibrated
language ratios achieve superior performance on native-language clinical tasks.
Furthermore, our results suggest that relying solely on fine-tuning may not be
the most effective approach for incorporating new language knowledge into LLMs.
Instead, data and computationally intensive pretraining methods may still be
necessary to achieve optimal performance in multilingual medical settings.
These findings provide valuable guidance for building effective and inclusive
medical AI systems for diverse linguistic communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Single-Image-Based Morphing Attack Detection Using Deep
  Representations from Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face morphing attacks have posed severe threats to Face Recognition Systems
(FRS), which are operated in border control and passport issuance use cases.
Correspondingly, morphing attack detection algorithms (MAD) are needed to
defend against such attacks. MAD approaches must be robust enough to handle
unknown attacks in an open-set scenario where attacks can originate from
various morphing generation algorithms, post-processing and the diversity of
printers/scanners. The problem of generalization is further pronounced when the
detection has to be made on a single suspected image. In this paper, we propose
a generalized single-image-based MAD (S-MAD) algorithm by learning the encoding
from Vision Transformer (ViT) architecture. Compared to CNN-based
architectures, ViT model has the advantage on integrating local and global
information and hence can be suitable to detect the morphing traces widely
distributed among the face region. Extensive experiments are carried out on
face morphing datasets generated using publicly available FRGC face datasets.
Several state-of-the-art (SOTA) MAD algorithms, including representative ones
that have been publicly evaluated, have been selected and benchmarked with our
ViT-based approach. Obtained results demonstrate the improved detection
performance of the proposed S-MAD method on inter-dataset testing (when
different data is used for training and testing) and comparable performance on
intra-dataset testing (when the same data is used for training and testing)
experimental protocol.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Generalization in Chain of Thought Reasoning for Smaller
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxwell J. Yin, Dingyi Jiang, Yongbing Chen, Boyu Wang, Charles Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning in smaller language models is a challenging
natural language process problem yet highly desirable in many real-life
applications. Existing CoT knowledge distillation methods often suffer from
overly conservative memorization in smaller LLMs, leading to low generalization
confidence. As fully preserving the CoT ability of teacher model is impossible,
we hypothesize that adversarial CoT fine-tuning is crucial for developing
smaller LLM with robust CoT generalization. To this end, we propose
\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled
fine-tuning framework that integrates diverse CoT domains. Specifically, PRADA
pioneers two CoT improvements in smaller LLM: (1) Recovering the
domain-invariant feature insight which typically lost during distillation with
domain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT
prompt engineering by employing domain-adversarial approaches. We theoretically
demonstrate the effectiveness of our approach and empirically show that it
significantly outperforms the state of the arts in a wide range of tasks.
Moreover, our empirical findings reveal that the smaller LLM, when leveraging
PRADA, aligns closely with domain knowledge, thereby improving the
explainability of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learnings from Scaling Visual Tokenizers for Reconstruction and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual tokenization via auto-encoding empowers state-of-the-art image and
video generative models by compressing pixels into a latent space. Although
scaling Transformer-based generators has been central to recent advances, the
tokenizer component itself is rarely scaled, leaving open questions about how
auto-encoder design choices influence both its objective of reconstruction and
downstream generative performance. Our work aims to conduct an exploration of
scaling in auto-encoders to fill in this blank. To facilitate this exploration,
we replace the typical convolutional backbone with an enhanced Vision
Transformer architecture for Tokenization (ViTok). We train ViTok on
large-scale image and video datasets far exceeding ImageNet-1K, removing data
constraints on tokenizer scaling. We first study how scaling the auto-encoder
bottleneck affects both reconstruction and generation -- and find that while it
is highly correlated with reconstruction, its relationship with generation is
more complex. We next explored the effect of separately scaling the
auto-encoders' encoder and decoder on reconstruction and generation
performance. Crucially, we find that scaling the encoder yields minimal gains
for either reconstruction or generation, while scaling the decoder boosts
reconstruction but the benefits for generation are mixed. Building on our
exploration, we design ViTok as a lightweight auto-encoder that achieves
competitive performance with state-of-the-art auto-encoders on ImageNet-1K and
COCO reconstruction tasks (256p and 512p) while outperforming existing
auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x
fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates
competitive performance on image generation for ImageNet-1K and sets new
state-of-the-art benchmarks for class-conditional video generation on UCF-101.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 25 figures, 7 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniThink: Expanding Knowledge Boundaries in Machine Writing through
  Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine writing with large language models often relies on
retrieval-augmented generation. However, these approaches remain confined
within the boundaries of the model's predefined scope, limiting the generation
of content with rich information. Specifically, vanilla-retrieved information
tends to lack depth, utility, and suffers from redundancy, which negatively
impacts the quality of generated articles, leading to shallow, repetitive, and
unoriginal outputs. To address these issues, we propose OmniThink, a machine
writing framework that emulates the human-like process of iterative expansion
and reflection. The core idea behind OmniThink is to simulate the cognitive
behavior of learners as they progressively deepen their knowledge of the
topics. Experimental results demonstrate that OmniThink improves the knowledge
density of generated articles without compromising metrics such as coherence
and depth. Human evaluations and expert feedback further highlight the
potential of OmniThink to address real-world challenges in the generation of
long-form articles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity
  Recognition and Normalization for Dysmorphology Physical Examination Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hajung Kim, Chanhwi Kim, Jiwoong Sohn, Tim Beck, Marek Rei, Sunkyu Kim, T Ian Simpson, Joram M Posma, Antoine Lain, Mujeen Sung, Jaewoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of BioCreative8 Track 3 is to extract phenotypic key medical
findings embedded within EHR texts and subsequently normalize these findings to
their Human Phenotype Ontology (HPO) terms. However, the presence of diverse
surface forms in phenotypic findings makes it challenging to accurately
normalize them to the correct HPO terms. To address this challenge, we explored
various models for named entity recognition and implemented data augmentation
techniques such as synonym marginalization to enhance the normalization step.
Our pipeline resulted in an exact extraction and normalization F1 score 2.6\%
higher than the mean score of all submissions received in response to the
challenge. Furthermore, in terms of the normalization F1 score, our approach
surpassed the average performance by 1.9\%. These findings contribute to the
advancement of automated medical data extraction and normalization techniques,
showcasing potential pathways for future research and application in the
biomedical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article is part of the Proceedings of the BioCreative VIII
  Challenge and Workshop: Curation and Evaluation in the era of Generative
  Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel multi-objective metaheuristics for smart communications in
  vehicular networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamal Toutouh, Enrique Alba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article analyzes the use of two parallel multi-objective soft computing
algorithms to automatically search for high-quality settings of the Ad hoc On
Demand Vector routing protocol for vehicular networks. These methods are based
on an evolutionary algorithm and on a swarm intelligence approach. The
experimental analysis demonstrates that the configurations computed by our
optimization algorithms outperform other state-of-the-art optimized ones. In
turn, the computational efficiency achieved by all the parallel versions is
greater than 87 %. Therefore, the line of work presented in this article
represents an efficient framework to improve vehicular communications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Aerial Detection Baseline of Multimodal Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, Xue Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multimodal language models (MLMs) based on generative pre-trained
Transformer are considered powerful candidates for unifying various domains and
tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding
performance in multiple tasks, such as visual question answering and visual
grounding. In addition to visual grounding that detects specific objects
corresponded to given instruction, aerial detection, which detects all objects
of multiple categories, is also a valuable and challenging task for RS
foundation models. However, aerial detection has not been explored by existing
RS MLMs because the autoregressive prediction mechanism of MLMs differs
significantly from the detection outputs. In this paper, we present a simple
baseline for applying MLMs to aerial detection for the first time, named
LMMRotate. Specifically, we first introduce a normalization method to transform
detection outputs into textual outputs to be compatible with the MLM framework.
Then, we propose a evaluation method, which ensures a fair comparison between
MLMs and conventional object detection models. We construct the baseline by
fine-tuning open-source general-purpose MLMs and achieve impressive detection
performance comparable to conventional detector. We hope that this baseline
will serve as a reference for future MLM development, enabling more
comprehensive capabilities for understanding RS images. Code is available at
https://github.com/Li-Qingyun/mllm-mmrotate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 table, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CyberMentor: AI Powered Learning Tool Platform to Address Diverse
  Student Needs in Cybersecurity Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Wang, Nianjun Zhou, Zhixiong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many non-traditional students in cybersecurity programs often lack access to
advice from peers, family members and professors, which can hinder their
educational experiences. Additionally, these students may not fully benefit
from various LLM-powered AI assistants due to issues like content relevance,
locality of advice, minimum expertise, and timing. This paper addresses these
challenges by introducing an application designed to provide comprehensive
support by answering questions related to knowledge, skills, and career
preparation advice tailored to the needs of these students. We developed a
learning tool platform, CyberMentor, to address the diverse needs and pain
points of students majoring in cybersecurity. Powered by agentic workflow and
Generative Large Language Models (LLMs), the platform leverages
Retrieval-Augmented Generation (RAG) for accurate and contextually relevant
information retrieval to achieve accessibility and personalization. We
demonstrated its value in addressing knowledge requirements for cybersecurity
education and for career marketability, in tackling skill requirements for
analytical and programming assignments, and in delivering real time on demand
learning support. Using three use scenarios, we showcased CyberMentor in
facilitating knowledge acquisition and career preparation and providing
seamless skill-based guidance and support. We also employed the LangChain
prompt-based evaluation methodology to evaluate the platform's impact,
confirming its strong performance in helpfulness, correctness, and
completeness. These results underscore the system's ability to support students
in developing practical cybersecurity skills while improving equity and
sustainability within higher education. Furthermore, CyberMentor's open-source
design allows for adaptation across other disciplines, fostering educational
innovation and broadening its potential impact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Goofus & Gallant Story Corpus for Practical Value Alignment <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Sultan Al Nahian, Tasmia Tasrin, Spencer Frazier, Mark Riedl, Brent Harrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Values or principles are key elements of human society that influence people
to behave and function according to an accepted standard set of social rules to
maintain social order. As AI systems are becoming ubiquitous in human society,
it is a major concern that they could violate these norms or values and
potentially cause harm. Thus, to prevent intentional or unintentional harm, AI
systems are expected to take actions that align with these principles. Training
systems to exhibit this type of behavior is difficult and often requires a
specialized dataset. This work presents a multi-modal dataset illustrating
normative and non-normative behavior in real-life situations described through
natural language and artistic images. This training set contains curated sets
of images that are designed to teach young children about social principles. We
argue that this is an ideal dataset to use for training socially normative
agents given this fact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Conference on Machine Learning and
  Applications (ICMLA) 2024. Main Conference, Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Continual Forgetting for <span class="highlight-title">Pre-train</span>ed Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For privacy and security concerns, the need to erase unwanted information
from pre-trained vision models is becoming evident nowadays. In real-world
scenarios, erasure requests originate at any time from both users and model
owners, and these requests usually form a sequence. Therefore, under such a
setting, selective information is expected to be continuously removed from a
pre-trained model while maintaining the rest. We define this problem as
continual forgetting and identify three key challenges. (i) For unwanted
knowledge, efficient and effective deleting is crucial. (ii) For remaining
knowledge, the impact brought by the forgetting procedure should be minimal.
(iii) In real-world scenarios, the training samples may be scarce or partially
missing during the process of forgetting. To address them, we first propose
Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA
modules to fine-tune the FFN layers in Transformer blocks for each forgetting
task independently, and towards (ii), a simple group sparse regularization is
adopted, enabling automatic selection of specific LoRA groups and zeroing out
the others. To further extend GS-LoRA to more practical scenarios, we
incorporate prototype information as additional supervision and introduce a
more practical approach, GS-LoRA++. For each forgotten class, we move the
logits away from its original prototype. For the remaining classes, we pull the
logits closer to their respective prototypes. We conduct extensive experiments
on face recognition, object detection and image classification and demonstrate
that our method manages to forget specific classes with minimal impact on other
classes. Codes have been released on https://github.com/bjzhb666/GS-LoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cueless EEG imagined speech for subject identification: dataset and
  benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Derakhshesh, Zahra Dehghanian, Reza Ebrahimpour, Hamid R. Rabiee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalogram (EEG) signals have emerged as a promising modality for
biometric identification. While previous studies have explored the use of
imagined speech with semantically meaningful words for subject identification,
most have relied on additional visual or auditory cues. In this study, we
introduce a cueless EEG-based imagined speech paradigm, where subjects imagine
the pronunciation of semantically meaningful words without any external cues.
This innovative approach addresses the limitations of prior methods by
requiring subjects to select and imagine words from a predefined list
naturally. The dataset comprises over 4,350 trials from 11 subjects across five
sessions. We assess a variety of classification methods, including traditional
machine learning techniques such as Support Vector Machines (SVM) and XGBoost,
as well as time-series foundation models and deep learning architectures
specifically designed for EEG classification, such as EEG Conformer and Shallow
ConvNet. A session-based hold-out validation strategy was employed to ensure
reliable evaluation and prevent data leakage. Our results demonstrate
outstanding classification accuracy, reaching 97.93%. These findings highlight
the potential of cueless EEG paradigms for secure and reliable subject
identification in real-world applications, such as brain-computer interfaces
(BCIs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward-Guided Controlled Generation for Inference-Time Alignment in
  Diffusion Models: Tutorial and <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, Tommaso Biancalani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This tutorial provides an in-depth guide on inference-time guidance and
alignment methods for optimizing downstream reward functions in diffusion
models. While diffusion models are renowned for their generative modeling
capabilities, practical applications in fields such as biology often require
sample generation that maximizes specific metrics (e.g., stability, affinity in
proteins, closeness to target structures). In these scenarios, diffusion models
can be adapted not only to generate realistic samples but also to explicitly
maximize desired measures at inference time without fine-tuning. This tutorial
explores the foundational aspects of such inference-time algorithms. We review
these methods from a unified perspective, demonstrating that current techniques
-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,
and classifier guidance -- aim to approximate soft optimal denoising processes
(a.k.a. policies in RL) that combine pre-trained denoising processes with value
functions serving as look-ahead functions that predict from intermediate states
to terminal rewards. Within this framework, we present several novel algorithms
not yet covered in the literature. Furthermore, we discuss (1) fine-tuning
methods combined with inference-time techniques, (2) inference-time algorithms
based on search algorithms such as Monte Carlo tree search, which have received
limited attention in current research, and (3) connections between
inference-time algorithms in language models and diffusion models. The code of
this tutorial on protein design is available at
https://github.com/masa-ue/AlignInversePro
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We plan to add more content/codes. Please let us know if there are
  any comments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Quantum Advantage in Quantum Circuit Generation through
  Genetic Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Stein, Michael Färber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing efficient quantum circuits that leverage quantum advantage compared
to classical computing has become increasingly critical. Genetic algorithms
have shown potential in generating such circuits through artificial evolution.
However, integrating quantum advantage into the fitness function of these
algorithms remains unexplored. In this paper, we aim to enhance the efficiency
of quantum circuit design by proposing two novel approaches for incorporating
quantum advantage metrics into the fitness function of genetic algorithms.1 We
evaluate our approaches based on the Bernstein-Vazirani Problem and the
Unstructured Database Search Problem as test cases. The results demonstrate
that our approaches not only improve the convergence speed of the genetic
algorithm but also produce circuits comparable to expert-designed solutions.
Our findings suggest that automated quantum circuit design using genetic
algorithms that incorporate a measure of quantum advantage is a promising
approach to accelerating the development of quantum algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Authenticated Delegation and Authorized AI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobin South, Samuele Marro, Thomas Hardjono, Robert Mahari, Cedric Deslandes Whitney, Dazza Greenwood, Alan Chan, Alex Pentland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid deployment of autonomous AI agents creates urgent challenges around
authorization, accountability, and access control in digital spaces. New
standards are needed to know whom AI agents act on behalf of and guide their
use appropriately, protecting online spaces while unlocking the value of task
delegation to autonomous agents. We introduce a novel framework for
authenticated, authorized, and auditable delegation of authority to AI agents,
where human users can securely delegate and restrict the permissions and scope
of agents while maintaining clear chains of accountability. This framework
builds on existing identification and access management protocols, extending
OAuth 2.0 and OpenID Connect with agent-specific credentials and metadata,
maintaining compatibility with established authentication and web
infrastructure. Further, we propose a framework for translating flexible,
natural language permissions into auditable access control configurations,
enabling robust scoping of AI agent capabilities across diverse interaction
modalities. Taken together, this practical approach facilitates immediate
deployment of AI agents while addressing key security and accountability
concerns, working toward ensuring agentic AI systems perform only appropriate
actions and providing a tool for digital service providers to enable AI agent
interactions without risking harm from scalable interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP
  Evaluation Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis Roger, Prateek Humane, Daniel Z. Kaplan, Kshitij Gupta, Qi Sun, George Adamopoulos, Jonathan Siu Chi Lim, Quentin Anthony, Edwin Fennell, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of Vision-Language Models (VLMs) in the past several years
calls for rigorous and comprehensive evaluation methods and benchmarks. This
work analyzes existing VLM evaluation techniques, including automated metrics,
AI-based assessments, and human evaluations across diverse tasks. We first
introduce Robin - a novel suite of VLMs that we built by combining Large
Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use
Robin to identify shortcomings of current evaluation approaches across scales.
Next, to overcome the identified limitations, we introduce CHIRP - a new long
form response benchmark we developed for more robust and complete VLM
evaluation. We provide open access to the Robin training code, model suite, and
CHIRP benchmark to promote reproducibility and advance VLM research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Heap: A Contamination-Free Multilingual <span class="highlight-title">Code</span> Dataset for Evaluating
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Katzy, Razvan Mihai Popescu, Arie van Deursen, Maliheh Izadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent rise in the popularity of large language models has spurred the
development of extensive code datasets needed to train them. This has left
limited code available for collection and use in the downstream investigation
of specific behaviors, or evaluation of large language models without suffering
from data contamination. To address this problem, we release The Heap, a large
multilingual dataset covering 57 programming languages that has been
deduplicated with respect to other open datasets of code, enabling researchers
to conduct fair evaluations of large language models without significant data
cleaning overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-Print. Accepted to FORGE 2025 Dataset Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monte Carlo Tree Search with Velocity Obstacles for safe and efficient
  motion planning in dynamic environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Bonanni, Daniele Meli, Alberto Castellini, Alessandro Farinelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online motion planning is a challenging problem for intelligent robots moving
in dense environments with dynamic obstacles, e.g., crowds. In this work, we
propose a novel approach for optimal and safe online motion planning with
minimal information about dynamic obstacles. Specifically, our approach
requires only the current position of the obstacles and their maximum speed,
but it does not need any information about their exact trajectories or dynamic
model. The proposed methodology combines Monte Carlo Tree Search (MCTS), for
online optimal planning via model simulations, with Velocity Obstacles (VO),
for obstacle avoidance. We perform experiments in a cluttered simulated
environment with walls, and up to 40 dynamic obstacles moving with random
velocities and directions. With an ablation study, we show the key contribution
of VO in scaling up the efficiency of MCTS, selecting the safest and most
rewarding actions in the tree of simulations. Moreover, we show the superiority
of our methodology with respect to state-of-the-art planners, including
Non-linear Model Predictive Control (NMPC), in terms of improved collision
rate, computational and task performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NS-Gym: Open-Source Simulation Environments and Benchmarks for
  Non-Stationary Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel S. Keplinger, Baiting Luo, Iliyas Bektas, Yunuo Zhang, Kyle Hollins Wray, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world applications, agents must make sequential decisions in
environments where conditions are subject to change due to various exogenous
factors. These non-stationary environments pose significant challenges to
traditional decision-making models, which typically assume stationary dynamics.
Non-stationary Markov decision processes (NS-MDPs) offer a framework to model
and solve decision problems under such changing conditions. However, the lack
of standardized benchmarks and simulation tools has hindered systematic
evaluation and advance in this field. We present NS-Gym, the first simulation
toolkit designed explicitly for NS-MDPs, integrated within the popular
Gymnasium framework. In NS-Gym, we segregate the evolution of the environmental
parameters that characterize non-stationarity from the agent's decision-making
module, allowing for modular and flexible adaptations to dynamic environments.
We review prior work in this domain and present a toolkit encapsulating key
problem characteristics and types in NS-MDPs. This toolkit is the first effort
to develop a set of standardized interfaces and benchmark problems to enable
consistent and reproducible evaluation of algorithms under non-stationary
conditions. We also benchmark six algorithmic approaches from prior work on
NS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to
assess the adaptability and robustness of their decision-making algorithms to
non-stationary conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through
  Category-Bounding <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Kirmayr, Lukas Stappen, Phillip Schneider, Florian Matthes, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's assistant landscape, personalisation enhances interactions,
fosters long-term relationships, and deepens engagement. However, many systems
struggle with retaining user preferences, leading to repetitive user requests
and disengagement. Furthermore, the unregulated and opaque extraction of user
preferences in industry applications raises significant concerns about privacy
and trust, especially in regions with stringent regulations like Europe. In
response to these challenges, we propose a long-term memory system for voice
assistants, structured around predefined categories. This approach leverages
Large Language Models to efficiently extract, store, and retrieve preferences
within these categories, ensuring both personalisation and transparency. We
also introduce a synthetic multi-turn, multi-session conversation dataset
(CarMem), grounded in real industry data, tailored to an in-car voice assistant
setting. Benchmarked on the dataset, our system achieves an F1-score of .78 to
.95 in preference extraction, depending on category granularity. Our
maintenance strategy reduces redundant preferences by 95% and contradictory
ones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,
the results demonstrate the system's suitability for industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the International Conference on
  Computational Linguistics (COLING 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Electronic Health Records: Towards Digital Twins in Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammet Alkan, Hester Huijsdens, Yola Jones, Fani Deligianni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pivotal shift from traditional paper-based records to sophisticated
Electronic Health Records (EHR), enabled systematic collection and analysis of
patient data through descriptive statistics, providing insight into patterns
and trends across patient populations. This evolution continued toward
predictive analytics, allowing healthcare providers to anticipate patient
outcomes and potential complications before they occur. This progression from
basic digital record-keeping to sophisticated predictive modelling and digital
twins reflects healthcare's broader evolution toward more integrated,
patient-centred approaches that combine data-driven insights with personalized
care delivery. This chapter explores the evolution and significance of
healthcare information systems, beginning with an examination of the
implementation of EHR in the UK and the USA. It provides a comprehensive
overview of the International Classification of Diseases (ICD) system, tracing
its development from ICD-9 to ICD-10. Central to this discussion is the
MIMIC-III database, a landmark achievement in healthcare data sharing and
arguably the most comprehensive critical care database freely available to
researchers worldwide. MIMIC-III has democratized access to high-quality
healthcare data, enabling unprecedented opportunities for research and
analysis. The chapter examines its structure, clinical outcome analysis
capabilities, and practical applications through case studies, with a
particular focus on mortality and length of stay metrics, vital signs
extraction, and ICD coding. Through detailed entity-relationship diagrams and
practical examples, the text illustrates MIMIC's complex data structure and
demonstrates how different querying approaches can lead to subtly different
results, emphasizing the critical importance of understanding the database's
architecture for accurate data extraction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Mechanistic Explanatory Strategy for XAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Rabiza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in XAI, scholars note a persistent lack of
solid conceptual foundations and integration with broader scientific discourse
on explanation. In response, emerging XAI research draws on explanatory
strategies from various sciences and philosophy of science literature to fill
these gaps. This paper outlines a mechanistic strategy for explaining the
functional organization of deep learning systems, situating recent advancements
in AI explainability within a broader philosophical context. According to the
mechanistic approach, the explanation of opaque AI systems involves identifying
mechanisms that drive decision-making. For deep neural networks, this means
discerning functionally relevant components -- such as neurons, layers,
circuits, or activation patterns -- and understanding their roles through
decomposition, localization, and recomposition. Proof-of-principle case studies
from image recognition and language modeling align these theoretical approaches
with the latest research from AI labs like OpenAI and Anthropic. This research
suggests that a systematic approach to studying model organization can reveal
elements that simpler (or ''more modest'') explainability techniques might
miss, fostering more thoroughly explainable AI. The paper concludes with a
discussion on the epistemic relevance of the mechanistic approach positioned in
the context of selected philosophical debates on XAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming in M\"uller, V. C., Dewey, A. R., Dung, L., & L\"ohr, G.
  (Eds.), Philosophy of Artificial Intelligence: The State of the Art, Synthese
  Library, Berlin: Springer Nature. Please cite the published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent Icing Detection Model of Wind Turbine Blades Based on SCADA
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.07914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.07914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqian Jiang, Junyang Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diagnosis of ice accretion on wind turbine blades is all the time a hard nut
to crack in condition monitoring of wind farms. Existing methods focus on
mechanism analysis of icing process, deviation degree analysis of feature
engineering. However, there have not been deep researches of neural networks
applied in this field at present. Supervisory control and data acquisition
(SCADA) makes it possible to train networks through continuously providing not
only operation parameters and performance parameters of wind turbines but also
environmental parameters and operation modes. This paper explores the
possibility that using convolutional neural networks (CNNs), generative
adversarial networks (GANs) and domain adaption learning to establish
intelligent diagnosis frameworks under different training scenarios.
Specifically, PGANC and PGANT are proposed for sufficient and non-sufficient
target wind turbine labeled data, respectively. The basic idea is that we
consider a two-stage training with parallel GANs, which are aimed at capturing
intrinsic features for normal and icing samples, followed by classification CNN
or domain adaption module in various training cases. Model validation on three
wind turbine SCADA data shows that two-stage training can effectively improve
the model performance. Besides, if there is no sufficient labeled data for a
target turbine, which is an extremely common phenomenon in real industrial
practices, the addition of domain adaption learning makes the trained model
show better performance. Overall, our proposed intelligent diagnosis frameworks
can achieve more accurate detection on the same wind turbine and more
generalized capability on a new wind turbine, compared with other machine
learning models and conventional CNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-hop Upstream Anticipatory Traffic Signal Control with Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaocan Li, Xiaoyu Wang, Ilia Smirnov, Scott Sanner, Baher Abdulhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordination in traffic signal control is crucial for managing congestion in
urban networks. Existing pressure-based control methods focus only on immediate
upstream links, leading to suboptimal green time allocation and increased
network delays. However, effective signal control inherently requires
coordination across a broader spatial scope, as the effect of upstream traffic
should influence signal control decisions at downstream intersections,
impacting a large area in the traffic network. Although agent communication
using neural network-based feature extraction can implicitly enhance spatial
awareness, it significantly increases the learning complexity, adding an
additional layer of difficulty to the challenging task of control in deep
reinforcement learning. To address the issue of learning complexity and myopic
traffic pressure definition, our work introduces a novel concept based on
Markov chain theory, namely \textit{multi-hop upstream pressure}, which
generalizes the conventional pressure to account for traffic conditions beyond
the immediate upstream links. This farsighted and compact metric informs the
deep reinforcement learning agent to preemptively clear the multi-hop upstream
queues, guiding the agent to optimize signal timings with a broader spatial
awareness. Simulations on synthetic and realistic (Toronto) scenarios
demonstrate controllers utilizing multi-hop upstream pressure significantly
reduce overall network delay by prioritizing traffic movements based on a
broader understanding of upstream congestion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 tables, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Alignment Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satchel Grant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When can we say that two neural systems are the same? The answer to this
question is goal-dependent, and it is often addressed through correlative
methods such as Representational Similarity Analysis (RSA) and Centered Kernel
Alignment (CKA). What do we miss when we forgo causal explorations, and how can
we target specific types of similarity? In this work, we introduce Model
Alignment Search (MAS), a method for causally exploring distributed
representational similarity. The method learns invertible linear
transformations that align a subspace between two distributed networks'
representations where causal information can be freely interchanged. We first
show that the method can be used to transfer specific causal variables, such as
the number of items in a counting task, between networks with different
training seeds. We then explore open questions in number cognition by comparing
different types of numeric representations in models trained on structurally
different numeric tasks. We then explore differences between MAS vs preexisting
causal similarity methods, and lastly, we introduce a counterfactual latent
auxiliary loss function that helps shape causally relevant alignments even in
cases where we do not have causal access to one of the two models for training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgRegNet: A Deep Regression Network for Flower and Fruit Density
  Estimation, Localization, and Counting in Orchards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uddhav Bhattarai, Santosh Bhusal, Qin Zhang, Manoj Karkee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the major challenges for the agricultural industry today is the
uncertainty in manual labor availability and the associated cost. Automated
flower and fruit density estimation, localization, and counting could help
streamline harvesting, yield estimation, and crop-load management strategies
such as flower and fruitlet thinning. This article proposes a deep
regression-based network, AgRegNet, to estimate density, count, and location of
flower and fruit in tree fruit canopies without explicit object detection or
polygon annotation. Inspired by popular U-Net architecture, AgRegNet is a
U-shaped network with an encoder-to-decoder skip connection and modified
ConvNeXt-T as an encoder feature extractor. AgRegNet can be trained based on
information from point annotation and leverages segmentation information and
attention modules (spatial and channel) to highlight relevant flower and fruit
features while suppressing non-relevant background features. Experimental
evaluation in apple flower and fruit canopy images under an unstructured
orchard environment showed that AgRegNet achieved promising accuracy as
measured by Structural Similarity Index (SSIM), percentage Mean Absolute Error
(pMAE) and mean Average Precision (mAP) to estimate flower and fruit density,
count, and centroid location, respectively. Specifically, the SSIM, pMAE, and
mAP values for flower images were 0.938, 13.7%, and 0.81, respectively. For
fruit images, the corresponding values were 0.910, 5.6%, and 0.93. Since the
proposed approach relies on information from point annotation, it is suitable
for sparsely and densely located objects. This simplified technique will be
highly applicable for growers to accurately estimate yields and decide on
optimal chemical and mechanical flower thinning practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Computers and Electronics in Agriculture</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meaning-Typed Programming: Language-level Abstractions and Runtime for
  GenAI Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08965v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08965v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Mars, Yiping Kang, Jayanaka L. Dantanarayana, Kugesan Sivasothynathan, Christopher Clarke, Baichuan Li, Krisztian Flautner, Lingjia Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software is rapidly evolving from being programmed with traditional logical
code, to neuro-integrated applications that leverage generative AI and large
language models (LLMs) for application functionality. This shift increases the
complexity of building applications, as developers now must reasoning about,
program, and prompt LLMs. Despite efforts to create tools to assist with prompt
engineering, these solutions often introduce additional layers of complexity to
the development of neuro-integrated applications. This paper proposes
meaning-typed programming (MTP), a novel approach to simplify the creation of
neuro-integrated applications by introducing new language-level abstractions
that hide the complexities of LLM integration. Our key insight is that typical
conventional code already possesses a high level of semantic richness that can
be automatically reasoned about, as it is designed to be readable and
maintainable by humans. Leveraging this insight, we conceptualize LLMs as
meaning-typed code constructs and introduce a by abstraction at the language
level, MT-IR, a new meaning-based intermediate representation at the compiler
level, and MT Runtime, an automated run-time engine for LLM integration and
operations. We implement MTP in a production-grade Python super-set language
called Jac and perform an extensive evaluation. Our results demonstrate that
MTP not only simplifies the development process but also meets or exceeds the
efficacy of state-of-the-art manual and tool-assisted prompt engineering
techniques in terms of accuracy and usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cold-Start Recommendation towards the Era of <span class="highlight-title">Large Language Model</span>s
  (LLMs): A Comprehensive <span class="highlight-title">Survey</span> and Roadmap 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Zhang, Yuanchen Bei, Liangwei Yang, Henry Peng Zou, Peilin Zhou, Aiwei Liu, Yinghui Li, Hao Chen, Jianling Wang, Yu Wang, Feiran Huang, Sheng Zhou, Jiajun Bu, Allen Lin, James Caverlee, Fakhri Karray, Irwin King, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cold-start problem is one of the long-standing challenges in recommender
systems, focusing on accurately modeling new or interaction-limited users or
items to provide better recommendations. Due to the diversification of internet
platforms and the exponential growth of users and items, the importance of
cold-start recommendation (CSR) is becoming increasingly evident. At the same
time, large language models (LLMs) have achieved tremendous success and possess
strong capabilities in modeling user and item information, providing new
potential for cold-start recommendations. However, the research community on
CSR still lacks a comprehensive review and reflection in this field. Based on
this, in this paper, we stand in the context of the era of large language
models and provide a comprehensive review and discussion on the roadmap,
related literature, and future directions of CSR. Specifically, we have
conducted an exploration of the development path of how existing CSR utilizes
information, from content features, graph relations, and domain information, to
the world knowledge possessed by large language models, aiming to provide new
insights for both the research and industrial communities on CSR. Related
resources of cold-start recommendations are collected and continuously updated
for the community in
https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Machine Learning to Discover Parsimonious and
  Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff
  Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan-Heng Wang, Hoshin V. Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the excellent real-world predictive performance of modern machine
learning (ML) methods, many scientists remain hesitant to discard traditional
physical-conceptual (PC) approaches due mainly to their relative
interpretability, which contributes to credibility during decision-making. In
this context, a currently underexplored aspect of ML is how to develop
minimally-optimal representations that can facilitate better insight regarding
system functioning. Regardless of how this is achieved, it is arguably true
that parsimonious representations better support the advancement of scientific
understanding. Our own view is that ML-based modeling of geoscientific systems
should be based in the use of computational units that are fundamentally
interpretable by design.
  This paper continues our exploration of how the strengths of ML can be
exploited in the service of better understanding via scientific investigation.
Here, we use the Mass Conserving Perceptron (MCP) as the fundamental
computational unit in a generic network architecture consisting of nodes
arranged in series and parallel to explore several generic and important issues
related to the use of observational data for constructing input-state-output
models of dynamical systems. In the context of lumped catchment modeling, we
show that physical interpretability and excellent predictive performance can
both be achieved using a relatively parsimonious distributed-state
multiple-flow-path network with context-dependent gating and information
sharing across the nodes, suggesting that MCP-based modeling can play a
significant role in application of ML to geoscientific investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>74 Pages, 4 Tables, 13 Figures, 11 Tables and 11 Figures in
  Supplementary Materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frechet Music Distance: A Metric For Generative Symbolic Music
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Retkowski, Jakub Stępniak, Mateusz Modrzejewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce the Frechet Music Distance (FMD), a novel
evaluation metric for generative symbolic music models, inspired by the Frechet
Inception Distance (FID) in computer vision and Frechet Audio Distance (FAD) in
generative audio. FMD calculates the distance between distributions of
reference and generated symbolic music embeddings, capturing abstract musical
features. We validate FMD across several datasets and models. Results indicate
that FMD effectively differentiates model quality, providing a domain-specific
metric for evaluating symbolic music generation, and establishing a
reproducible standard for future research in symbolic music modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04202v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04202v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Growing concerns about safety and alignment of AI systems highlight the
importance of embedding moral capabilities in artificial agents: a promising
solution is the use of learning from experience, i.e., Reinforcement Learning.
In multi-agent (social) environments, complex population-level phenomena may
emerge from interactions between individual learning agents. Many of the
existing studies rely on simulated social dilemma environments to study the
interactions of independent learning agents; however, they tend to ignore the
moral heterogeneity that is likely to be present in societies of agents in
practice. For example, at different points in time a single learning agent may
face opponents who are consequentialist (i.e., focused on maximizing outcomes
over time), norm-based (i.e., conforming to specific norms), or virtue-based
(i.e., considering a combination of different virtues). The extent to which
agents' co-development may be impacted by such moral heterogeneity in
populations is not well understood. In this paper, we present a study of the
learning dynamics of morally heterogeneous populations interacting in a social
dilemma setting. Using an Iterated Prisoner's Dilemma environment with a
partner selection mechanism, we investigate the extent to which the prevalence
of diverse moral agents in populations affects individual agents' learning
behaviors and emergent population-level outcomes. We observe several types of
non-trivial interactions between pro-social and anti-social agents, and find
that certain types of moral agents are able to steer selfish agents towards
more cooperative behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and
  Society - San Jose, CA, USA) - see
  https://ojs.aaai.org/index.php/AIES/article/view/31736</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convex Markov Games: A Framework for Creativity, Imitation, Fairness,
  and Safety in Multiagent Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Gemp, Andreas Haupt, Luke Marris, Siqi Liu, Georgios Piliouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Behavioral diversity, expert imitation, fairness, safety goals and others
give rise to preferences in sequential decision making domains that do not
decompose additively across time. We introduce the class of convex Markov games
that allow general convex preferences over occupancy measures. Despite infinite
time horizon and strictly higher generality than Markov games, pure strategy
Nash equilibria exist. Furthermore, equilibria can be approximated empirically
by performing gradient descent on an upper bound of exploitability. Our
experiments reveal novel solutions to classic repeated normal-form games, find
fair solutions in a repeated asymmetric coordination game, and prioritize safe
long-term behavior in a robot warehouse environment. In the prisoner's dilemma,
our algorithm leverages transient imitation to find a policy profile that
deviates from observed human play only slightly, yet achieves higher per-player
utility while also being three orders of magnitude less exploitable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systems Thinking Approach to Algorithmic Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16641v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16641v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systems thinking provides us with a way to model the algorithmic fairness
problem by allowing us to encode prior knowledge and assumptions about where we
believe bias might exist in the data generating process. We can then encode
these beliefs as a series of causal graphs, enabling us to link AI/ML systems
to politics and the law. This allows us to combine techniques from machine
learning, causal inference, and system dynamics in order to capture different
emergent aspects of the fairness problem. We can use systems thinking to help
policymakers on both sides of the political aisle to understand the complex
trade-offs that exist from different types of fairness policies, providing a
sociotechnical foundation for designing AI policy that is aligned to their
political agendas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to the 2025 ACM FAccT conference for
  review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing Refactoring Engine via Historical Bug Report driven LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Wang, Zhuolin Xu, Shin Hwei Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Refactoring is the process of restructuring existing code without changing
its external behavior while improving its internal structure. Refactoring
engines are integral components of modern Integrated Development Environments
(IDEs) and can automate or semi-automate this process to enhance code
readability, reduce complexity, and improve the maintainability of software
products. Similar to traditional software systems such as compilers,
refactoring engines may also contain bugs that can lead to unexpected
behaviors. In this paper, we propose a novel approach called RETESTER, a
LLM-based framework for automated refactoring engine testing. Specifically, by
using input program structure templates extracted from historical bug reports
and input program characteristics that are error-prone, we design
chain-of-thought (CoT) prompts to perform refactoring-preserving
transformations. The generated variants are then tested on the latest version
of refactoring engines using differential testing. We evaluate RETESTER on two
most popular modern refactoring engines (i.e., ECLIPSE, and INTELLIJ IDEA). It
successfully revealed 18 new bugs in the latest version of those refactoring
engines. By the time we submit our paper, seven of them were confirmed by their
developers, and three were fixed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatically Detecting Heterogeneous Bugs in High-Performance Computing
  Scientific Software 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Davis, Aakash Kulkarni, Ziyan Chen, Yunhan Qiao, Christopher Terrazas, Manish Motwani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific advancements rely on high-performance computing (HPC) applications
that model real-world phenomena through simulations. These applications process
vast amounts of data on specialized accelerators (eg., GPUs) using special
libraries. Heterogeneous bugs occur in these applications when managing data
movement across different platforms, such as CPUs and GPUs, leading to
divergent behavior when using heterogeneous platforms compared to using only
CPUs. Existing software testing techniques often fail to detect such bugs
because either they do not account for platform-specific characteristics or
target specific platforms. To address this problem, we present HeteroBugDetect,
an automated approach to detect platform-dependent heterogeneous bugs in HPC
scientific applications. HeteroBugDetect combines natural-language processing,
off-target testing, custom fuzzing, and differential testing to provide an
end-to-end solution for detecting platform-specific bugs in scientific
applications. We evaluate HeteroBugDetect on LAMMPS, a molecular dynamics
simulator, where it detected multiple heterogeneous bugs, enhancing its
reliability across diverse HPC environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-grained Testing for Autonomous Driving Software: a Study on
  Autoware with LLM-driven Unit Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Wang, Xuan Xie, Yuheng Huang, Renzhi Wang, An Ran Chen, Lei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Testing autonomous driving systems (ADS) is critical to ensuring their
reliability and safety. Existing ADS testing works focuses on designing
scenarios to evaluate system-level behaviors, while fine-grained testing of ADS
source code has received comparatively little attention. To address this gap,
we present the first study on testing, specifically unit testing, for ADS
source code. Our study focuses on an industrial ADS framework, Autoware. We
analyze both human-written test cases and those generated by large language
models (LLMs). Our findings reveal that human-written test cases in Autoware
exhibit limited test coverage, and significant challenges remain in applying
LLM-generated tests for Autoware unit testing. To overcome these challenges, we
propose AwTest-LLM, a novel approach to enhance test coverage and improve test
case pass rates across Autoware packages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimization is Better than Generation: Optimizing Commit Message
  Leveraging Human-written Commit Message 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Li, David Faragó, Christian Petrov, Iftekhar Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commit messages are crucial in software development, supporting maintenance
tasks and communication among developers. While Large Language Models (LLMs)
have advanced Commit Message Generation (CMG) using various software contexts,
some contexts developers consider are often missed by CMG techniques and can't
be easily retrieved or even retrieved at all by automated tools. To address
this, we propose Commit Message Optimization (CMO), which enhances
human-written messages by leveraging LLMs and search-based optimization. CMO
starts with human-written messages and iteratively improves them by integrating
key contexts and feedback from external evaluators. Our extensive evaluation
shows CMO generates commit messages that are significantly more Rational,
Comprehensive, and Expressive while outperforming state-of-the-art CMG methods
and human messages 88.2%-95.4% of the time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuFF: Stable and Sensitive Post-training MutationTesting for Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhan Kim, Nargiz Humbatova, Gunel Jahangirova, Shin Yoo, Paolo Tonella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid adoptions of Deep Learning (DL) in a broad range of fields led to the
development of specialised testing techniques for DL systems, including DL
mutation testing. However, existing post-training DL mutation techniques often
generate unstable mutants across multiple training repetitions and multiple
applications of the same mutation operator. Additionally, while extremely
efficient, they generate mutants without taking into account the mutants'
sensitivity and killability, resulting in a large number of ineffective mutants
compared to pre-training mutants. In this paper, we present a new efficient
post-training DL mutation technique, named MuFF, designed to ensure the
stability of the mutants and capable of generating killable and sensitive
mutants. MuFF implements an automated stability check and introduces two
mutation operators, named weight and neuron inhibitors. Our extensive empirical
experiments show that MuFF generates mutants with 60%pt and 25%pt higher
sensitivity compared to DeepMutation++ and DeepCrime, respectively, while also
producing mutants that are more stable than those of DeepMutation++ and
different from the mutants of DeepCrime. Moreover, MuFF preserves the benefits
of the post-training mutation technique, being 61 times faster than DeepCrime
in generating mutants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suggesting <span class="highlight-title">Code</span> Edits in Interactive Machine Learning Notebooks Using
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bihui Jin, Jiayue Wang, Pengyu Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning developers frequently use interactive computational
notebooks, such as Jupyter notebooks, to host code for data processing and
model training. Jupyter notebooks provide a convenient tool for writing machine
learning pipelines and interactively observing outputs, however, maintaining
Jupyter notebooks, e.g., to add new features or fix bugs, can be challenging
due to the length and complexity of the notebooks. Moreover, there is no
existing benchmark related to developer edits on Jupyter notebooks. To address
this, we present the first dataset of 48,398 Jupyter notebook edits derived
from 20,095 revisions of 792 machine learning repositories on GitHub, and
perform the first study of the using LLMs to predict code edits in Jupyter
notebooks. Our dataset captures granular details of cell-level and line-level
modifications, offering a foundation for understanding real-world maintenance
patterns in machine learning workflows. We observed that the edits on Jupyter
notebooks are highly localized, with changes averaging only 166 lines of code
in repositories. While larger models outperform smaller counterparts in code
editing, all models have low accuracy on our dataset even after finetuning,
demonstrating the complexity of real-world machine learning maintenance tasks.
Our findings emphasize the critical role of contextual information in improving
model performance and point toward promising avenues for advancing large
language models' capabilities in engineering machine learning code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulated Interactive Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannic Noller, Erick Chandra, Srinidhi HC, Kenny Choo, Cyrille Jegourel, Oka Kurniawan, Christopher M. Poskitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Debugging software, i.e., the localization of faults and their repair, is a
main activity in software engineering. Therefore, effective and efficient
debugging is one of the core skills a software engineer must develop. However,
the teaching of debugging techniques is usually very limited or only taught in
indirect ways, e.g., during software projects. As a result, most Computer
Science (CS) students learn debugging only in an ad-hoc and unstructured way.
In this work, we present our approach called Simulated Interactive Debugging
that interactively guides students along the debugging process. The guidance
aims to empower the students to repair their solutions and have a proper
"learning" experience. We envision that such guided debugging techniques can be
integrated into programming courses early in the CS education curriculum. To
perform an initial evaluation, we developed a prototypical implementation using
traditional fault localization techniques and large language models. Students
can use features like the automated setting of breakpoints or an interactive
chatbot. We designed and executed a controlled experiment that included this
IDE-integrated tooling with eight undergraduate CS students. Based on the
responses, we conclude that the participants liked the systematic guidance by
the assisted debugger. In particular, they rated the automated setting of
breakpoints as the most effective, followed by the interactive debugging and
chatting, and the explanations for how breakpoints were set. In our future
work, we will improve our concept and implementation, add new features, and
perform more intensive user studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Energy Consumption of Test Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fitsum Kifetew, Davide Prandi, Angelo Susi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in the area of automated test generation has seen remarkable
progress in recent years, resulting in several approaches and tools for
effective and efficient generation of test cases. In particular, the EvoSuite
tool has been at the forefront of this progress embodying various algorithms
for automated test generation of Java programs. EvoSuite has been used to
generate test cases for a wide variety of programs as well. While there are a
number of empirical studies that report results on the effectiveness, in terms
of code coverage and other related metrics, of the various test generation
strategies and algorithms implemented in EvoSuite, there are no studies, to the
best of our knowledge, on the energy consumption associated to the automated
test generation. In this paper, we set out to investigate this aspect by
measuring the energy consumed by EvoSuite when generating tests. We also
measure the energy consumed in the execution of the test cases generated,
comparing them with those manually written by developers. The results show that
the different test generation algorithms consumed different amounts of energy,
in particular on classes with high cyclomatic complexity. Furthermore, we also
observe that manual tests tend to consume more energy as compared to
automatically generated tests, without necessarily achieving higher code
coverage. Our results also give insight into the methods that consume
significantly higher levels of energy, indicating potential points of
improvement both for EvoSuite as well as the different programs under test.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at 2025 IEEE Conference on Software Testing,
  Verification and Validation (ICST)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Debugging of Auto-Translated <span class="highlight-title">Code</span> Using Differential Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengnan Wu, Xinyu Sun, Xin Wang, Yangfan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) hold great promise in the task of code
translation. However, the lack of explainability complicates the identification
of the inevitable translation errors. In this paper, we propose tHinter, a
debugging tool to locate translation errors in auto-translated code. The core
idea of tHinter is that correctly translated, the source and translated code
should present the same functionalities, giving the same output for the same
input. Hence, lines in the translated code responsible for output differences
are possibly translation errors. First, tHinter employs fuzzing to generate
diverse test cases that thoroughly explore the translated code. Then, tHinter
relies on a heuristic algorithm to pinpoint translation errors from coverage
information and differential testing execution results of those test cases.
This heuristic algorithm is designed to leverage both the statistics and the
expertise of developers. Comprehensive experiments with real code show its
effectiveness. It reduces 71% lines developers need to review during debugging
and increases the likelihood of the LLM fixing translation errors in a single
query by 59%. Developers generally consider it satisfactory and helpful.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Language for Scenario Development of Autonomous Driving Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toshiaki Aoki, Takashi Tomita, Tatsuji Kawai, Daisuke Kawakami, Nobuo Chida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving systems are typically verified based on scenarios. To
represent the positions and movements of cars in these scenarios, diagrams that
utilize icons are typically employed. However, the interpretation of such
diagrams is typically ambiguous, which can lead to misunderstandings among
users, making them unsuitable for the development of high-reliability systems.
To address this issue, this study introduces a notation called the car position
diagram (CPD). The CPD allows for the concise representation of numerous
scenarios and is particularly suitable for scenario analysis and design. In
addition, we propose a method for converting CPD-based models into
propositional logic formulas and enumerating all scenarios using a SAT solver.
A tool for scenario enumeration is implemented, and experiments are conducted
on both typical car behaviors and international standards. The results
demonstrate that the CPD enables the concise description of numerous scenarios,
thereby confirming the effectiveness of our scenario analysis method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study of In-Context-Learning-Based Text-to-SQL Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Shen, Chengcheng Wan, Ruoyi Qiao, Jiazhen Zou, Hang Xu, Yuchen Shao, Yueling Zhang, Weikai Miao, Geguang Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been adopted to perform text-to-SQL tasks,
utilizing their in-context learning (ICL) capability to translate natural
language questions into structured query language (SQL). However, such a
technique faces correctness problems and requires efficient repairing
solutions. In this paper, we conduct the first comprehensive study of
text-to-SQL errors. Our study covers four representative ICL-based techniques,
five basic repairing methods, two benchmarks, and two LLM settings. We find
that text-to-SQL errors are widespread and summarize 29 error types of 7
categories. We also find that existing repairing attempts have limited
correctness improvement at the cost of high computational overhead with many
mis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL
error detection and repairing framework. The evaluation demonstrates that
MapleRepair outperforms existing solutions by repairing 13.8% more queries with
neglectable mis-repairs and 67.4% less overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Capabilities of Vision-Language Models to Detect Visual
  Bugs in HTML5 <canvas> Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Finlay Macklon, Cor-Paul Bezemer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The HyperText Markup Language 5 (HTML5) <canvas> is useful for creating
visual-centric web applications. However, unlike traditional web applications,
HTML5 <canvas> applications render objects onto the <canvas> bitmap without
representing them in the Document Object Model (DOM). Mismatches between the
expected and actual visual output of the <canvas> bitmap are termed visual
bugs. Due to the visual-centric nature of <canvas> applications, visual bugs
are important to detect because such bugs can render a <canvas> application
useless. As we showed in prior work, Asset-Based graphics can provide the
ground truth for a visual test oracle. However, many <canvas> applications
procedurally generate their graphics. In this paper, we investigate how to
detect visual bugs in <canvas> applications that use Procedural graphics as
well. In particular, we explore the potential of Vision-Language Models (VLMs)
to automatically detect visual bugs. Instead of defining an exact visual test
oracle, information about the application's expected functionality (the
context) can be provided with the screenshot as input to the VLM. To evaluate
this approach, we constructed a dataset containing 80 bug-injected screenshots
across four visual bug types (Layout, Rendering, Appearance, and State) plus 20
bug-free screenshots from 20 <canvas> applications. We ran experiments with a
state-of-the-art VLM using several combinations of text and image context to
describe each application's expected functionality. Our results show that by
providing the application README(s), a description of visual bug types, and a
bug-free screenshot as context, VLMs can be leveraged to detect visual bugs
with up to 100% per-application accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Empirical Software Engineering (EMSE) journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Random Testing with Qgrams: The Illusion Comes True 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Biagiola, Robert Feldt, Paolo Tonella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive Random Testing (ART) has faced criticism, particularly for its
computational inefficiency, as highlighted by Arcuri and Briand. Their analysis
clarified how ART requires a quadratic number of distance computations as the
number of test executions increases, which limits its scalability in scenarios
requiring extensive testing to uncover faults. Simulation results support this,
showing that the computational overhead of these distance calculations often
outweighs ART's benefits. While various ART variants have attempted to reduce
these costs, they frequently do so at the expense of fault detection, lack
complexity guarantees, or are restricted to specific input types, such as
numerical or discrete data.
  In this paper, we introduce a novel framework for adaptive random testing
that replaces pairwise distance computations with a compact aggregation of past
executions, such as counting the Qgrams observed in previous runs. Test case
selection then leverages this aggregated data to measure diversity (e.g.,
entropy of Qgrams), allowing us to reduce the computational complexity from
quadratic to linear.
  Experiments with a benchmark of six web applications, show that ART with
Qgrams covers, on average, 4x more unique targets than random testing, and 3.5x
more than ART using traditional distance-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the ACM International Conference on the Foundations of
  Software Engineering (FSE) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> aiX<span class="highlight-title">code</span>r-7B: A Lightweight and Effective <span class="highlight-title">Large Language Model</span> for <span class="highlight-title">Code</span>
  Processing <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13187v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13187v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Jiang, Jia Li, He Zong, Huanyu Liu, Hao Zhu, Shukai Hu, Erlu Li, Jiazheng Ding, Yu Han, Wei Ning, Gen Wang, Yihong Dong, Kechi Zhang, <span class="highlight-author">Ge Li</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been widely used in code completion, and
researchers are focusing on scaling up LLMs to improve their accuracy. However,
larger LLMs have lower inference efficiency, affecting developers' experience
and productivity. In this paper, we propose a lightweight and effective LLM for
code completion named aiXcoder-7B. Compared to existing LLMs, aiXcoder-7B
achieves higher code completion accuracy while having smaller scales (i.e., 7
billion parameters). We attribute the superiority of aiXcoder-7B to three key
factors: (1) Multi-objective training. We employ three training objectives, one
of which is our proposed Structured Fill-In-the-Middle (SFIM). SFIM considers
the syntax structures in code and effectively improves the performance of LLMs
for code. (2) Diverse data sampling strategies. They consider inter-file
relationships and enhance the capability of LLMs in understanding cross-file
contexts. (3) Extensive high-quality data. We establish a rigorous data
collection pipeline and consume a total of 1.2 trillion unique tokens for
training aiXcoder-7B. This vast volume of data enables aiXcoder-7B to learn a
broad distribution of code. We evaluate aiXcoder-7B in five popular code
completion benchmarks and a new benchmark collected by this paper. The results
show that aiXcoder-7B outperforms the latest six LLMs with similar sizes and
even surpasses four larger LLMs (e.g., StarCoder2-15B and CodeLlama-34B),
positioning aiXcoder-7B as a lightweight and effective LLM for academia and
industry. Finally, we summarize three valuable insights for helping
practitioners train the next generations of LLMs for code. aiXcoder-7B has been
open-souced and gained significant attention. Until January 2025, aiXcoder-7B
has received 2,226 GitHub Stars.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>(1) Accepted by the 47th International Conference on Software
  Engineering (ICSE 2025). (2) aiXcoder-7B is available at
  https://github.com/aixcoder-plugin/aiXcoder-7B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finding Safety Violations of AI-Enabled Control Systems through the Lens
  of Synthesized Proxy Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieke Shi, Zhou Yang, Junda He, Bowen Xu, Dongsun Kim, DongGyun Han, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the increasing adoption of modern AI-enabled control systems, ensuring
their safety and reliability has become a critical task in software testing.
One prevalent approach to testing control systems is falsification, which aims
to find an input signal that causes the control system to violate a formal
safety specification using optimization algorithms. However, applying
falsification to AI-enabled control systems poses two significant challenges:
(1)~it requires the system to execute numerous candidate test inputs, which can
be time-consuming, particularly for systems with AI models that have many
parameters, and (2)~multiple safety requirements are typically defined as a
conjunctive specification, which is difficult for existing falsification
approaches to comprehensively cover.
  This paper introduces Synthify, a falsification framework tailored for
AI-enabled control systems. Our approach performs falsification in a two-phase
process. At the start, Synthify synthesizes a program that implements one or a
few linear controllers to serve as a proxy for the AI controller. This proxy
program mimics the AI controller's functionality but is computationally more
efficient. Then, Synthify employs the $\epsilon$-greedy strategy to sample a
promising sub-specification from the conjunctive safety specification. It then
uses a Simulated Annealing-based falsification algorithm to find violations of
the sampled sub-specification for the control system. To evaluate Synthify, we
compare it to PSY-TaLiRo, a state-of-the-art and industrial-strength
falsification tool, on 8 publicly available control systems. On average,
Synthify achieves a 83.5% higher success rate in falsification compared to
PSY-TaLiRo with the same budget of falsification trials. The safety violations
found by Synthify are also more diverse than those found by PSY-TaLiRo,
covering 137.7% more sub-specifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Transactions on Software Engineering and Methodology
  (TOSEM), 35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Types: Exploring the Impact of Type Checking on Neural Bug
  Detection in Dynamically Typed Languages <span class="chip">ICSE'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boqi Chen, José Antonio Hernández López, Gunter Mussbacher, Dániel Varró
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivation: Automated bug detection in dynamically typed languages such as
Python is essential for maintaining code quality. The lack of mandatory type
annotations in such languages can lead to errors that are challenging to
identify early with traditional static analysis tools. Recent progress in deep
neural networks has led to increased use of neural bug detectors. In statically
typed languages, a type checker is integrated into the compiler and thus taken
into consideration when the neural bug detector is designed for these
languages.
  Problem: However, prior studies overlook this aspect during the training and
testing of neural bug detectors for dynamically typed languages. When an
optional type checker is used, assessing existing neural bug detectors on bugs
easily detectable by type checkers may impact their performance estimation.
Moreover, including these bugs in the training set of neural bug detectors can
shift their detection focus toward the wrong type of bugs.
  Contribution: We explore the impact of type checking on various neural bug
detectors for variable misuse bugs, a common type targeted by neural bug
detectors. Existing synthetic and real-world datasets are type-checked to
evaluate the prevalence of type-related bugs. Then, we investigate how
type-related bugs influence the training and testing of the neural bug
detectors.
  Findings: Our findings indicate that existing bug detection datasets contain
a significant proportion of type-related bugs. Building on this insight, we
discover integrating the neural bug detector with a type checker can be
beneficial, especially when the code is annotated with types. Further
investigation reveals neural bug detectors perform better on type-related bugs
than other bugs. Moreover, removing type-related bugs from the training data
helps improve neural bug detectors' ability to identify bugs beyond the scope
of type checkers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICSE'25 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CleanVul: Automatic Function-Level Vulnerability Detection in <span class="highlight-title">Code</span>
  Commits Using LLM Heuristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17274v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17274v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikun Li, Ting Zhang, Ratnadira Widyasari, Yan Naing Tun, Huu Hung Nguyen, Tan Bui, Ivana Clairine Irsan, Yiran Cheng, Xiang Lan, Han Wei Ang, Frank Liauw, Martin Weyssow, Hong Jin Kang, Eng Lieh Ouh, Lwin Khin Shar, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate identification of software vulnerabilities is crucial for system
integrity. Vulnerability datasets, often derived from the National
Vulnerability Database (NVD) or directly from GitHub, are essential for
training machine learning models to detect these security flaws. However, these
datasets frequently suffer from significant noise, typically 40% to 75%, due
primarily to the automatic and indiscriminate labeling of all changes in
vulnerability-fixing commits (VFCs) as vulnerability-related. This
misclassification occurs because not all changes in a commit aimed at fixing
vulnerabilities pertain to security threats; many are routine updates like bug
fixes or test improvements.
  This paper introduces the first methodology that uses the Large Language
Model (LLM) with a heuristic enhancement to automatically identify
vulnerability-fixing changes from VFCs, achieving an F1-score of 0.82.
VulSifter was applied to a large-scale study, where we conducted a crawl of
127,063 repositories on GitHub, resulting in the acquisition of 5,352,105
commits. VulSifter involves utilizing an LLM to comprehend code semantics and
contextual information, while applying heuristics to filter out unrelated
changes. We then developed CleanVul, a high-quality dataset comprising 11,632
functions using our LLM heuristic enhancement approach, demonstrating
Correctness (90.6%) comparable to established datasets such as SVEN and
PrimeVul.
  To evaluate the CleanVul dataset, we conducted experiments focusing on
fine-tuning various LLMs on CleanVul and other high-quality datasets.
Evaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit
enhanced accuracy but also superior generalization capabilities compared to
those trained on uncleaned datasets. Specifically, models trained on CleanVul
and tested on PrimeVul achieve accuracy higher than those trained and tested
exclusively on PrimeVul.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">104</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Image-Based Narrative Extraction: A Case Study with
  Historical Photographic Records <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fausto German, Brian Keith, Mauricio Matus, Diego Urrutia, Claudio Meneses
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a semi-supervised approach to extracting narratives from
historical photographic records using an adaptation of the narrative maps
algorithm. We extend the original unsupervised text-based method to work with
image data, leveraging deep learning techniques for visual feature extraction
and similarity computation. Our method is applied to the ROGER dataset, a
collection of photographs from the 1928 Sacambaya Expedition in Bolivia
captured by Robert Gerstmann. We compare our algorithmically extracted visual
narratives with expert-curated timelines of varying lengths (5 to 30 images) to
evaluate the effectiveness of our approach. In particular, we use the Dynamic
Time Warping (DTW) algorithm to match the extracted narratives with the
expert-curated baseline. In addition, we asked an expert on the topic to
qualitatively evaluate a representative example of the resulting narratives.
Our findings show that the narrative maps approach generally outperforms random
sampling for longer timelines (10+ images, p < 0.05), with expert evaluation
confirming the historical accuracy and coherence of the extracted narratives.
This research contributes to the field of computational analysis of visual
cultural heritage, offering new tools for historians, archivists, and digital
humanities scholars to explore and understand large-scale image collections.
The method's ability to generate meaningful narratives from visual data opens
up new possibilities for the study and interpretation of historical events
through photographic evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for oral presentation in the findings
  track of the 47th European Conference on Information Retrieval (ECIR 2025).
  Source code and experiments are available at
  https://github.com/faustogerman/ROGER-Concept-Narratives</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASTRA: A Scene-aware <span class="highlight-title">TRAnsformer</span>-based model for trajectory prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Izzeddin Teeti, Aniket Thomas, Munish Monga, Sachin Kumar, Uddeshya Singh, Andrew Bradley, Biplab Banerjee, Fabio Cuzzolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ASTRA (A} Scene-aware TRAnsformer-based model for trajectory
prediction), a light-weight pedestrian trajectory forecasting model that
integrates the scene context, spatial dynamics, social inter-agent interactions
and temporal progressions for precise forecasting. We utilised a U-Net-based
feature extractor, via its latent vector representation, to capture scene
representations and a graph-aware transformer encoder for capturing social
interactions. These components are integrated to learn an agent-scene aware
embedding, enabling the model to learn spatial dynamics and forecast the future
trajectory of pedestrians. The model is designed to produce both deterministic
and stochastic outcomes, with the stochastic predictions being generated by
incorporating a Conditional Variational Auto-Encoder (CVAE). ASTRA also
proposes a simple yet effective weighted penalty loss function, which helps to
yield predictions that outperform a wide array of state-of-the-art
deterministic and generative models. ASTRA demonstrates an average improvement
of 27%/10% in deterministic/stochastic settings on the ETH-UCY dataset, and 26%
improvement on the PIE dataset, respectively, along with seven times fewer
parameters than the existing state-of-the-art model (see Figure 1).
Additionally, the model's versatility allows it to generalize across different
perspectives, such as Bird's Eye View (BEV) and Ego-Vehicle View (EVV).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of Vascular Leukoencephalopathy in CT Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Z. Cernekova, V. Sisik, F. Jafari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has seen a significant surge in popularity,
particularly in its application to medicine. This study explores AI's role in
diagnosing leukoencephalopathy, a small vessel disease of the brain, and a
leading cause of vascular dementia and hemorrhagic strokes. We utilized a
dataset of approximately 1200 patients with axial brain CT scans to train
convolutional neural networks (CNNs) for binary disease classification.
Addressing the challenge of varying scan dimensions due to different patient
physiologies, we processed the data to a uniform size and applied three
preprocessing methods to improve model accuracy. We compared four neural
network architectures: ResNet50, ResNet50 3D, ConvNext, and Densenet. The
ConvNext model achieved the highest accuracy of 98.5% without any
preprocessing, outperforming models with 3D convolutions. To gain insights into
model decision-making, we implemented Grad-CAM heatmaps, which highlighted the
focus areas of the models on the scans. Our results demonstrate that AI,
particularly the ConvNext architecture, can significantly enhance diagnostic
accuracy for leukoencephalopathy. This study underscores AI's potential in
advancing diagnostic methodologies for brain diseases and highlights the
effectiveness of CNNs in medical imaging applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrossModalityDiffusion: Multi-Modal Novel View Synthesis with Unified
  Intermediate Representation <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Berian, Daniel Brignac, JhihYang Wu, Natnael Daba, Abhijit Mahalanobis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geospatial imaging leverages data from diverse sensing modalities-such as EO,
SAR, and LiDAR, ranging from ground-level drones to satellite views. These
heterogeneous inputs offer significant opportunities for scene understanding
but present challenges in interpreting geometry accurately, particularly in the
absence of precise ground truth data. To address this, we propose
CrossModalityDiffusion, a modular framework designed to generate images across
different modalities and viewpoints without prior knowledge of scene geometry.
CrossModalityDiffusion employs modality-specific encoders that take multiple
input images and produce geometry-aware feature volumes that encode scene
structure relative to their input camera positions. The space where the feature
volumes are placed acts as a common ground for unifying input modalities. These
feature volumes are overlapped and rendered into feature images from novel
perspectives using volumetric rendering techniques. The rendered feature images
are used as conditioning inputs for a modality-specific diffusion model,
enabling the synthesis of novel images for the desired output modality. In this
paper, we show that jointly training different modules ensures consistent
geometric understanding across all modalities within the framework. We validate
CrossModalityDiffusion's capabilities on the synthetic ShapeNet cars dataset,
demonstrating its effectiveness in generating accurate and consistent novel
views across multiple imaging modalities and perspectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 2025 WACV workshop GeoCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EraseBench: Understanding The Ripple Effects of Concept Erasure
  Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibtihel Amara, Ahmed Imtiaz Humayun, Ivana Kajic, Zarana Parekh, Natalie Harris, Sarah Young, Chirag Nagpal, Najoung Kim, Junfeng He, Cristina Nader Vasconcelos, Deepak Ramachandran, Goolnoosh Farnadi, Katherine Heller, Mohammad Havaei, Negar Rostamzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept erasure techniques have recently gained significant attention for
their potential to remove unwanted concepts from text-to-image models. While
these methods often demonstrate success in controlled scenarios, their
robustness in real-world applications and readiness for deployment remain
uncertain. In this work, we identify a critical gap in evaluating sanitized
models, particularly in terms of their performance across various concept
dimensions. We systematically investigate the failure modes of current concept
erasure techniques, with a focus on visually similar, binomial, and
semantically related concepts. We propose that these interconnected
relationships give rise to a phenomenon of concept entanglement resulting in
ripple effects and degradation in image quality. To facilitate more
comprehensive evaluation, we introduce EraseBENCH, a multi-dimensional
benchmark designed to assess concept erasure methods with greater depth. Our
dataset includes over 100 diverse concepts and more than 1,000 tailored
prompts, paired with a comprehensive suite of metrics that together offer a
holistic view of erasure efficacy. Our findings reveal that even
state-of-the-art techniques struggle with maintaining quality post-erasure,
indicating that these approaches are not yet ready for real-world deployment.
This highlights the gap in reliability of the concept erasure techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages main; 9 pages supplemental material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIXELS: Progressive Image Xemplar-based Editing with Latent Surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shristi Das Biswas, Matthew Shreve, Xuelu Li, Prateek Singhal, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language-guided diffusion models for image editing are
often bottle-necked by cumbersome prompt engineering to precisely articulate
desired changes. An intuitive alternative calls on guidance from in-the-wild
image exemplars to help users bring their imagined edits to life. Contemporary
exemplar-based editing methods shy away from leveraging the rich latent space
learnt by pre-existing large text-to-image (TTI) models and fall back on
training with curated objective functions to achieve the task. Though somewhat
effective, this demands significant computational resources and lacks
compatibility with diverse base models and arbitrary exemplar count. On further
investigation, we also find that these techniques restrict user control to only
applying uniform global changes over the entire edited region. In this paper,
we introduce a novel framework for progressive exemplar-driven editing with
off-the-shelf diffusion models, dubbed PIXELS, to enable customization by
providing granular control over edits, allowing adjustments at the pixel or
region level. Our method operates solely during inference to facilitate
imitative editing, enabling users to draw inspiration from a dynamic number of
reference images, or multimodal prompts, and progressively incorporate all the
desired changes without retraining or fine-tuning existing TTI models. This
capability of fine-grained control opens up a range of new possibilities,
including selective modification of individual objects and specifying gradual
spatial changes. We demonstrate that PIXELS delivers high-quality edits
efficiently, leading to a notable improvement in quantitative metrics as well
as human evaluation. By making high-quality image editing more accessible,
PIXELS has the potential to enable professional-grade edits to a wider audience
with the ease of using any open-source image generation model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Single-Image-Based Morphing Attack Detection Using Deep
  Representations from Vision <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Zhang, Raghavendra Ramachandra, Kiran Raja, Christoph Busch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face morphing attacks have posed severe threats to Face Recognition Systems
(FRS), which are operated in border control and passport issuance use cases.
Correspondingly, morphing attack detection algorithms (MAD) are needed to
defend against such attacks. MAD approaches must be robust enough to handle
unknown attacks in an open-set scenario where attacks can originate from
various morphing generation algorithms, post-processing and the diversity of
printers/scanners. The problem of generalization is further pronounced when the
detection has to be made on a single suspected image. In this paper, we propose
a generalized single-image-based MAD (S-MAD) algorithm by learning the encoding
from Vision Transformer (ViT) architecture. Compared to CNN-based
architectures, ViT model has the advantage on integrating local and global
information and hence can be suitable to detect the morphing traces widely
distributed among the face region. Extensive experiments are carried out on
face morphing datasets generated using publicly available FRGC face datasets.
Several state-of-the-art (SOTA) MAD algorithms, including representative ones
that have been publicly evaluated, have been selected and benchmarked with our
ViT-based approach. Obtained results demonstrate the improved detection
performance of the proposed S-MAD method on inter-dataset testing (when
different data is used for training and testing) and comparable performance on
intra-dataset testing (when the same data is used for training and testing)
experimental protocol.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lossy Compression with <span class="highlight-title">Pretrain</span>ed Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy Vonderfecht, Feng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We apply the DiffC algorithm (Theis et al. 2022) to Stable Diffusion 1.5,
2.1, XL, and Flux-dev, and demonstrate that these pretrained models are
remarkably capable lossy image compressors. A principled algorithm for lossy
compression using pretrained diffusion models has been understood since at
least Ho et al. 2020, but challenges in reverse-channel coding have prevented
such algorithms from ever being fully implemented. We introduce simple
workarounds that lead to the first complete implementation of DiffC, which is
capable of compressing and decompressing images using Stable Diffusion in under
10 seconds. Despite requiring no additional training, our method is competitive
with other state-of-the-art generative compression methods at low ultra-low
bitrates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Multi-modal <span class="highlight-title">Large Language Model</span>s for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal M. Patel, Fatih Porikli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving demands safe motion planning, especially in critical
"long-tail" scenarios. Recent end-to-end autonomous driving systems leverage
large language models (LLMs) as planners to improve generalizability to rare
events. However, using LLMs at test time introduces high computational costs.
To address this, we propose DiMA, an end-to-end autonomous driving system that
maintains the efficiency of an LLM-free (or vision-based) planner while
leveraging the world knowledge of an LLM. DiMA distills the information from a
multi-modal LLM to a vision-based end-to-end planner through a set of specially
designed surrogate tasks. Under a joint training strategy, a scene encoder
common to both networks produces structured representations that are
semantically grounded as well as aligned to the final planning objective.
Notably, the LLM is optional at inference, enabling robust planning without
compromising on efficiency. Training with DiMA results in a 37% reduction in
the L2 trajectory error and an 80% reduction in the collision rate of the
vision-based planner, as well as a 44% trajectory error reduction in longtail
scenarios. DiMA also achieves state-of-the-art performance on the nuScenes
planning benchmark.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynthLight: Portrait Relighting with Diffusion Model by Learning to
  Re-render Synthetic Faces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumit Chaturvedi, Mengwei Ren, Yannick Hold-Geoffroy, Jingyuan Liu, Julie Dorsey, Zhixin Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SynthLight, a diffusion model for portrait relighting. Our
approach frames image relighting as a re-rendering problem, where pixels are
transformed in response to changes in environmental lighting conditions. Using
a physically-based rendering engine, we synthesize a dataset to simulate this
lighting-conditioned transformation with 3D head assets under varying lighting.
We propose two training and inference strategies to bridge the gap between the
synthetic and real image domains: (1) multi-task training that takes advantage
of real human portraits without lighting labels; (2) an inference time
diffusion sampling procedure based on classifier-free guidance that leverages
the input portrait to better preserve details. Our method generalizes to
diverse real photographs and produces realistic illumination effects, including
specular highlights and cast shadows, while preserving the subject's identity.
Our quantitative experiments on Light Stage data demonstrate results comparable
to state-of-the-art relighting methods. Our qualitative results on in-the-wild
images showcase rich and unprecedented illumination effects. Project Page:
\url{https://vrroom.github.io/synthlight/}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 25 figures, Project Page
  https://vrroom.github.io/synthlight/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMPLest-X: Ultimate Scaling for Expressive Human Pose and Shape
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqi Yin, Zhongang Cai, Ruisi Wang, Ailing Zeng, Chen Wei, Qingping Sun, Haiyi Mei, Yanjun Wang, Hui En Pang, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Atsushi Yamashita, Lei Yang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressive human pose and shape estimation (EHPS) unifies body, hands, and
face motion capture with numerous applications. Despite encouraging progress,
current state-of-the-art methods focus on training innovative architectural
designs on confined datasets. In this work, we investigate the impact of
scaling up EHPS towards a family of generalist foundation models. 1) For data
scaling, we perform a systematic investigation on 40 EHPS datasets,
encompassing a wide range of scenarios that a model trained on any single
dataset cannot handle. More importantly, capitalizing on insights obtained from
the extensive benchmarking process, we optimize our training scheme and select
datasets that lead to a significant leap in EHPS capabilities. Ultimately, we
achieve diminishing returns at 10M training instances from diverse data
sources. 2) For model scaling, we take advantage of vision transformers (up to
ViT-Huge as the backbone) to study the scaling law of model sizes in EHPS. To
exclude the influence of algorithmic design, we base our experiments on two
minimalist architectures: SMPLer-X, which consists of an intermediate step for
hand and face localization, and SMPLest-X, an even simpler version that reduces
the network to its bare essentials and highlights significant advances in the
capture of articulated hands. With big data and the large model, the foundation
models exhibit strong performance across diverse test benchmarks and excellent
transferability to even unseen environments. Moreover, our finetuning strategy
turns the generalist into specialist models, allowing them to achieve further
performance boosts. Notably, our foundation models consistently deliver
state-of-the-art results on seven benchmarks such as AGORA, UBody, EgoBody, and
our proposed SynHand dataset for comprehensive hand evaluation. (Code is
available at: https://github.com/wqyin/SMPLest-X).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extension of SMPLer-X [arXiv:2309.17448]. Homepage:
  https://caizhongang.com/projects/SMPLer-X/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoWorld: Exploring Knowledge Learning from Unlabeled Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwei Ren, Yunchao Wei, Xun Guo, Yao Zhao, Bingyi Kang, Jiashi Feng, Xiaojie Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores whether a deep generative model can learn complex
knowledge solely from visual input, in contrast to the prevalent focus on
text-based models like large language models (LLMs). We develop VideoWorld, an
auto-regressive video generation model trained on unlabeled video data, and
test its knowledge acquisition abilities in video-based Go and robotic control
tasks. Our experiments reveal two key findings: (1) video-only training
provides sufficient information for learning knowledge, including rules,
reasoning and planning capabilities, and (2) the representation of visual
change is crucial for knowledge acquisition. To improve both the efficiency and
efficacy of this process, we introduce the Latent Dynamics Model (LDM) as a key
component of VideoWorld. Remarkably, VideoWorld reaches a 5-dan professional
level in the Video-GoBench with just a 300-million-parameter model, without
relying on search algorithms or reward mechanisms typical in reinforcement
learning. In robotic tasks, VideoWorld effectively learns diverse control
operations and generalizes across environments, approaching the performance of
oracle models in CALVIN and RLBench. This study opens new avenues for knowledge
acquisition from visual data, with all code, data, and models open-sourced for
further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are released at:
  https://maverickren.github.io/VideoWorld.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learnings from Scaling Visual Tokenizers for Reconstruction and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, Xinlei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual tokenization via auto-encoding empowers state-of-the-art image and
video generative models by compressing pixels into a latent space. Although
scaling Transformer-based generators has been central to recent advances, the
tokenizer component itself is rarely scaled, leaving open questions about how
auto-encoder design choices influence both its objective of reconstruction and
downstream generative performance. Our work aims to conduct an exploration of
scaling in auto-encoders to fill in this blank. To facilitate this exploration,
we replace the typical convolutional backbone with an enhanced Vision
Transformer architecture for Tokenization (ViTok). We train ViTok on
large-scale image and video datasets far exceeding ImageNet-1K, removing data
constraints on tokenizer scaling. We first study how scaling the auto-encoder
bottleneck affects both reconstruction and generation -- and find that while it
is highly correlated with reconstruction, its relationship with generation is
more complex. We next explored the effect of separately scaling the
auto-encoders' encoder and decoder on reconstruction and generation
performance. Crucially, we find that scaling the encoder yields minimal gains
for either reconstruction or generation, while scaling the decoder boosts
reconstruction but the benefits for generation are mixed. Building on our
exploration, we design ViTok as a lightweight auto-encoder that achieves
competitive performance with state-of-the-art auto-encoders on ImageNet-1K and
COCO reconstruction tasks (256p and 512p) while outperforming existing
auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x
fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates
competitive performance on image generation for ImageNet-1K and sets new
state-of-the-art benchmarks for class-conditional video generation on UCF-101.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 25 figures, 7 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lost in Translation, Found in Context: Sign Language Translation with
  Contextual Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngjoon Jang, Haran Raajesh, Liliane Momeni, Gül Varol, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our objective is to translate continuous sign language into spoken language
text. Inspired by the way human interpreters rely on context for accurate
translation, we incorporate additional contextual cues together with the
signing video, into a new translation framework. Specifically, besides visual
sign recognition features that encode the input video, we integrate
complementary textual information from (i) captions describing the background
show, (ii) translation of previous sentences, as well as (iii) pseudo-glosses
transcribing the signing. These are automatically extracted and inputted along
with the visual features to a pre-trained large language model (LLM), which we
fine-tune to generate spoken language translations in text form. Through
extensive ablation studies, we show the positive contribution of each input cue
to the translation performance. We train and evaluate our approach on BOBSL --
the largest British Sign Language dataset currently available. We show that our
contextual approach significantly enhances the quality of the translations
compared to previously reported results on BOBSL, and also to state-of-the-art
methods that we implement as baselines. Furthermore, we demonstrate the
generality of our approach by applying it also to How2Sign, an American Sign
Language dataset, and achieve competitive results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical
  Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexi Du, Jiazhen Zhang, Tal Zeevi, Nicha C. Dvornek, John A. Onofrey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are essential tools for computer vision
tasks, but they lack traditionally desired properties of extracted features
that could further improve model performance, e.g., rotational equivariance.
Such properties are ubiquitous in biomedical images, which often lack explicit
orientation. While current work largely relies on data augmentation or explicit
modules to capture orientation information, this comes at the expense of
increased training costs or ineffective approximations of the desired
equivariance. To overcome these challenges, we propose a novel and efficient
implementation of the Symmetric Rotation-Equivariant (SRE) Convolution
(SRE-Conv) kernel, designed to learn rotation-invariant features while
simultaneously compressing the model size. The SRE-Conv kernel can easily be
incorporated into any CNN backbone. We validate the ability of a deep SRE-CNN
to capture equivariance to rotation using the public MedMNISTv2 dataset (16
total tasks). SRE-Conv-CNN demonstrated improved rotated image classification
performance accuracy on all 16 test datasets in both 2D and 3D images, all
while increasing efficiency with fewer parameters and reduced memory footprint.
The code is available at https://github.com/XYPB/SRE-Conv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ISBI 2025 4-page paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComplexVAD: Detecting Interaction Anomalies in Video <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Furkan Mumcu, Michael J. Jones, Yasin Yilmaz, Anoop Cherian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing video anomaly detection datasets are inadequate for representing
complex anomalies that occur due to the interactions between objects. The
absence of complex anomalies in previous video anomaly detection datasets
affects research by shifting the focus onto simple anomalies. To address this
problem, we introduce a new large-scale dataset: ComplexVAD. In addition, we
propose a novel method to detect complex anomalies via modeling the
interactions between objects using a scene graph with spatio-temporal
attributes. With our proposed method and two other state-of-the-art video
anomaly detection methods, we obtain baseline scores on ComplexVAD and
demonstrate that our new method outperforms existing works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures, to appear in WACV Workshop ASTAD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference-Time Scaling for Diffusion Models beyond Scaling Denoising
  Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, Saining Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have made significant impacts across various domains,
largely due to their ability to scale during training by increasing data,
computational resources, and model size, a phenomenon characterized by the
scaling laws. Recent research has begun to explore inference-time scaling
behavior in Large Language Models (LLMs), revealing how performance can further
improve with additional computation during inference. Unlike LLMs, diffusion
models inherently possess the flexibility to adjust inference-time computation
via the number of denoising steps, although the performance gains typically
flatten after a few dozen. In this work, we explore the inference-time scaling
behavior of diffusion models beyond increasing denoising steps and investigate
how the generation performance can further improve with increased computation.
Specifically, we consider a search problem aimed at identifying better noises
for the diffusion sampling process. We structure the design space along two
axes: the verifiers used to provide feedback, and the algorithms used to find
better noise candidates. Through extensive experiments on class-conditioned and
text-conditioned image generation benchmarks, our findings reveal that
increasing inference-time compute leads to substantial improvements in the
quality of samples generated by diffusion models, and with the complicated
nature of images, combinations of the components in the framework can be
specifically chosen to conform with different application scenario.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Aerial Detection Baseline of Multimodal Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyun Li, Yushi Chen, Xinya Shu, Dong Chen, Xin He, Yi Yu, Xue Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multimodal language models (MLMs) based on generative pre-trained
Transformer are considered powerful candidates for unifying various domains and
tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding
performance in multiple tasks, such as visual question answering and visual
grounding. In addition to visual grounding that detects specific objects
corresponded to given instruction, aerial detection, which detects all objects
of multiple categories, is also a valuable and challenging task for RS
foundation models. However, aerial detection has not been explored by existing
RS MLMs because the autoregressive prediction mechanism of MLMs differs
significantly from the detection outputs. In this paper, we present a simple
baseline for applying MLMs to aerial detection for the first time, named
LMMRotate. Specifically, we first introduce a normalization method to transform
detection outputs into textual outputs to be compatible with the MLM framework.
Then, we propose a evaluation method, which ensures a fair comparison between
MLMs and conventional object detection models. We construct the baseline by
fine-tuning open-source general-purpose MLMs and achieve impressive detection
performance comparable to conventional detector. We hope that this baseline
will serve as a reference for future MLM development, enabling more
comprehensive capabilities for understanding RS images. Code is available at
https://github.com/Li-Qingyun/mllm-mmrotate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 table, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLOL: Fast Baselines for Real-World Low-Light Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan C. Benito, Daniel Feijoo, Alvaro Garcia, Marcos V. Conde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Light Image Enhancement (LLIE) is a key task in computational photography
and imaging. The problem of enhancing images captured during night or in dark
environments has been well-studied in the image signal processing literature.
However, current deep learning-based solutions struggle with efficiency and
robustness in real-world scenarios (e.g. scenes with noise, saturated pixels,
bad illumination). We propose a lightweight neural network that combines image
processing in the frequency and spatial domains. Our method, FLOL+, is one of
the fastest models for this task, achieving state-of-the-art results on popular
real scenes datasets such as LOL and LSRW. Moreover, we are able to process
1080p images under 12ms. Code and models at https://github.com/cidautai/FLOL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Continual Forgetting for <span class="highlight-title">Pre-train</span>ed Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For privacy and security concerns, the need to erase unwanted information
from pre-trained vision models is becoming evident nowadays. In real-world
scenarios, erasure requests originate at any time from both users and model
owners, and these requests usually form a sequence. Therefore, under such a
setting, selective information is expected to be continuously removed from a
pre-trained model while maintaining the rest. We define this problem as
continual forgetting and identify three key challenges. (i) For unwanted
knowledge, efficient and effective deleting is crucial. (ii) For remaining
knowledge, the impact brought by the forgetting procedure should be minimal.
(iii) In real-world scenarios, the training samples may be scarce or partially
missing during the process of forgetting. To address them, we first propose
Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA
modules to fine-tune the FFN layers in Transformer blocks for each forgetting
task independently, and towards (ii), a simple group sparse regularization is
adopted, enabling automatic selection of specific LoRA groups and zeroing out
the others. To further extend GS-LoRA to more practical scenarios, we
incorporate prototype information as additional supervision and introduce a
more practical approach, GS-LoRA++. For each forgotten class, we move the
logits away from its original prototype. For the remaining classes, we pull the
logits closer to their respective prototypes. We conduct extensive experiments
on face recognition, object detection and image classification and demonstrate
that our method manages to forget specific classes with minimal impact on other
classes. Codes have been released on https://github.com/bjzhb666/GS-LoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models via DPO:
  On-Policy Data Hold the Key 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination remains a major challenge for Large Vision-Language Models
(LVLMs). Direct Preference Optimization (DPO) has gained increasing attention
as a simple solution to hallucination issues. It directly learns from
constructed preference pairs that reflect the severity of hallucinations in
responses to the same prompt and image. Nonetheless, different data
construction methods in existing works bring notable performance variations. We
identify a crucial factor here: outcomes are largely contingent on whether the
constructed data aligns on-policy w.r.t the initial (reference) policy of DPO.
Theoretical analysis suggests that learning from off-policy data is impeded by
the presence of KL-divergence between the updated policy and the reference
policy. From the perspective of dataset distribution, we systematically
summarize the inherent flaws in existing algorithms that employ DPO to address
hallucination issues. To alleviate the problems, we propose On-Policy Alignment
(OPA)-DPO framework, which uniquely leverages expert feedback to correct
hallucinated responses and aligns both the original and expert-revised
responses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO
achieves an additional reduction in the hallucination rate of LLaVA-1.5-7B:
13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared
to the previous SOTA algorithm trained with 16k samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Grained Image-Text Correspondence with Cost Aggregation for
  Open-Vocabulary Part Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiho Choi, Seonho Lee, Minhyun Lee, Seungho Lee, Hyunjung Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing
fine-grained parts in unseen categories. We identify two primary challenges in
OVPS: (1) the difficulty in aligning part-level image-text correspondence, and
(2) the lack of structural understanding in segmenting object parts. To address
these issues, we propose PartCATSeg, a novel framework that integrates
object-aware part-level cost aggregation, compositional loss, and structural
guidance from DINO. Our approach employs a disentangled cost aggregation
strategy that handles object and part-level costs separately, enhancing the
precision of part-level segmentation. We also introduce a compositional loss to
better capture part-object relationships, compensating for the limited part
annotations. Additionally, structural guidance from DINO features improves
boundary delineation and inter-part understanding. Extensive experiments on
Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that
our method significantly outperforms state-of-the-art approaches, setting a new
baseline for robust generalization to unseen part categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP
  Evaluation Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis Roger, Prateek Humane, Daniel Z. Kaplan, Kshitij Gupta, Qi Sun, George Adamopoulos, Jonathan Siu Chi Lim, Quentin Anthony, Edwin Fennell, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of Vision-Language Models (VLMs) in the past several years
calls for rigorous and comprehensive evaluation methods and benchmarks. This
work analyzes existing VLM evaluation techniques, including automated metrics,
AI-based assessments, and human evaluations across diverse tasks. We first
introduce Robin - a novel suite of VLMs that we built by combining Large
Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use
Robin to identify shortcomings of current evaluation approaches across scales.
Next, to overcome the identified limitations, we introduce CHIRP - a new long
form response benchmark we developed for more robust and complete VLM
evaluation. We provide open access to the Robin training code, model suite, and
CHIRP benchmark to promote reproducibility and advance VLM research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Face Matching and Physical-Digital Spoofing Attack Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Kunwar, Ajita Rattani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition technology has dramatically transformed the landscape of
security, surveillance, and authentication systems, offering a user-friendly
and non-invasive biometric solution. However, despite its significant
advantages, face recognition systems face increasing threats from physical and
digital spoofing attacks. Current research typically treats face recognition
and attack detection as distinct classification challenges. This approach
necessitates the implementation of separate models for each task, leading to
considerable computational complexity, particularly on devices with limited
resources. Such inefficiencies can stifle scalability and hinder performance.
In response to these challenges, this paper introduces an innovative unified
model designed for face recognition and detection of physical and digital
attacks. By leveraging the advanced Swin Transformer backbone and incorporating
HiLo attention in a convolutional neural network framework, we address unified
face recognition and spoof attack detection more effectively. Moreover, we
introduce augmentation techniques that replicate the traits of physical and
digital spoofing cues, significantly enhancing our model robustness. Through
comprehensive experimental evaluation across various datasets, we showcase the
effectiveness of our model in unified face recognition and spoof detection.
Additionally, we confirm its resilience against unseen physical and digital
spoofing attacks, underscoring its potential for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WMamba: Wavelet-based Mamba for Face Forgery Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siran Peng, Tianshuo Zhang, Li Gao, Xiangyu Zhu, Haoyuan Zhang, Kai Pang, Zhen Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of deepfake generation technologies, the demand
for robust and accurate face forgery detection algorithms has become
increasingly critical. Recent studies have demonstrated that wavelet analysis
can uncover subtle forgery artifacts that remain imperceptible in the spatial
domain. Wavelets effectively capture important facial contours, which are often
slender, fine-grained, and global in nature. However, existing wavelet-based
approaches fail to fully leverage these unique characteristics, resulting in
sub-optimal feature extraction and limited generalizability. To address this
challenge, we introduce WMamba, a novel wavelet-based feature extractor built
upon the Mamba architecture. WMamba maximizes the utility of wavelet
information through two key innovations. First, we propose Dynamic Contour
Convolution (DCConv), which employs specially crafted deformable kernels to
adaptively model slender facial contours. Second, by leveraging the Mamba
architecture, our method captures long-range spatial relationships with linear
computational complexity. This efficiency allows for the extraction of
fine-grained, global forgery artifacts from small image patches. Extensive
experimental results show that WMamba achieves state-of-the-art (SOTA)
performance, highlighting its effectiveness and superiority in face forgery
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metric Learning with Progressive Self-Distillation for Audio-Visual
  Embedding Learning <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghuo Zeng, Kazushi Ikeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metric learning projects samples into an embedded space, where similarities
and dissimilarities are quantified based on their learned representations.
However, existing methods often rely on label-guided representation learning,
where representations of different modalities, such as audio and visual data,
are aligned based on annotated labels. This approach tends to underutilize
latent complex features and potential relationships inherent in the
distributions of audio and visual data that are not directly tied to the
labels, resulting in suboptimal performance in audio-visual embedding learning.
To address this issue, we propose a novel architecture that integrates
cross-modal triplet loss with progressive self-distillation. Our method
enhances representation learning by leveraging inherent distributions and
dynamically refining soft audio-visual alignments -- probabilistic alignments
between audio and visual data that capture the inherent relationships beyond
explicit labels. Specifically, the model distills audio-visual
distribution-based knowledge from annotated labels in a subset of each batch.
This self-distilled knowledge is used t
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 2 tables. Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential PatchCore: Anomaly Detection for Surface Inspection using
  Synthetic Impurities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runzhou Mao, Juraj Fulir, Christoph Garth, Petra Gospodnetić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The appearance of surface impurities (e.g., water stains, fingerprints,
stickers) is an often-mentioned issue that causes degradation of automated
visual inspection systems. At the same time, synthetic data generation
techniques for visual surface inspection have focused primarily on generating
perfect examples and defects, disregarding impurities. This study highlights
the importance of considering impurities when generating synthetic data. We
introduce a procedural method to include photorealistic water stains in
synthetic data. The synthetic datasets are generated to correspond to real
datasets and are further used to train an anomaly detection model and
investigate the influence of water stains. The high-resolution images used for
surface inspection lead to memory bottlenecks during anomaly detection
training. To address this, we introduce Sequential PatchCore - a method to
build coresets sequentially and make training on large images using
consumer-grade hardware tractable. This allows us to perform transfer learning
using coresets pre-trained on different dataset versions. Our results show the
benefits of using synthetic data for pre-training an explicit coreset anomaly
model and the extended performance benefits of finetuning the coreset using
real data. We observed how the impurities and labelling ambiguity lower the
model performance and have additionally reported the defect-wise recall to
provide an industrially relevant perspective on model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New Teacher-<span class="highlight-title">Review</span>er-Student Framework for Semi-supervised 2D Human
  Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wulian Yun, Mengshi Qi, Fei Peng, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional 2D human pose estimation methods typically require extensive
labeled annotations, which are both labor-intensive and expensive. In contrast,
semi-supervised 2D human pose estimation can alleviate the above problems by
leveraging a large amount of unlabeled data along with a small portion of
labeled data. Existing semi-supervised 2D human pose estimation methods update
the network through backpropagation, ignoring crucial historical information
from the previous training process. Therefore, we propose a novel
semi-supervised 2D human pose estimation method by utilizing a newly designed
Teacher-Reviewer-Student framework. Specifically, we first mimic the phenomenon
that human beings constantly review previous knowledge for consolidation to
design our framework, in which the teacher predicts results to guide the
student's learning and the reviewer stores important historical parameters to
provide additional supervision signals. Secondly, we introduce a Multi-level
Feature Learning strategy, which utilizes the outputs from different stages of
the backbone to estimate the heatmap to guide network training, enriching the
supervisory information while effectively capturing keypoint relationships.
Finally, we design a data augmentation strategy, i.e., Keypoint-Mix, to perturb
pose information by mixing different keypoints, thus enhancing the network's
ability to discern keypoints. Extensive experiments on publicly available
datasets, demonstrate our method achieves significant improvements compared to
the existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-driven Adaptation of Foundation Models for Few-shot Surgical
  Workflow Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingxuan Chen, Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Surgical workflow analysis is crucial for improving surgical
efficiency and safety. However, previous studies rely heavily on large-scale
annotated datasets, posing challenges in cost, scalability, and reliance on
expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven
Adaptation), designed to handle various surgical workflow analysis tasks with
minimal paired image-label data.
  Methods: Our approach has two key components. First, Few-shot selection-based
modality alignment selects a small subset of images and aligns their embeddings
with text embeddings from the downstream task, bridging the modality gap.
Second, Text-driven adaptation leverages only text data to train a decoder,
eliminating the need for paired image-text data. This decoder is then applied
to aligned image embeddings, enabling image-related tasks without explicit
image-text pairs.
  Results: We evaluate our approach to generative tasks (image captioning) and
discriminative tasks (triplet recognition and phase recognition). Results show
that Surg-FTDA outperforms baselines and generalizes well across downstream
tasks.
  Conclusion: We propose a text-driven adaptation approach that mitigates the
modality gap and handles multiple downstream tasks in surgical workflow
analysis, with minimal reliance on large annotated datasets. The code and
dataset will be released in https://github.com/TingxuanSix/Surg-FTDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring AI-based System Design for Pixel-level Protected Health
  Information Detection in Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Truong, Ivo M. Baltruschat, Mark Klemens, Grit Werner, Matthias Lenga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  De-identification of medical images is a critical step to ensure privacy
during data sharing in research and clinical settings. The initial step in this
process involves detecting Protected Health Information (PHI), which can be
found in image metadata or imprinted within image pixels. Despite the
importance of such systems, there has been limited evaluation of existing
AI-based solutions, creating barriers to the development of reliable and robust
tools. In this study, we present an AI-based pipeline for PHI detection,
comprising three key components: text detection, text extraction, and analysis
of PHI content in medical images. By experimenting with exchanging roles of
vision and language models within the pipeline, we evaluate the performance and
recommend the best setup for the PHI detection task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention
  Mixture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Han, Liang Du, Yiwen Wu, Xiangguo Zhou, Hongwei Du, Weibo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of VLMs often relies on the dynamic high-resolution schema that
adaptively augments the input images to multiple crops, so that the details of
the images can be retained. However, such approaches result in a large number
of redundant visual tokens, thus significantly reducing the efficiency of the
VLMs. To improve the VLMs' efficiency without introducing extra training costs,
many research works are proposed to reduce the visual tokens by filtering the
uninformative visual tokens or aggregating their information. Some approaches
propose to reduce the visual tokens according to the self-attention of VLMs,
which are biased, to result in inaccurate responses. The token reduction
approaches solely rely on visual cues are text-agnostic, and fail to focus on
the areas that are most relevant to the question, especially when the queried
objects are non-salient to the image. In this work, we first conduct
experiments to show that the original text embeddings are aligned with the
visual tokens, without bias on the tailed visual tokens. We then propose a
self-adaptive cross-modality attention mixture mechanism that dynamically
leverages the effectiveness of visual saliency and text-to-image similarity in
the pre-LLM layers to select the visual tokens that are informative. Extensive
experiments demonstrate that the proposed approach achieves state-of-the-art
training-free VLM acceleration performance, especially when the reduction rate
is sufficiently large.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HydraMix: Multi-Image Feature Mixing for Small Data Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Reinders, Frederik Schubert, Bodo Rosenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep neural networks requires datasets with a large number of
annotated examples. The collection and annotation of these datasets is not only
extremely expensive but also faces legal and privacy problems. These factors
are a significant limitation for many real-world applications. To address this,
we introduce HydraMix, a novel architecture that generates new image
compositions by mixing multiple different images from the same class. HydraMix
learns the fusion of the content of various images guided by a
segmentation-based mixing mask in feature space and is optimized via a
combination of unsupervised and adversarial training. Our data augmentation
scheme allows the creation of models trained from scratch on very small
datasets. We conduct extensive experiments on ciFAIR-10, STL-10, and
ciFAIR-100. Additionally, we introduce a novel text-image metric to assess the
generality of the augmented datasets. Our results show that HydraMix
outperforms existing state-of-the-art methods for image classification on small
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnyStory: Towards Unified Single and Multiple Subject Personalization in
  Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie He, Yuxiang Tuo, Binghui Chen, Chongyang Zhong, Yifeng Geng, Liefeng Bo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large-scale generative models have demonstrated outstanding
text-to-image generation capabilities. However, generating high-fidelity
personalized images with specific subjects still presents challenges,
especially in cases involving multiple subjects. In this paper, we propose
AnyStory, a unified approach for personalized subject generation. AnyStory not
only achieves high-fidelity personalization for single subjects, but also for
multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory
models the subject personalization problem in an "encode-then-route" manner. In
the encoding step, AnyStory utilizes a universal and powerful image encoder,
i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve
high-fidelity encoding of subject features. In the routing step, AnyStory
utilizes a decoupled instance-aware subject router to accurately perceive and
predict the potential location of the corresponding subject in the latent
space, and guide the injection of subject conditions. Detailed experimental
results demonstrate the excellent performance of our method in retaining
subject details, aligning text descriptions, and personalizing for multiple
subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report; Project page:
  https://aigcdesigngroup.github.io/AnyStory/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Omni-Emotion: Extending Video MLLM with Detailed Face and Audio Modeling
  for Multimodal Emotion Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qize Yang, Detao Bai, Yi-Xing Peng, Xihan Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding emotions accurately is essential for fields like human-computer
interaction. Due to the complexity of emotions and their multi-modal nature
(e.g., emotions are influenced by facial expressions and audio), researchers
have turned to using multi-modal models to understand human emotions rather
than single-modality. However, current video multi-modal large language models
(MLLMs) encounter difficulties in effectively integrating audio and identifying
subtle facial micro-expressions. Furthermore, the lack of detailed emotion
analysis datasets also limits the development of multimodal emotion analysis.
To address these issues, we introduce a self-reviewed dataset and a
human-reviewed dataset, comprising 24,137 coarse-grained samples and 3,500
manually annotated samples with detailed emotion annotations, respectively.
These datasets allow models to learn from diverse scenarios and better
generalize to real-world applications. Moreover, in addition to the audio
modeling, we propose to explicitly integrate facial encoding models into the
existing advanced Video MLLM, enabling the MLLM to effectively unify audio and
the subtle facial cues for emotion understanding. By aligning these features
within a unified space and employing instruction tuning in our proposed
datasets, our Omni-Emotion achieves state-of-the-art performance in both
emotion recognition and reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VanGogh: A Unified Multimodal Diffusion-based Framework for Video
  Colorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixun Fang, Zhiheng Liu, Kai Zhu, Yu Liu, Ka Leong Cheng, Wei Zhai, Yang Cao, Zheng-Jun Zha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video colorization aims to transform grayscale videos into vivid color
representations while maintaining temporal consistency and structural
integrity. Existing video colorization methods often suffer from color bleeding
and lack comprehensive control, particularly under complex motion or diverse
semantic cues. To this end, we introduce VanGogh, a unified multimodal
diffusion-based framework for video colorization. VanGogh tackles these
challenges using a Dual Qformer to align and fuse features from multiple
modalities, complemented by a depth-guided generation process and an optical
flow loss, which help reduce color overflow. Additionally, a color injection
strategy and luma channel replacement are implemented to improve generalization
and mitigate flickering artifacts. Thanks to this design, users can exercise
both global and local control over the generation process, resulting in
higher-quality colorized videos. Extensive qualitative and quantitative
evaluations, and user studies, demonstrate that VanGogh achieves superior
temporal consistency and color fidelity.Project page:
https://becauseimbatman0.github.io/VanGogh.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of Various SLAM Systems for Mobile Robot in an Indoor
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09490v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09490v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Filipenko, Ilya Afanasyev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a comparative analysis of a mobile robot trajectories
computed by various ROS-based SLAM systems. For this reason we developed a
prototype of a mobile robot with common sensors: 2D lidar, a monocular and ZED
stereo cameras. Then we conducted experiments in a typical office environment
and collected data from all sensors, running all tested SLAM systems based on
the acquired dataset. We studied the following SLAM systems: (a) 2D
lidar-based: GMapping, Hector SLAM, Cartographer; (b) monocular camera-based:
Large Scale Direct monocular SLAM (LSD SLAM), ORB SLAM, Direct Sparse Odometry
(DSO); and (c) stereo camera-based: ZEDfu, Real-Time Appearance-Based Mapping
(RTAB map), ORB SLAM, Stereo Parallel Tracking and Mapping (S-PTAM). Since all
SLAM methods were tested on the same dataset we compared results for different
SLAM systems with appropriate metrics, demonstrating encouraging results for
lidar-based Cartographer SLAM, Monocular ORB SLAM and Stereo RTAB Map methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Devil is in the Details: Simple Remedies for Image-to-LiDAR
  Representation Learning <span class="chip">ACCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09485v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09485v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjun Jo, Kwon Byung-Ki, Kim Ji-Yeon, Hawook Jeong, Kyungdon Joo, Tae-Hyun Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR is a crucial sensor in autonomous driving, commonly used alongside
cameras. By exploiting this camera-LiDAR setup and recent advances in image
representation learning, prior studies have shown the promising potential of
image-to-LiDAR distillation. These prior arts focus on the designs of their own
losses to effectively distill the pre-trained 2D image representations into a
3D model. However, the other parts of the designs have been surprisingly
unexplored. We find that fundamental design elements, e.g., the LiDAR
coordinate system, quantization according to the existing input interface, and
data utilization, are more critical than developing loss functions, which have
been overlooked in prior works. In this work, we show that simple fixes to
these designs notably outperform existing methods by 16% in 3D semantic
segmentation on the nuScenes dataset and 13% in 3D object detection on the
KITTI dataset in downstream task performance. We focus on overlooked design
choices along the spatial and temporal axes. Spatially, prior work has used
cylindrical coordinate and voxel sizes without considering their side effects
yielded with a commonly deployed sparse convolution layer input interface,
leading to spatial quantization errors in 3D models. Temporally, existing work
has avoided cumbersome data curation by discarding unsynced data, limiting the
use to only the small portion of data that is temporally synced across sensors.
We analyze these effects and propose simple solutions for each overlooked
aspect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MonoSOWA: Scalable monocular 3D Object detector Without human
  Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Skvrna, Lukas Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting the three-dimensional position and orientation of objects using a
single RGB camera is a foundational task in computer vision with many important
applications. Traditionally, 3D object detection methods are trained in a
fully-supervised setup, requiring vast amounts of human annotations, which are
laborious, costly, and do not scale well with the ever-increasing amounts of
data being captured.
  In this paper, we present the first method to train 3D object detectors for
monocular RGB cameras without domain-specific human annotations, thus making
orders of magnitude more data available for training. Thanks to newly proposed
Canonical Object Space, the method can not only exploit data across a variety
of datasets and camera setups to train a single 3D detector, but unlike
previous work it also works out of the box in previously unseen camera setups.
All this is crucial for practical applications, where the data and cameras are
extremely heterogeneous.
  The method is evaluated on two standard autonomous driving datasets, where it
outperforms previous works, which, unlike our method, still rely on 2D human
annotations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEFOM-Stereo: Depth Foundation Model Based Stereo Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hualie Jiang, Zhiqiang Lou, Laiyan Ding, Rui Xu, Minglang Tan, Wenjie Jiang, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo matching is a key technique for metric depth estimation in computer
vision and robotics. Real-world challenges like occlusion and non-texture
hinder accurate disparity estimation from binocular matching cues. Recently,
monocular relative depth estimation has shown remarkable generalization using
vision foundation models. Thus, to facilitate robust stereo matching with
monocular depth cues, we incorporate a robust monocular relative depth model
into the recurrent stereo-matching framework, building a new framework for
depth foundation model-based stereo-matching, DEFOM-Stereo. In the feature
extraction stage, we construct the combined context and matching feature
encoder by integrating features from conventional CNNs and DEFOM. In the update
stage, we use the depth predicted by DEFOM to initialize the recurrent
disparity and introduce a scale update module to refine the disparity at the
correct scale. DEFOM-Stereo is verified to have comparable performance on the
Scene Flow dataset with state-of-the-art (SOTA) methods and notably shows much
stronger zero-shot generalization. Moreover, DEFOM-Stereo achieves SOTA
performance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks,
ranking 1st on many metrics. In the joint evaluation under the robust vision
challenge, our model simultaneously outperforms previous models on the
individual benchmarks. Both results demonstrate the outstanding capabilities of
the proposed model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/Insta360-Research-Team/DEFOM-Stereo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and
  Offloading for Edge Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianrui Shi, Yong Zhao, Zeyang Cui, Xiaoming Shen, Minhang Zeng, Xiaojie Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection plays a crucial role in smart video analysis, with
applications ranging from autonomous driving and security to smart cities.
However, achieving real-time object detection on edge devices presents
significant challenges due to their limited computational resources and the
high demands of deep neural network (DNN)-based detection models, particularly
when processing high-resolution video. Conventional strategies, such as input
down-sampling and network up-scaling, often compromise detection accuracy for
faster performance or lead to higher inference latency. To address these
issues, this paper introduces RE-POSE, a Reinforcement Learning (RL)-Driven
Partitioning and Edge Offloading framework designed to optimize the
accuracy-latency trade-off in resource-constrained edge environments. Our
approach features an RL-Based Dynamic Clustering Algorithm (RL-DCA) that
partitions video frames into non-uniform blocks based on object distribution
and the computational characteristics of DNNs. Furthermore, a parallel edge
offloading scheme is implemented to distribute these blocks across multiple
edge servers for concurrent processing. Experimental evaluations show that
RE-POSE significantly enhances detection accuracy and reduces inference
latency, surpassing existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Normal-NeRF: Ambiguity-Robust Normal Estimation for Highly Reflective
  Scenes <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Shi, Xianghua Ying, Ruohao Guo, Bowei Xing, Wenzhen Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) often struggle with reconstructing and
rendering highly reflective scenes. Recent advancements have developed various
reflection-aware appearance models to enhance NeRF's capability to render
specular reflections. However, the robust reconstruction of highly reflective
scenes is still hindered by the inherent shape ambiguity on specular surfaces.
Existing methods typically rely on additional geometry priors to regularize the
shape prediction, but this can lead to oversmoothed geometry in complex scenes.
Observing the critical role of surface normals in parameterizing reflections,
we introduce a transmittance-gradient-based normal estimation technique that
remains robust even under ambiguous shape conditions. Furthermore, we propose a
dual activated densities module that effectively bridges the gap between smooth
surface normals and sharp object boundaries. Combined with a reflection-aware
appearance model, our proposed method achieves robust reconstruction and
high-fidelity rendering of scenes featuring both highly specular reflections
and intricate geometric structures. Extensive experiments demonstrate that our
method outperforms existing state-of-the-art methods on various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025, code available at https://github.com/sjj118/Normal-NeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Relation between Optical Aperture and Automotive Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ofer Bar-Shalom, Tzvi Philipp, Eran Kishon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the impact of aperture size and shape on automotive camera systems
for deep-learning-based tasks like traffic sign recognition and light state
detection. A method is proposed to simulate optical effects using the point
spread function (PSF), enhancing realism and reducing the domain gap between
synthetic and real-world images. Computer-generated scenes are refined with
this technique to model optical distortions and improve simulation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double Visual Defense: Adversarial <span class="highlight-title">Pre-train</span>ing and Instruction Tuning
  for Improving Vision-Language Model Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Wang, Cihang Xie, Brian Bartoldson, Bhavya Kailkhura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the robustness of vision-language models against
adversarial visual perturbations and introduces a novel ``double visual
defense" to enhance this robustness. Unlike previous approaches that resort to
lightweight adversarial fine-tuning of a pre-trained CLIP model, we perform
large-scale adversarial vision-language pre-training from scratch using
web-scale data. We then strengthen the defense by incorporating adversarial
visual instruction tuning. The resulting models from each stage, $\Delta$CLIP
and $\Delta^2$LLaVA, show substantially enhanced zero-shot robustness and set a
new state-of-the-art in adversarial defense for vision-language models. For
example, the adversarial robustness of $\Delta$CLIP surpasses that of the
previous best models on ImageNet-1k by ~20%. %For example, $\Delta$CLIP
surpasses the previous best models on ImageNet-1k by ~20% in terms of
adversarial robustness. Similarly, compared to prior art, $\Delta^2$LLaVA
brings a ~30% robustness improvement to image captioning task and a ~20%
robustness improvement to visual question answering task. Furthermore, our
models exhibit stronger zero-shot recognition capability, fewer hallucinations,
and superior reasoning performance compared to baselines. Our project page is
https://doublevisualdefense.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling up <span class="highlight-title">self-supervised</span> learning for improved surgical foundation
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim J. M. Jaspers, Ronald L. P. D. de Jong, Yiping Li, Carolus H. J. Kusters, Franciscus H. A. Bakker, Romy C. van Jaarsveld, Gino M. Kuiper, Richard van Hillegersberg, Jelle P. Ruurda, Willem M. Brinkman, Josien P. W. Pluim, Peter H. N. de With, Marcel Breeuwer, Yasmina Al Khalil, Fons van der Sommen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have revolutionized computer vision by achieving vastly
superior performance across diverse tasks through large-scale pretraining on
extensive datasets. However, their application in surgical computer vision has
been limited. This study addresses this gap by introducing SurgeNetXL, a novel
surgical foundation model that sets a new benchmark in surgical computer
vision. Trained on the largest reported surgical dataset to date, comprising
over 4.7 million video frames, SurgeNetXL achieves consistent top-tier
performance across six datasets spanning four surgical procedures and three
tasks, including semantic segmentation, phase recognition, and critical view of
safety (CVS) classification. Compared with the best-performing surgical
foundation models, SurgeNetXL shows mean improvements of 2.4, 9.0, and 12.6
percent for semantic segmentation, phase recognition, and CVS classification,
respectively. Additionally, SurgeNetXL outperforms the best-performing
ImageNet-based variants by 14.4, 4.0, and 1.6 percent in the respective tasks.
In addition to advancing model performance, this study provides key insights
into scaling pretraining datasets, extending training durations, and optimizing
model architectures specifically for surgical computer vision. These findings
pave the way for improved generalizability and robustness in data-scarce
scenarios, offering a comprehensive framework for future research in this
domain. All models and a subset of the SurgeNetXL dataset, including over 2
million video frames, are publicly available at:
https://github.com/TimJaspers0801/SurgeNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hwan Heo, Jangyeong Kim, Seongyeong Lee, Jeong A Wi, Junyoung Choi, Sangjun Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The synthesis of high-quality 3D assets from textual or visual inputs has
become a central objective in modern generative modeling. Despite the
proliferation of 3D generation algorithms, they frequently grapple with
challenges such as multi-view inconsistency, slow generation times, low
fidelity, and surface reconstruction problems. While some studies have
addressed some of these issues, a comprehensive solution remains elusive. In
this paper, we introduce \textbf{CaPa}, a carve-and-paint framework that
generates high-fidelity 3D assets efficiently. CaPa employs a two-stage
process, decoupling geometry generation from texture synthesis. Initially, a 3D
latent diffusion model generates geometry guided by multi-view inputs, ensuring
structural consistency across perspectives. Subsequently, leveraging a novel,
model-agnostic Spatially Decoupled Attention, the framework synthesizes
high-resolution textures (up to 4K) for a given geometry. Furthermore, we
propose a 3D-aware occlusion inpainting algorithm that fills untextured
regions, resulting in cohesive results across the entire model. This pipeline
generates high-quality 3D assets in less than 30 seconds, providing
ready-to-use outputs for commercial applications. Experimental results
demonstrate that CaPa excels in both texture fidelity and geometric stability,
establishing a new standard for practical, scalable 3D asset generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://ncsoft.github.io/CaPa/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AugRefer: Advancing 3D Visual Grounding via Cross-Modal Augmentation and
  Spatial Relation-based Referring <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wang, Na Zhao, Zhiyuan Han, Dan Guo, Xun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding (3DVG), which aims to correlate a natural language
description with the target object within a 3D scene, is a significant yet
challenging task. Despite recent advancements in this domain, existing
approaches commonly encounter a shortage: a limited amount and diversity of
text3D pairs available for training. Moreover, they fall short in effectively
leveraging different contextual clues (e.g., rich spatial relations within the
3D visual space) for grounding. To address these limitations, we propose
AugRefer, a novel approach for advancing 3D visual grounding. AugRefer
introduces cross-modal augmentation designed to extensively generate diverse
text-3D pairs by placing objects into 3D scenes and creating accurate and
semantically rich descriptions using foundation models. Notably, the resulting
pairs can be utilized by any existing 3DVG methods for enriching their training
data. Additionally, AugRefer presents a language-spatial adaptive decoder that
effectively adapts the potential referring objects based on the language
description and various 3D spatial relations. Extensive experiments on three
benchmark datasets clearly validate the effectiveness of AugRefer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vision-Language Models Do Not Understand Negation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumail Alhamoud, Shaden Alshammari, Yonglong Tian, Guohao Li, Philip Torr, Yoon Kim, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many practical vision-language applications require models that understand
negation, e.g., when using natural language to retrieve images which contain
certain objects but not others. Despite advancements in vision-language models
(VLMs) through large-scale training, their ability to comprehend negation
remains underexplored. This study addresses the question: how well do current
VLMs understand negation? We introduce NegBench, a new benchmark designed to
evaluate negation understanding across 18 task variations and 79k examples
spanning image, video, and medical datasets. The benchmark consists of two core
tasks designed to evaluate negation understanding in diverse multimodal
settings: Retrieval with Negation and Multiple Choice Questions with Negated
Captions. Our evaluation reveals that modern VLMs struggle significantly with
negation, often performing at chance level. To address these shortcomings, we
explore a data-centric approach wherein we finetune CLIP models on large-scale
synthetic datasets containing millions of negated captions. We show that this
approach can result in a 10% increase in recall on negated queries and a 40%
boost in accuracy on multiple-choice questions with negated captions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://negbench.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Neural Style Transfer for Artistic Image Generation using VGG19 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kapil Kashyap, Mehak Garg, Sean Fargose, Sindhu Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Throughout history, humans have created remarkable works of art, but
artificial intelligence has only recently started to make strides in generating
visually compelling art. Breakthroughs in the past few years have focused on
using convolutional neural networks (CNNs) to separate and manipulate the
content and style of images, applying texture synthesis techniques.
Nevertheless, a number of current techniques continue to encounter obstacles,
including lengthy processing times, restricted choices of style images, and the
inability to modify the weight ratio of styles. We proposed a neural style
transfer system that can add various artistic styles to a desired image to
address these constraints allowing flexible adjustments to style weight ratios
and reducing processing time. The system uses the VGG19 model for feature
extraction, ensuring high-quality, flexible stylization without compromising
content integrity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust and Realistic Human Pose Estimation via WiFi Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Jingcai Guo, Song Guo, Jingren Zhou, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust WiFi-based human pose estimation is a challenging task that bridges
discrete and subtle WiFi signals to human skeletons. This paper revisits this
problem and reveals two critical yet overlooked issues: 1) cross-domain gap,
i.e., due to significant variations between source-target domain pose
distributions; and 2) structural fidelity gap, i.e., predicted skeletal poses
manifest distorted topology, usually with misplaced joints and disproportionate
bone lengths. This paper fills these gaps by reformulating the task into a
novel two-phase framework dubbed DT-Pose: Domain-consistent representation
learning and Topology-constrained Pose decoding. Concretely, we first propose a
temporal-consistent contrastive learning strategy with uniformity
regularization, coupled with self-supervised masking-reconstruction operations,
to enable robust learning of domain-consistent and motion-discriminative
WiFi-specific representations. Beyond this, we introduce a simple yet effective
pose decoder with task prompts, which integrates Graph Convolution Network
(GCN) and Transformer layers to constrain the topology structure of the
generated skeleton by exploring the adjacent-overarching relationships among
human joints. Extensive experiments conducted on various benchmark datasets
highlight the superior performance of our method in tackling these fundamental
challenges in both 2D/3D human pose estimation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PISCO: <span class="highlight-title">Self-Supervised</span> k-Space Regularization for Improved Neural
  Implicit k-Space Representations of Dynamic MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronika Spieker, Hannah Eichhorn, Wenqi Huang, Jonathan K. Stelter, Tabita Catalan, Rickmer F. Braren, Daniel Rueckert, Francisco Sahli Costabal, Kerstin Hammernik, Dimitrios C. Karampinos, Claudia Prieto, Julia A. Schnabel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit k-space representations (NIK) have shown promising results
for dynamic magnetic resonance imaging (MRI) at high temporal resolutions. Yet,
reducing acquisition time, and thereby available training data, results in
severe performance drops due to overfitting. To address this, we introduce a
novel self-supervised k-space loss function $\mathcal{L}_\mathrm{PISCO}$,
applicable for regularization of NIK-based reconstructions. The proposed loss
function is based on the concept of parallel imaging-inspired self-consistency
(PISCO), enforcing a consistent global k-space neighborhood relationship
without requiring additional data. Quantitative and qualitative evaluations on
static and dynamic MR reconstructions show that integrating PISCO significantly
improves NIK representations. Particularly for high acceleration factors
(R$\geq$54), NIK with PISCO achieves superior spatio-temporal reconstruction
quality compared to state-of-the-art methods. Furthermore, an extensive
analysis of the loss assumptions and stability shows PISCO's potential as
versatile self-supervised k-space loss function for further applications and
architectures. Code is available at:
https://github.com/compai-lab/2025-pisco-spieker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Transmission and Deblurring: A Semantic Communication Approach
  Using Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pujing Yang, Guangyi Zhang, Yunlong Cai, Lei Yu, Guanding Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based joint source-channel coding (JSCC) is emerging as a
promising technology for effective image transmission. However, most existing
approaches focus on transmitting clear images, overlooking real-world
challenges such as motion blur caused by camera shaking or fast-moving objects.
Motion blur often degrades image quality, making transmission and
reconstruction more challenging. Event cameras, which asynchronously record
pixel intensity changes with extremely low latency, have shown great potential
for motion deblurring tasks. However, the efficient transmission of the
abundant data generated by event cameras remains a significant challenge. In
this work, we propose a novel JSCC framework for the joint transmission of
blurry images and events, aimed at achieving high-quality reconstructions under
limited channel bandwidth. This approach is designed as a deblurring
task-oriented JSCC system. Since RGB cameras and event cameras capture the same
scene through different modalities, their outputs contain both shared and
domain-specific information. To avoid repeatedly transmitting the shared
information, we extract and transmit their shared information and
domain-specific information, respectively. At the receiver, the received
signals are processed by a deblurring decoder to generate clear images.
Additionally, we introduce a multi-stage training strategy to train the
proposed model. Simulation results demonstrate that our method significantly
outperforms existing JSCC-based image transmission schemes, addressing motion
blur effectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVIA: A Street View Image Anonymization Framework for Self-Driving
  Applications <span class="chip">SC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyu Liu, Xuhong Wang, Cen Chen, Yanhao Wang, Shengyue Yao, Yilun Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been an increasing interest in image
anonymization, particularly focusing on the de-identification of faces and
individuals. However, for self-driving applications, merely de-identifying
faces and individuals might not provide sufficient privacy protection since
street views like vehicles and buildings can still disclose locations,
trajectories, and other sensitive information. Therefore, it remains crucial to
extend anonymization techniques to street view images to fully preserve the
privacy of users, pedestrians, and vehicles. In this paper, we propose a Street
View Image Anonymization (SVIA) framework for self-driving applications. The
SVIA framework consists of three integral components: a semantic segmenter to
segment an input image into functional regions, an inpainter to generate
alternatives to privacy-sensitive regions, and a harmonizer to seamlessly
stitch modified regions to guarantee visual coherence. Compared to existing
methods, SVIA achieves a much better trade-off between image generation quality
and privacy protection, as evidenced by experimental results for five common
metrics on two widely used public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 3 tables. Accepted by IEEE ITSC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Segmentation with <span class="highlight-title">transformer</span>s: An <span class="highlight-title">Overview</span>, Challenges and Future 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepjyoti Chetia, Debasish Dutta, Sanjib Kr Kalita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation, a key task in computer vision, has traditionally relied
on convolutional neural networks (CNNs), yet these models struggle with
capturing complex spatial dependencies, objects with varying scales, need for
manually crafted architecture components and contextual information. This paper
explores the shortcomings of CNN-based models and the shift towards transformer
architectures -to overcome those limitations. This work reviews
state-of-the-art transformer-based segmentation models, addressing
segmentation-specific challenges and their solutions. The paper discusses
current challenges in transformer-based segmentation and outlines promising
future trends, such as lightweight architectures and enhanced data efficiency.
This survey serves as a guide for understanding the impact of transformers in
advancing segmentation capabilities and overcoming the limitations of
traditional models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification of Traditional Medicinal Plant Leaves Using an effective
  Deep Learning model and Self-Curated Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepjyoti Chetia, Sanjib Kr Kalita, Prof Partha Pratim Baruah, Debasish Dutta, Tanaz Akhter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medicinal plants have been a key component in producing traditional and
modern medicines, especially in the field of Ayurveda, an ancient Indian
medical system. Producing these medicines and collecting and extracting the
right plant is a crucial step due to the visually similar nature of some
plants. The extraction of these plants from nonmedicinal plants requires human
expert intervention. To solve the issue of accurate plant identification and
reduce the need for a human expert in the collection process; employing
computer vision methods will be efficient and beneficial. In this paper, we
have proposed a model that solves such issues. The proposed model is a custom
convolutional neural network (CNN) architecture with 6 convolution layers,
max-pooling layers, and dense layers. The model was tested on three different
datasets named Indian Medicinal Leaves Image Dataset,MED117 Medicinal Plant
Leaf Dataset, and the self-curated dataset by the authors. The proposed model
achieved respective accuracies of 99.5%, 98.4%, and 99.7% using various
optimizers including Adam, RMSprop, and SGD with momentum.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strategic Base Representation Learning via Feature Augmentations for
  Few-Shot Class Incremental Learning <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parinita Nema, Vinod K Kurmi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot class incremental learning implies the model to learn new classes
while retaining knowledge of previously learned classes with a small number of
training instances. Existing frameworks typically freeze the parameters of the
previously learned classes during the incorporation of new classes. However,
this approach often results in suboptimal class separation of previously
learned classes, leading to overlap between old and new classes. Consequently,
the performance of old classes degrades on new classes. To address these
challenges, we propose a novel feature augmentation driven contrastive learning
framework designed to enhance the separation of previously learned classes to
accommodate new classes. Our approach involves augmenting feature vectors and
assigning proxy labels to these vectors. This strategy expands the feature
space, ensuring seamless integration of new classes within the expanded space.
Additionally, we employ a self-supervised contrastive loss to improve the
separation between previous classes. We validate our framework through
experiments on three FSCIL benchmark datasets: CIFAR100, miniImageNet, and
CUB200. The results demonstrate that our Feature Augmentation driven
Contrastive Learning framework significantly outperforms other approaches,
achieving state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YETI (YET to Intervene) Proactive Interventions by Multimodal AI Agents
  in Augmented Reality Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saptarashmi Bandyopadhyay, Vikas Bahirwani, Lavisha Aggarwal, Bhanu Guda, Lin Li, Andrea Colaco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal AI Agents are AI models that have the capability of interactively
and cooperatively assisting human users to solve day-to-day tasks. Augmented
Reality (AR) head worn devices can uniquely improve the user experience of
solving procedural day-to-day tasks by providing egocentric multimodal (audio
and video) observational capabilities to AI Agents. Such AR capabilities can
help AI Agents see and listen to actions that users take which can relate to
multimodal capabilities of human users. Existing AI Agents, either Large
Language Models (LLMs) or Multimodal Vision-Language Models (VLMs) are reactive
in nature, which means that models cannot take an action without reading or
listening to the human user's prompts. Proactivity of AI Agents on the other
hand can help the human user detect and correct any mistakes in agent observed
tasks, encourage users when they do tasks correctly or simply engage in
conversation with the user - akin to a human teaching or assisting a user. Our
proposed YET to Intervene (YETI) multimodal agent focuses on the research
question of identifying circumstances that may require the agent to intervene
proactively. This allows the agent to understand when it can intervene in a
conversation with human users that can help the user correct mistakes on tasks,
like cooking, using AR. Our YETI Agent learns scene understanding signals based
on interpretable notions of Structural Similarity (SSIM) on consecutive video
frames. We also define the alignment signal which the AI Agent can learn to
identify if the video frames corresponding to the user's actions on the task
are consistent with expected actions. These signals are used by our AI Agent to
determine when it should proactively intervene. We compare our results on the
instances of proactive intervention in the HoloAssist multimodal benchmark for
an expert agent guiding a user to complete procedural tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making Your Dreams A Reality: Decoding the Dreams into a Coherent Video
  Story from fMRI Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Fu, Jianxiong Gao, Baofeng Yang, Jianfeng Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the brave new idea for Multimedia community, and proposes
a novel framework to convert dreams into coherent video narratives using fMRI
data. Essentially, dreams have intrigued humanity for centuries, offering
glimpses into our subconscious minds. Recent advancements in brain imaging,
particularly functional magnetic resonance imaging (fMRI), have provided new
ways to explore the neural basis of dreaming. By combining subjective dream
experiences with objective neurophysiological data, we aim to understand the
visual aspects of dreams and create complete video narratives. Our process
involves three main steps: reconstructing visual perception, decoding dream
imagery, and integrating dream stories. Using innovative techniques in fMRI
analysis and language modeling, we seek to push the boundaries of dream
research and gain deeper insights into visual experiences during sleep. This
technical report introduces a novel approach to visually decoding dreams using
fMRI signals and weaving dream visuals into narratives using language models.
We gather a dataset of dreams along with descriptions to assess the
effectiveness of our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UVRM: A Scalable 3D Reconstruction Model from Unposed Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiu-hong Kao, Xiao Li, Jinglu Wang, Chi-Keung Tang, Yu-Wing Tai, Yan Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Reconstruction Models (LRMs) have recently become a popular method for
creating 3D foundational models. Training 3D reconstruction models with 2D
visual data traditionally requires prior knowledge of camera poses for the
training samples, a process that is both time-consuming and prone to errors.
Consequently, 3D reconstruction training has been confined to either synthetic
3D datasets or small-scale datasets with annotated poses. In this study, we
investigate the feasibility of 3D reconstruction using unposed video data of
various objects. We introduce UVRM, a novel 3D reconstruction model capable of
being trained and evaluated on monocular videos without requiring any
information about the pose. UVRM uses a transformer network to implicitly
aggregate video frames into a pose-invariant latent feature space, which is
then decoded into a tri-plane 3D representation. To obviate the need for
ground-truth pose annotations during training, UVRM employs a combination of
the score distillation sampling (SDS) method and an analysis-by-synthesis
approach, progressively synthesizing pseudo novel-views using a pre-trained
diffusion model. We qualitatively and quantitatively evaluate UVRM's
performance on the G-Objaverse and CO3D datasets without relying on pose
information. Extensive experiments show that UVRM is capable of effectively and
efficiently reconstructing a wide range of 3D objects from unposed videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SE-BSFV: Online Subspace Learning based Shadow Enhancement and
  Background Suppression for ViSAR under Complex Background 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangqu Yan, Chenyang Luo, Yaowen Fu, Wenpeng Zhang, Wei Yang, Ruofeng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video synthetic aperture radar (ViSAR) has attracted substantial attention in
the moving target detection (MTD) field due to its ability to continuously
monitor changes in the target area. In ViSAR, the moving targets' shadows will
not offset and defocus, which is widely used as a feature for MTD. However, the
shadows are difficult to distinguish from the low scattering region in the
background, which will cause more missing and false alarms. Therefore, it is
worth investigating how to enhance the distinction between the shadows and
background. In this study, we proposed the Shadow Enhancement and Background
Suppression for ViSAR (SE-BSFV) algorithm. The SE-BSFV algorithm is based on
the low-rank representation (LRR) theory and adopts online subspace learning
technique to enhance shadows and suppress background for ViSAR images. Firstly,
we use a registration algorithm to register the ViSAR images and utilize
Gaussian mixture distribution (GMD) to model the ViSAR data. Secondly, the
knowledge learned from the previous frames is leveraged to estimate the GMD
parameters of the current frame, and the Expectation-maximization (EM)
algorithm is used to estimate the subspace parameters. Then, the foreground
matrix of the current frame can be obtained. Finally, the alternating direction
method of multipliers (ADMM) is used to eliminate strong scattering objects in
the foreground matrix to obtain the final results. The experimental results
indicate that the SE-BSFV algorithm significantly enhances the shadows'
saliency and greatly improves the detection performance while ensuring
efficiency compared with several other advanced pre-processing algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-CAM: A Simpler Interpretable <span class="highlight-title">Transformer</span> for Fine-Grained
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arpita Chowdhury, Dipanjyoti Paul, Zheda Mai, Jianyang Gu, Ziheng Zhang, Kazi Sajeed Mehrab, Elizabeth G. Campolongo, Daniel Rubenstein, Charles V. Stewart, Anuj Karpatne, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple usage of pre-trained Vision Transformers (ViTs) for
fine-grained analysis, aiming to identify and localize the traits that
distinguish visually similar categories, such as different bird species or dog
breeds. Pre-trained ViTs such as DINO have shown remarkable capabilities to
extract localized, informative features. However, using saliency maps like
Grad-CAM can hardly point out the traits: they often locate the whole object by
a blurred, coarse heatmap, not traits. We propose a novel approach Prompt Class
Attention Map (Prompt-CAM) to the rescue. Prompt-CAM learns class-specific
prompts to a pre-trained ViT and uses the corresponding outputs for
classification. To classify an image correctly, the true-class prompt must
attend to the unique image patches not seen in other classes' images, i.e.,
traits. As such, the true class's multi-head attention maps reveal traits and
their locations. Implementation-wise, Prompt-CAM is almost a free lunch by
simply modifying the prediction head of Visual Prompt Tuning (VPT). This makes
Prompt-CAM fairly easy to train and apply, sharply contrasting other
interpretable methods that design specific models and training processes. It is
even simpler than the recently published INterpretable TRansformer (INTR),
whose encoder-decoder architecture prevents it from leveraging pre-trained
ViTs. Extensive empirical studies on a dozen datasets from various domains
(e.g., birds, fishes, insects, fungi, flowers, food, and cars) validate
Prompt-CAM superior interpretation capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Knowledge Distillation with Multi-Dimensional Cross-Net Attention
  for Image Restoration Models Compression <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongheng Zhang, Danfeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based encoder-decoder models have achieved remarkable success in
image-to-image transfer tasks, particularly in image restoration. However,
their high computational complexity-manifested in elevated FLOPs and parameter
counts-limits their application in real-world scenarios. Existing knowledge
distillation methods in image restoration typically employ lightweight student
models that directly mimic the intermediate features and reconstruction results
of the teacher, overlooking the implicit attention relationships between them.
To address this, we propose a Soft Knowledge Distillation (SKD) strategy that
incorporates a Multi-dimensional Cross-net Attention (MCA) mechanism for
compressing image restoration models. This mechanism facilitates interaction
between the student and teacher across both channel and spatial dimensions,
enabling the student to implicitly learn the attention matrices. Additionally,
we employ a Gaussian kernel function to measure the distance between student
and teacher features in kernel space, ensuring stable and efficient feature
learning. To further enhance the quality of reconstructed images, we replace
the commonly used L1 or KL divergence loss with a contrastive learning loss at
the image level. Experiments on three tasks-image deraining, deblurring, and
denoising-demonstrate that our SKD strategy significantly reduces computational
complexity while maintaining strong image restoration capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shape-Based Single Object Classification Using Ensemble Method
  Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nur Shazwani Kamarudin, Mokhairi Makhtar, Syadiah Nor Wan Shamsuddin, Syed Abdullah Fadzli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, more and more images are available. Annotation and retrieval of the
images pose classification problems, where each class is defined as the group
of database images labelled with a common semantic label. Various systems have
been proposed for content-based retrieval, as well as for image classification
and indexing. In this paper, a hierarchical classification framework has been
proposed for bridging the semantic gap effectively and achieving multi-category
image classification. A well known pre-processing and post-processing method
was used and applied to three problems; image segmentation, object
identification and image classification. The method was applied to classify
single object images from Amazon and Google datasets. The classification was
tested for four different classifiers; BayesNetwork (BN), Random Forest (RF),
Bagging and Vote. The estimated classification accuracies ranged from 20% to
99% (using 10-fold cross validation). The Bagging classifier presents the best
performance, followed by the Random Forest classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-conditioned and Temporal-guided Diffusion Modeling for
  Accelerated Dynamic MRI Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liping Zhang, Iris Yuwen Zhou, Sydney B. Montesi, Li Feng, Fang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: To propose a domain-conditioned and temporal-guided diffusion
modeling method, termed dynamic Diffusion Modeling (dDiMo), for accelerated
dynamic MRI reconstruction, enabling diffusion process to characterize
spatiotemporal information for time-resolved multi-coil Cartesian and
non-Cartesian data. Methods: The dDiMo framework integrates temporal
information from time-resolved dimensions, allowing for the concurrent capture
of intra-frame spatial features and inter-frame temporal dynamics in diffusion
modeling. It employs additional spatiotemporal ($x$-$t$) and self-consistent
frequency-temporal ($k$-$t$) priors to guide the diffusion process. This
approach ensures precise temporal alignment and enhances the recovery of fine
image details. To facilitate a smooth diffusion process, the nonlinear
conjugate gradient algorithm is utilized during the reverse diffusion steps.
The proposed model was tested on two types of MRI data: Cartesian-acquired
multi-coil cardiac MRI and Golden-Angle-Radial-acquired multi-coil
free-breathing lung MRI, across various undersampling rates. Results: dDiMo
achieved high-quality reconstructions at various acceleration factors,
demonstrating improved temporal alignment and structural recovery compared to
other competitive reconstruction methods, both qualitatively and
quantitatively. This proposed diffusion framework exhibited robust performance
in handling both Cartesian and non-Cartesian acquisitions, effectively
reconstructing dynamic datasets in cardiac and lung MRI under different imaging
conditions. Conclusion: This study introduces a novel diffusion modeling method
for dynamic MRI reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding the Trigger: Causal Abductive Reasoning on Video Events 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Minh Le, Vuong Le, Kien Do, Sunil Gupta, Svetha Venkatesh, Truyen Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new problem, Causal Abductive Reasoning on Video
Events (CARVE), which involves identifying causal relationships between events
in a video and generating hypotheses about causal chains that account for the
occurrence of a target event. To facilitate research in this direction, we
create two new benchmark datasets with both synthetic and realistic videos,
accompanied by trigger-target labels generated through a novel counterfactual
synthesis approach. To explore the challenge of solving CARVE, we present a
Causal Event Relation Network (CERN) that examines the relationships between
video events in temporal and semantic spaces to efficiently determine the
root-cause trigger events. Through extensive experiments, we demonstrate the
critical roles of event relational representation learning and interaction
modeling in solving video causal reasoning challenges. The introduction of the
CARVE task, along with the accompanying datasets and the CERN framework, will
advance future research on video causal reasoning and significantly facilitate
various applications, including video surveillance, root-cause analysis and
movie content management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creating Virtual Environments with 3D Gaussian Splatting: A Comparative
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has recently emerged as an innovative and
efficient 3D representation technique. While its potential for extended reality
(XR) applications is frequently highlighted, its practical effectiveness
remains underexplored. In this work, we examine three distinct 3DGS-based
approaches for virtual environment (VE) creation, leveraging their unique
strengths for efficient and visually compelling scene representation. By
conducting a comparable study, we evaluate the feasibility of 3DGS in creating
immersive VEs, identify its limitations in XR applications, and discuss future
research and development opportunities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE VR 2025 Posters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Few-Shot Medical Image Analysis via Hierarchical Contrastive
  Vision-Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harrison Fuller, Fernando Gabriela Garcia, Victor Flores
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot learning in medical image classification presents a significant
challenge due to the limited availability of annotated data and the complex
nature of medical imagery. In this work, we propose Adaptive Vision-Language
Fine-tuning with Hierarchical Contrastive Alignment (HiCA), a novel framework
that leverages the capabilities of Large Vision-Language Models (LVLMs) for
medical image analysis. HiCA introduces a two-stage fine-tuning strategy,
combining domain-specific pretraining and hierarchical contrastive learning to
align visual and textual representations at multiple levels. We evaluate our
approach on two benchmark datasets, Chest X-ray and Breast Ultrasound,
achieving state-of-the-art performance in both few-shot and zero-shot settings.
Further analyses demonstrate the robustness, generalizability, and
interpretability of our method, with substantial improvements in performance
compared to existing baselines. Our work highlights the potential of
hierarchical contrastive strategies in adapting LVLMs to the unique challenges
of medical imaging tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoccerSynth-Detection: A Synthetic Dataset for Soccer Player Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobin Qin, Calvin Yeung, Rikuhei Umemoto, Keisuke Fujii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In soccer video analysis, player detection is essential for identifying key
events and reconstructing tactical positions. The presence of numerous players
and frequent occlusions, combined with copyright restrictions, severely
restricts the availability of datasets, leaving limited options such as
SoccerNet-Tracking and SportsMOT. These datasets suffer from a lack of
diversity, which hinders algorithms from adapting effectively to varied soccer
video contexts. To address these challenges, we developed
SoccerSynth-Detection, the first synthetic dataset designed for the detection
of synthetic soccer players. It includes a broad range of random lighting and
textures, as well as simulated camera motion blur. We validated its efficacy
using the object detection model (Yolov8n) against real-world datasets
(SoccerNet-Tracking and SportsMoT). In transfer tests, it matched the
performance of real datasets and significantly outperformed them in images with
motion blur; in pre-training tests, it demonstrated its efficacy as a
pre-training dataset, significantly enhancing the algorithm's overall
performance. Our work demonstrates the potential of synthetic datasets to
replace real datasets for algorithm training in the field of soccer video
analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias for Action: Video Implicit Neural Representations with Bias
  Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alper Kayabasi, Anil Kumar Vadathya, Guha Balakrishnan, Vishwanath Saragadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new continuous video modeling framework based on implicit neural
representations (INRs) called ActINR. At the core of our approach is the
observation that INRs can be considered as a learnable dictionary, with the
shapes of the basis functions governed by the weights of the INR, and their
locations governed by the biases. Given compact non-linear activation
functions, we hypothesize that an INR's biases are suitable to capture motion
across images, and facilitate compact representations for video sequences.
Using these observations, we design ActINR to share INR weights across frames
of a video sequence, while using unique biases for each frame. We further model
the biases as the output of a separate INR conditioned on time index to promote
smoothness. By training the video INR and this bias INR together, we
demonstrate unique capabilities, including $10\times$ video slow motion,
$4\times$ spatial super resolution along with $2\times$ slow motion, denoising,
and video inpainting. ActINR performs remarkably well across numerous video
processing tasks (often achieving more than 6dB improvement), setting a new
standard for continuous modeling of videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Distillation for Image Restoration : Simultaneous Learning
  from Degraded and Clean Images <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongheng Zhang, Danfeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model compression through knowledge distillation has seen extensive
application in classification and segmentation tasks. However, its potential in
image-to-image translation, particularly in image restoration, remains
underexplored. To address this gap, we propose a Simultaneous Learning
Knowledge Distillation (SLKD) framework tailored for model compression in image
restoration tasks. SLKD employs a dual-teacher, single-student architecture
with two distinct learning strategies: Degradation Removal Learning (DRL) and
Image Reconstruction Learning (IRL), simultaneously. In DRL, the student
encoder learns from Teacher A to focus on removing degradation factors, guided
by a novel BRISQUE extractor. In IRL, the student decoder learns from Teacher B
to reconstruct clean images, with the assistance of a proposed PIQE extractor.
These strategies enable the student to learn from degraded and clean images
simultaneously, ensuring high-quality compression of image restoration models.
Experimental results across five datasets and three tasks demonstrate that SLKD
achieves substantial reductions in FLOPs and parameters, exceeding 80\%, while
maintaining strong image restoration performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Open-Vocabulary Models Ready for Detection of MEP Elements on
  Construction Sites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdalwhab Abdalwhab, Ali Imran, Sina Heydarian, Ivanka Iordanova, David St-Onge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The construction industry has long explored robotics and computer vision, yet
their deployment on construction sites remains very limited. These technologies
have the potential to revolutionize traditional workflows by enhancing
accuracy, efficiency, and safety in construction management. Ground robots
equipped with advanced vision systems could automate tasks such as monitoring
mechanical, electrical, and plumbing (MEP) systems. The present research
evaluates the applicability of open-vocabulary vision-language models compared
to fine-tuned, lightweight, closed-set object detectors for detecting MEP
components using a mobile ground robotic platform. A dataset collected with
cameras mounted on a ground robot was manually annotated and analyzed to
compare model performance. The results demonstrate that, despite the
versatility of vision-language models, fine-tuned lightweight models still
largely outperform them in specialized environments and for domain-specific
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TakuNet: an Energy-Efficient CNN for Real-Time Inference on Embedded UAV
  systems in Emergency Response Scenarios <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Rossi, Guido Borghi, Roberto Vezzani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing efficient neural networks for embedded devices is a critical
challenge, particularly in applications requiring real-time performance, such
as aerial imaging with drones and UAVs for emergency responses. In this work,
we introduce TakuNet, a novel light-weight architecture which employs
techniques such as depth-wise convolutions and an early downsampling stem to
reduce computational complexity while maintaining high accuracy. It leverages
dense connections for fast convergence during training and uses 16-bit
floating-point precision for optimization on embedded hardware accelerators.
Experimental evaluation on two public datasets shows that TakuNet achieves
near-state-of-the-art accuracy in classifying aerial images of emergency
situations, despite its minimal parameter count. Real-world tests on embedded
devices, namely Jetson Orin Nano and Raspberry Pi, confirm TakuNet's
efficiency, achieving more than 650 fps on the 15W Jetson board, making it
suitable for real-time AI processing on resource-constrained platforms and
advancing the applicability of drones in emergency scenarios. The code and
implementation details are publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at WACVW 2025, which will take place on
  28/02/2025. The official conference proceedings have not yet been published
  at the time of submission to arXiv. The final version of the paper,
  incorporating any changes based on feedback received during the conference,
  will be included in the proceedings once they are made available</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgRegNet: A Deep Regression Network for Flower and Fruit Density
  Estimation, Localization, and Counting in Orchards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uddhav Bhattarai, Santosh Bhusal, Qin Zhang, Manoj Karkee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the major challenges for the agricultural industry today is the
uncertainty in manual labor availability and the associated cost. Automated
flower and fruit density estimation, localization, and counting could help
streamline harvesting, yield estimation, and crop-load management strategies
such as flower and fruitlet thinning. This article proposes a deep
regression-based network, AgRegNet, to estimate density, count, and location of
flower and fruit in tree fruit canopies without explicit object detection or
polygon annotation. Inspired by popular U-Net architecture, AgRegNet is a
U-shaped network with an encoder-to-decoder skip connection and modified
ConvNeXt-T as an encoder feature extractor. AgRegNet can be trained based on
information from point annotation and leverages segmentation information and
attention modules (spatial and channel) to highlight relevant flower and fruit
features while suppressing non-relevant background features. Experimental
evaluation in apple flower and fruit canopy images under an unstructured
orchard environment showed that AgRegNet achieved promising accuracy as
measured by Structural Similarity Index (SSIM), percentage Mean Absolute Error
(pMAE) and mean Average Precision (mAP) to estimate flower and fruit density,
count, and centroid location, respectively. Specifically, the SSIM, pMAE, and
mAP values for flower images were 0.938, 13.7%, and 0.81, respectively. For
fruit images, the corresponding values were 0.910, 5.6%, and 0.93. Since the
proposed approach relies on information from point annotation, it is suitable
for sparsely and densely located objects. This simplified technique will be
highly applicable for growers to accurately estimate yields and decide on
optimal chemical and mechanical flower thinning practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Computers and Electronics in Agriculture</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FutureDepth: Learning to Predict the Future Improves Video Depth
  Estimation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajeev Yasarla, Manish Kumar Singh, Hong Cai, Yunxiao Shi, Jisoo Jeong, Yinhao Zhu, Shizhong Han, Risheek Garrepalli, Fatih Porikli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel video depth estimation approach,
FutureDepth, which enables the model to implicitly leverage multi-frame and
motion cues to improve depth estimation by making it learn to predict the
future at training. More specifically, we propose a future prediction network,
F-Net, which takes the features of multiple consecutive frames and is trained
to predict multi-frame features one time step ahead iteratively. In this way,
F-Net learns the underlying motion and correspondence information, and we
incorporate its features into the depth decoding process. Additionally, to
enrich the learning of multiframe correspondence cues, we further leverage a
reconstruction network, R-Net, which is trained via adaptively masked
auto-encoding of multiframe feature volumes. At inference time, both F-Net and
R-Net are used to produce queries to work with the depth decoder, as well as a
final refinement network. Through extensive experiments on several benchmarks,
i.e., NYUDv2, KITTI, DDAD, and Sintel, which cover indoor, driving, and
open-domain scenarios, we show that FutureDepth significantly improves upon
baseline models, outperforms existing video depth estimation methods, and sets
new state-of-the-art (SOTA) accuracy. Furthermore, FutureDepth is more
efficient than existing SOTA video depth estimation models and has similar
latencies when comparing to monocular models
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAMo: Leveraging Memory and Attention for Monocular Video Depth
  Estimation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14336v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14336v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajeev Yasarla, Hong Cai, Jisoo Jeong, Yunxiao Shi, Risheek Garrepalli, Fatih Porikli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose MAMo, a novel memory and attention frame-work for monocular video
depth estimation. MAMo can augment and improve any single-image depth
estimation networks into video depth estimation models, enabling them to take
advantage of the temporal information to predict more accurate depth. In MAMo,
we augment model with memory which aids the depth prediction as the model
streams through the video. Specifically, the memory stores learned visual and
displacement tokens of the previous time instances. This allows the depth
network to cross-reference relevant features from the past when predicting
depth on the current frame. We introduce a novel scheme to continuously update
the memory, optimizing it to keep tokens that correspond with both the past and
the present visual information. We adopt attention-based approach to process
memory features where we first learn the spatio-temporal relation among the
resultant visual and displacement memory tokens using self-attention module.
Further, the output features of self-attention are aggregated with the current
visual features through cross-attention. The cross-attended features are
finally given to a decoder to predict depth on the current frame. Through
extensive experiments on several benchmarks, including KITTI, NYU-Depth V2, and
DDAD, we show that MAMo consistently improves monocular depth estimation
networks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo video
depth estimation provides higher accuracy with lower latency, when omparing to
SOTA cost-volume-based video depth models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vulnerability-Aware Spatio-Temporal Learning for Generalizable and
  Interpretable Deepfake Video Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dat Nguyen, Marcella Astrid, Anis Kacem, Enjie Ghorbel, Djamila Aouada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting deepfake videos is highly challenging due to the complex
intertwined spatial and temporal artifacts in forged sequences. Most recent
approaches rely on binary classifiers trained on both real and fake data.
However, such methods may struggle to focus on important artifacts, which can
hinder their generalization capability. Additionally, these models often lack
interpretability, making it difficult to understand how predictions are made.
To address these issues, we propose FakeSTormer, offering two key
contributions. First, we introduce a multi-task learning framework with
additional spatial and temporal branches that enable the model to focus on
subtle spatio-temporal artifacts. These branches also provide interpretability
by highlighting video regions that may contain artifacts. Second, we propose a
video-level data synthesis algorithm that generates pseudo-fake videos with
subtle artifacts, providing the model with high-quality samples and ground
truth data for our spatial and temporal branches. Extensive experiments on
several challenging benchmarks demonstrate the competitiveness of our approach
compared to recent state-of-the-art methods. The code is available at
https://github.com/10Ring/FakeSTormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Super-class guided <span class="highlight-title">Transformer</span> for Zero-Shot Attribute Classification <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sehyung Kim, Chanhyeong Yang, Jihwan Park, Taehoon Song, Hyunwoo J. Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attribute classification is crucial for identifying specific characteristics
within image regions. Vision-Language Models (VLMs) have been effective in
zero-shot tasks by leveraging their general knowledge from large-scale
datasets. Recent studies demonstrate that transformer-based models with
class-wise queries can effectively address zero-shot multi-label
classification. However, poor utilization of the relationship between seen and
unseen attributes makes the model lack generalizability. Additionally,
attribute classification generally involves many attributes, making maintaining
the model's scalability difficult. To address these issues, we propose
Super-class guided transFormer (SugaFormer), a novel framework that leverages
super-classes to enhance scalability and generalizability for zero-shot
attribute classification. SugaFormer employs Super-class Query Initialization
(SQI) to reduce the number of queries, utilizing common semantic information
from super-classes, and incorporates Multi-context Decoding (MD) to handle
diverse visual cues. To strengthen generalizability, we introduce two knowledge
transfer strategies that utilize VLMs. During training, Super-class guided
Consistency Regularization (SCR) aligns model's features with VLMs using
super-class guided prompts, and during inference, Zero-shot Retrieval-based
Score Enhancement (ZRSE) refines predictions for unseen attributes. Extensive
experiments demonstrate that SugaFormer achieves state-of-the-art performance
across three widely-used attribute classification benchmarks under zero-shot,
and cross-dataset transfer settings. Our code is available at
https://github.com/mlvlab/SugaFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIS-MAE: An Efficient <span class="highlight-title">Self-supervised</span> Learning Approach on Medical Image
  Segmentation and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelong Liu, Andrew Tieu, Nikhil Patel, Georgios Soultanidis, Louisa Deyer, Ying Wang, Sean Huver, Alexander Zhou, Yunhao Mei, Zahi A. Fayad, Timothy Deyer, Xueyan Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has the potential to revolutionize diagnosis and
segmentation in medical imaging. However, development and clinical
implementation face multiple challenges including limited data availability,
lack of generalizability, and the necessity to incorporate multi-modal data
effectively. A foundation model, which is a large-scale pre-trained AI model,
offers a versatile base that can be adapted to a variety of specific tasks and
contexts. Here, we present VIsualization and Segmentation Masked AutoEncoder
(VIS-MAE), novel model weights specifically designed for medical imaging.
Specifically, VIS-MAE is trained on a dataset of 2.5 million unlabeled images
from various modalities (CT, MR, PET,X-rays, and ultrasound), using
self-supervised learning techniques. It is then adapted to classification and
segmentation tasks using explicit labels. VIS-MAE has high label efficiency,
outperforming several benchmark models in both in-domain and out-of-domain
applications. In addition, VIS-MAE has improved label efficiency as it can
achieve similar performance to other models with a reduced amount of labeled
training data (50% or 80%) compared to other pre-trained weights. VIS-MAE
represents a significant advancement in medical imaging AI, offering a
generalizable and robust solution for improving segmentation and classification
tasks while reducing the data annotation workload. The source code of this work
is available at https://github.com/lzl199704/VIS-MAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study on Multi-task Uncertainty Quantification in Semantic
  Segmentation and Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Landgraf, Markus Hillemann, Theodor Kapler, Markus Ulrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks excel in perception tasks such as semantic segmentation
and monocular depth estimation, making them indispensable in safety-critical
applications like autonomous driving and industrial inspection. However, they
often suffer from overconfidence and poor explainability, especially for
out-of-domain data. While uncertainty quantification has emerged as a promising
solution to these challenges, multi-task settings have yet to be explored. In
an effort to shed light on this, we evaluate Monte Carlo Dropout, Deep
Sub-Ensembles, and Deep Ensembles for joint semantic segmentation and monocular
depth estimation. Thereby, we reveal that Deep Ensembles stand out as the
preferred choice, particularly in out-of-domain scenarios, and show the
potential benefit of multi-task learning with regard to the uncertainty quality
in comparison to solving both tasks separately. Additionally, we highlight the
impact of employing different uncertainty thresholds to classify pixels as
certain or uncertain, with the median uncertainty emerging as a robust default.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is an extended version of a previously published
  conference paper and is currently in review for a journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> of Foundation Models in Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10729v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10729v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wasif Khan, Seowung Leem, Kyle B. See, Joshua K. Wong, Shaoting Zhang, Ruogu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) are large-scale deep learning models trained on
massive datasets, often using self-supervised learning techniques. These models
serve as a versatile base for a wide range of downstream tasks, including those
in medicine and healthcare. FMs have demonstrated remarkable success across
multiple healthcare domains. However, existing surveys in this field do not
comprehensively cover all areas where FMs have made significant strides. In
this survey, we present a comprehensive review of FMs in medicine, focusing on
their evolution, learning strategies, flagship models, applications, and
associated challenges. We examine how prominent FMs, such as the BERT and GPT
families, are transforming various aspects of healthcare, including clinical
large language models, medical image analysis, and omics research.
Additionally, we provide a detailed taxonomy of FM-enabled healthcare
applications, spanning clinical natural language processing, medical computer
vision, graph learning, and other biology- and omics- related tasks. Despite
the transformative potentials of FMs, they also pose unique challenges. This
survey delves into these challenges and highlights open research questions and
lessons learned to guide researchers and practitioners. Our goal is to provide
valuable insights into the capabilities of FMs in health, facilitating
responsible deployment and mitigating associated risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Currently under review in IEEE REVIEWS IN BIOMEDICAL ENGINEERING</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Zero-Shot Object-Level Change Detection by Incorporating
  Visual Correspondence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Huy Nguyen, Pooyan Rahmanzadehgervi, Long Mai, Anh Totti Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting object-level changes between two images across possibly different
views is a core task in many applications that involve visual inspection or
camera surveillance. Existing change-detection approaches suffer from three
major limitations: (1) lack of evaluation on image pairs that contain no
changes, leading to unreported false positive rates; (2) lack of
correspondences (i.e., localizing the regions before and after a change); and
(3) poor zero-shot generalization across different domains. To address these
issues, we introduce a novel method that leverages change correspondences (a)
during training to improve change detection accuracy, and (b) at test time, to
minimize false positives. That is, we harness the supervision labels of where
an object is added or removed to supervise change detectors, improving their
accuracy over previous work by a large margin. Our work is also the first to
predict correspondences between pairs of detected changes using estimated
homography and the Hungarian algorithm. Our model demonstrates superior
performance over existing methods, achieving state-of-the-art results in change
detection and change correspondence accuracy across both in-distribution and
zero-shot benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VITA-1.5: Towards <span class="highlight-title">GPT</span>-4o Level Real-Time Vision and Speech Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Multimodal Large Language Models (MLLMs) have typically focused on
integrating visual and textual modalities, with less emphasis placed on the
role of speech in enhancing interaction. However, speech plays a crucial role
in multimodal dialogue systems, and implementing high-performance in both
vision and speech tasks remains a significant challenge due to the fundamental
modality differences. In this paper, we propose a carefully designed
multi-stage training methodology that progressively trains LLM to understand
both visual and speech information, ultimately enabling fluent vision and
speech interaction. Our approach not only preserves strong vision-language
capacity, but also enables efficient speech-to-speech dialogue capabilities
without separate ASR and TTS modules, significantly accelerating multimodal
end-to-end response speed. By comparing our method against state-of-the-art
counterparts across benchmarks for image, video, and speech tasks, we
demonstrate that our model is equipped with both strong visual and speech
capabilities, making near real-time vision and speech interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/VITA-MLLM/VITA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian
  Neural Networks <span class="chip">AAAI'2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20891v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20891v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bao Gia Doan, Afshar Shamsi, Xiao-Yu Guo, Arash Mohammadi, Hamid Alinejad-Rokny, Dino Sejdinovic, Damien Teney, Damith C. Ranasinghe, Ehsan Abbasnejad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational complexity of Bayesian learning is impeding its adoption in
practical, large-scale tasks. Despite demonstrations of significant merits such
as improved robustness and resilience to unseen or out-of-distribution inputs
over their non- Bayesian counterparts, their practical use has faded to near
insignificance. In this study, we introduce an innovative framework to mitigate
the computational burden of Bayesian neural networks (BNNs). Our approach
follows the principle of Bayesian techniques based on deep ensembles, but
significantly reduces their cost via multiple low-rank perturbations of
parameters arising from a pre-trained neural network. Both vanilla version of
ensembles as well as more sophisticated schemes such as Bayesian learning with
Stein Variational Gradient Descent (SVGD), previously deemed impractical for
large models, can be seamlessly implemented within the proposed framework,
called Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a
dramatic reduction in the number of trainable parameters required to
approximate a Bayesian posterior; and ii) it not only maintains, but in some
instances, surpasses the performance of conventional Bayesian learning methods
and non-Bayesian baselines. Our results with large-scale tasks such as
ImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the
effectiveness and versatility of Bella in building highly scalable and
practical Bayesian deep models for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted in AAAI'2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Space Characterization of Autoen<span class="highlight-title">code</span>r Variants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anika Shrivastava, Renu Rameshan, Samar Agnihotri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the latent spaces learned by deep learning models is crucial in
exploring how they represent and generate complex data. Autoencoders (AEs) have
played a key role in the area of representation learning, with numerous
regularization techniques and training principles developed not only to enhance
their ability to learn compact and robust representations, but also to reveal
how different architectures influence the structure and smoothness of the
lower-dimensional non-linear manifold. We strive to characterize the structure
of the latent spaces learned by different autoencoders including convolutional
autoencoders (CAEs), denoising autoencoders (DAEs), and variational
autoencoders (VAEs) and how they change with the perturbations in the input. By
characterizing the matrix manifolds corresponding to the latent spaces, we
provide an explanation for the well-known observation that the latent spaces of
CAE and DAE form non-smooth manifolds, while that of VAE forms a smooth
manifold. We also map the points of the matrix manifold to a Hilbert space
using distance preserving transforms and provide an alternate view in terms of
the subspaces generated in the Hilbert space as a function of the distortion in
the input. The results show that the latent manifolds of CAE and DAE are
stratified with each stratum being a smooth product manifold, while the
manifold of VAE is a smooth product manifold of two symmetric positive definite
matrices and a symmetric positive semi-definite matrix.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, and 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STROOBnet Optimization via GPU-Accelerated Proximal Recurrence
  Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14388v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14388v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ted Edward Holmberg, Mahdi Abdelguerfi, Elias Ioup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal networks' observational capabilities are crucial for accurate
data gathering and informed decisions across multiple sectors. This study
focuses on the Spatiotemporal Ranged Observer-Observable Bipartite Network
(STROOBnet), linking observational nodes (e.g., surveillance cameras) to events
within defined geographical regions, enabling efficient monitoring. Using data
from Real-Time Crime Camera (RTCC) systems and Calls for Service (CFS) in New
Orleans, where RTCC combats rising crime amidst reduced police presence, we
address the network's initial observational imbalances. Aiming for uniform
observational efficacy, we propose the Proximal Recurrence approach. It
outperformed traditional clustering methods like k-means and DBSCAN by offering
holistic event frequency and spatial consideration, enhancing observational
coverage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 figures, 2023 IEEE International Conference on Big Data
  (BigData)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Few-Shot Image Classification through Learnable Multi-Scale
  Embedding and Attention Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Askari, Amirreza Fateh, Mohammad Reza Mohammadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of few-shot classification, the goal is to train a classifier
using a limited number of samples while maintaining satisfactory performance.
However, traditional metric-based methods exhibit certain limitations in
achieving this objective. These methods typically rely on a single distance
value between the query feature and support feature, thereby overlooking the
contribution of shallow features. To overcome this challenge, we propose a
novel approach in this paper. Our approach involves utilizing a multi-output
embedding network that maps samples into distinct feature spaces. The proposed
method extracts feature vectors at different stages, enabling the model to
capture both global and abstract features. By utilizing these diverse feature
spaces, our model enhances its performance. Moreover, employing a
self-attention mechanism improves the refinement of features at each stage,
leading to even more robust representations and improved overall performance.
Furthermore, assigning learnable weights to each stage significantly improved
performance and results. We conducted comprehensive evaluations on the
MiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way
5-shot scenarios. Additionally, we performed cross-domain tasks across eight
benchmark datasets, achieving high accuracy in the testing domains. These
evaluations demonstrate the efficacy of our proposed method in comparison to
state-of-the-art approaches. https://github.com/FatemehAskari/MSENet
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Modal Approach for Face Anti-Spoofing in Non-Calibrated Systems
  using Disparity Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24031v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24031v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Larey, Eyal Rond, Omer Achrack
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition technologies are increasingly used in various applications,
yet they are vulnerable to face spoofing attacks. These spoofing attacks often
involve unique 3D structures, such as printed papers or mobile device screens.
Although stereo-depth cameras can detect such attacks effectively, their
high-cost limits their widespread adoption. Conversely, two-sensor systems
without extrinsic calibration offer a cost-effective alternative but are unable
to calculate depth using stereo techniques. In this work, we propose a method
to overcome this challenge by leveraging facial attributes to derive disparity
information and estimate relative depth for anti-spoofing purposes, using
non-calibrated systems. We introduce a multi-modal anti-spoofing model, coined
Disparity Model, that incorporates created disparity maps as a third modality
alongside the two original sensor modalities. We demonstrate the effectiveness
of the Disparity Model in countering various spoof attacks using a
comprehensive dataset collected from the Intel RealSense ID Solution F455. Our
method outperformed existing methods in the literature, achieving an Equal
Error Rate (EER) of 1.71% and a False Negative Rate (FNR) of 2.77% at a False
Positive Rate (FPR) of 1%. These errors are lower by 2.45% and 7.94% than the
errors of the best comparison method, respectively. Additionally, we introduce
a model ensemble that addresses 3D spoof attacks as well, achieving an EER of
2.04% and an FNR of 3.83% at an FPR of 1%. Overall, our work provides a
state-of-the-art solution for the challenging task of anti-spoofing in
non-calibrated systems that lack depth information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating alignment between humans and neural network representations
  in image-based learning tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09377v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09377v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Demircan, Tankred Saanum, Leonardo Pettini, Marcel Binz, Blazej M Baczkowski, Christian F Doeller, Mona M Garvert, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans represent scenes and objects in rich feature spaces, carrying
information that allows us to generalise about category memberships and
abstract functions with few examples. What determines whether a neural network
model generalises like a human? We tested how well the representations of $86$
pretrained neural network models mapped to human learning trajectories across
two tasks where humans had to learn continuous relationships and categories of
natural images. In these tasks, both human participants and neural networks
successfully identified the relevant stimulus features within a few trials,
demonstrating effective generalisation. We found that while training dataset
size was a core determinant of alignment with human choices, contrastive
training with multi-modal data (text and imagery) was a common feature of
currently publicly available models that predicted human generalisation.
Intrinsic dimensionality of representations had different effects on alignment
for different model types. Lastly, we tested three sets of human-aligned
representations and found no consistent improvements in predictive accuracy
compared to the baselines. In conclusion, pretrained neural networks can serve
to extract representations for cognitive models, as they appear to capture some
fundamental aspects of cognition that are transferable across tasks. Both our
paradigms and modelling approach offer a novel way to quantify alignment
between neural networks and humans and extend cognitive science into more
naturalistic domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models in Vision: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04747v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04747v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models represent a recent emerging topic in computer
vision, demonstrating remarkable results in the area of generative modeling. A
diffusion model is a deep generative model that is based on two stages, a
forward diffusion stage and a reverse diffusion stage. In the forward diffusion
stage, the input data is gradually perturbed over several steps by adding
Gaussian noise. In the reverse stage, a model is tasked at recovering the
original input data by learning to gradually reverse the diffusion process,
step by step. Diffusion models are widely appreciated for the quality and
diversity of the generated samples, despite their known computational burdens,
i.e. low speeds due to the high number of steps involved during sampling. In
this survey, we provide a comprehensive review of articles on denoising
diffusion models applied in vision, comprising both theoretical and practical
contributions in the field. First, we identify and present three generic
diffusion modeling frameworks, which are based on denoising diffusion
probabilistic models, noise conditioned score networks, and stochastic
differential equations. We further discuss the relations between diffusion
models and other deep generative models, including variational auto-encoders,
generative adversarial networks, energy-based models, autoregressive models and
normalizing flows. Then, we introduce a multi-perspective categorization of
diffusion models applied in computer vision. Finally, we illustrate the current
limitations of diffusion models and envision some interesting directions for
future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Transactions on Pattern Analysis and Machine
  Intelligence. 25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DriveLM: Driving with Graph Visual Question Answering <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14150v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14150v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, Hongyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how vision-language models (VLMs) trained on web-scale data can be
integrated into end-to-end driving systems to boost generalization and enable
interactivity with human users. While recent approaches adapt VLMs to driving
via single-round visual question answering (VQA), human drivers reason about
decisions in multiple steps. Starting from the localization of key objects,
humans estimate object interactions before taking actions. The key insight is
that with our proposed task, Graph VQA, where we model graph-structured
reasoning through perception, prediction and planning question-answer pairs, we
obtain a suitable proxy task to mimic the human reasoning process. We
instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose
a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA
and end-to-end driving. The experiments demonstrate that Graph VQA provides a
simple, principled framework for reasoning about a driving scene, and
DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent
baseline performs end-to-end autonomous driving competitively in comparison to
state-of-the-art driving-specific architectures. Notably, its benefits are
pronounced when it is evaluated zero-shot on unseen objects or sensor
configurations. We hope this work can be the starting point to shed new light
on how to apply VLMs for autonomous driving. To facilitate future research, all
code, data, and models are available to the public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024 as Oral paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards an End-to-End (E2E) Adversarial Learning and Application in the
  Physical World 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dudi Biton, Jacob Shams, Satoru Koda, Asaf Shabtai, Yuval Elovici, Ben Nassi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The traditional learning process of patch-based adversarial attacks,
conducted in the digital domain and then applied in the physical domain (e.g.,
via printed stickers), may suffer from reduced performance due to adversarial
patches' limited transferability from the digital domain to the physical
domain. Given that previous studies have considered using projectors to apply
adversarial attacks, we raise the following question: can adversarial learning
(i.e., patch generation) be performed entirely in the physical domain with a
projector? In this work, we propose the Physical-domain Adversarial Patch
Learning Augmentation (PAPLA) framework, a novel end-to-end (E2E) framework
that converts adversarial learning from the digital domain to the physical
domain using a projector. We evaluate PAPLA across multiple scenarios,
including controlled laboratory settings and realistic outdoor environments,
demonstrating its ability to ensure attack success compared to conventional
digital learning-physical application (DL-PA) methods. We also analyze the
impact of environmental factors, such as projection surface color, projector
strength, ambient light, distance, and angle of the target object relative to
the camera, on the effectiveness of projected patches. Finally, we demonstrate
the feasibility of the attack against a parked car and a stop sign in a
real-world outdoor environment. Our results show that under specific
conditions, E2E adversarial learning in the physical domain eliminates the
transferability issue and ensures evasion by object detectors. Finally, we
provide insights into the challenges and opportunities of applying adversarial
learning in the physical domain and explain where such an approach is more
effective than using a sticker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TextureCrop: Enhancing Synthetic Image Detection through Texture-based
  Cropping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15500v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15500v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Despina Konstantinidou, Christos Koutlis, Symeon Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI technologies produce increasingly realistic imagery, which,
despite its potential for creative applications, can also be misused to produce
misleading and harmful content. This renders Synthetic Image Detection (SID)
methods essential for identifying AI-generated content online. State-of-the-art
SID methods typically resize or center-crop input images due to architectural
or computational constraints, which hampers the detection of artifacts that
appear in high-resolution images. To address this limitation, we propose
TextureCrop, an image pre-processing component that can be plugged in any
pre-trained SID model to improve its performance. By focusing on high-frequency
image parts where generative artifacts are prevalent, TextureCrop enhances SID
performance with manageable memory requirements. Experimental results
demonstrate a consistent improvement in AUC across various detectors by 6.1%
compared to center cropping and by 15% compared to resizing, across
high-resolution images from the Forensynths, Synthbuster and TWIGMA datasets.
Code available at https : //github.com/mever-team/texture-crop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skinned Motion Retargeting with Dense Geometric Interaction Perception <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Ye, Jia-Wei Liu, Jia Jia, Shikun Sun, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing and maintaining geometric interactions among different body parts
is crucial for successful motion retargeting in skinned characters. Existing
approaches often overlook body geometries or add a geometry correction stage
after skeletal motion retargeting. This results in conflicts between skeleton
interaction and geometry correction, leading to issues such as jittery,
interpenetration, and contact mismatches. To address these challenges, we
introduce a new retargeting framework, MeshRet, which directly models the dense
geometric interactions in motion retargeting. Initially, we establish dense
mesh correspondences between characters using semantically consistent sensors
(SCS), effective across diverse mesh topologies. Subsequently, we develop a
novel spatio-temporal representation called the dense mesh interaction (DMI)
field. This field, a collection of interacting SCS feature vectors, skillfully
captures both contact and non-contact interactions between body geometries. By
aligning the DMI field during retargeting, MeshRet not only preserves motion
semantics but also prevents self-interpenetration and ensures contact
preservation. Extensive experiments on the public Mixamo dataset and our
newly-collected ScanRet dataset demonstrate that MeshRet achieves
state-of-the-art performance. Code available at
https://github.com/abcyzj/MeshRet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ reBEN: Refined BigEarthNet Dataset for Remote Sensing Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03653v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03653v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Norman Clasen, Leonard Hackel, Tom Burgert, Gencer Sumbul, Begüm Demir, Volker Markl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents refined BigEarthNet (reBEN) that is a large-scale,
multi-modal remote sensing dataset constructed to support deep learning (DL)
studies for remote sensing image analysis. The reBEN dataset consists of
549,488 pairs of Sentinel-1 and Sentinel-2 image patches. To construct reBEN,
we initially consider the Sentinel-1 and Sentinel-2 tiles used to construct the
BigEarthNet dataset and then divide them into patches of size 1200 m x 1200 m.
We apply atmospheric correction to the Sentinel-2 patches using the latest
version of the sen2cor tool, resulting in higher-quality patches compared to
those present in BigEarthNet. Each patch is then associated with a pixel-level
reference map and scene-level multi-labels. This makes reBEN suitable for
pixel- and scene-based learning tasks. The labels are derived from the most
recent CORINE Land Cover (CLC) map of 2018 by utilizing the 19-class
nomenclature as in BigEarthNet. The use of the most recent CLC map results in
overcoming the label noise present in BigEarthNet. Furthermore, we introduce a
new geographical-based split assignment algorithm that significantly reduces
the spatial correlation among the train, validation, and test sets with respect
to those present in BigEarthNet. This increases the reliability of the
evaluation of DL models. To minimize the DL model training time, we introduce
software tools that convert the reBEN dataset into a DL-optimized data format.
In our experiments, we show the potential of reBEN for multi-modal multi-label
image classification problems by considering several state-of-the-art DL
models. The pre-trained model weights, associated code, and complete dataset
are available at https://bigearth.net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DehazeGS: Seeing Through Fog with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03659v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03659v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current novel view synthesis tasks primarily rely on high-quality and clear
images. However, in foggy scenes, scattering and attenuation can significantly
degrade the reconstruction and rendering quality. Although NeRF-based dehazing
reconstruction algorithms have been developed, their use of deep fully
connected neural networks and per-ray sampling strategies leads to high
computational costs. Moreover, NeRF's implicit representation struggles to
recover fine details from hazy scenes. In contrast, recent advancements in 3D
Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly
modeling point clouds into 3D Gaussians. In this paper, we propose leveraging
the explicit Gaussian representation to explain the foggy image formation
process through a physically accurate forward rendering process. We introduce
DehazeGS, a method capable of decomposing and rendering a fog-free background
from participating media using only muti-view foggy images as input. We model
the transmission within each Gaussian distribution to simulate the formation of
fog. During this process, we jointly learn the atmospheric light and scattering
coefficient while optimizing the Gaussian representation of the hazy scene. In
the inference stage, we eliminate the effects of scattering and attenuation on
the Gaussians and directly project them onto a 2D plane to obtain a clear view.
Experiments on both synthetic and real-world foggy datasets demonstrate that
DehazeGS achieves state-of-the-art performance in terms of both rendering
quality and computational efficiency. visualizations are available at
https://dehazegs.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StructSR: Refuse Spurious Details in Real-World Image Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yachao Li, Dong Liang, Tianyu Ding, Sheng-Jun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based models have shown great promise in real-world image
super-resolution (Real-ISR), but often generate content with structural errors
and spurious texture details due to the empirical priors and illusions of these
models. To address this issue, we introduce StructSR, a simple, effective, and
plug-and-play method that enhances structural fidelity and suppresses spurious
details for diffusion-based Real-ISR. StructSR operates without the need for
additional fine-tuning, external model priors, or high-level semantic
knowledge. At its core is the Structure-Aware Screening (SAS) mechanism, which
identifies the image with the highest structural similarity to the
low-resolution (LR) input in the early inference stage, allowing us to leverage
it as a historical structure knowledge to suppress the generation of spurious
details. By intervening in the diffusion inference process, StructSR seamlessly
integrates with existing diffusion-based Real-ISR models. Our experimental
results demonstrate that StructSR significantly improves the fidelity of
structure and texture, improving the PSNR and SSIM metrics by an average of
5.27% and 9.36% on a synthetic dataset (DIV2K-Val) and 4.13% and 8.64% on two
real-world datasets (RealSR and DRealSR) when integrated with four
state-of-the-art diffusion-based Real-ISR methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Direct Unlearning Optimization for Robust and Safe Text-to-Image Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong-Hyun Park, Sangdoo Yun, Jin-Hwa Kim, Junho Kim, Geonhui Jang, Yonghyun Jeong, Junghyo Jo, Gayoung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-image (T2I) models have unlocked a wide range
of applications but also present significant risks, particularly in their
potential to generate unsafe content. To mitigate this issue, researchers have
developed unlearning techniques to remove the model's ability to generate
potentially harmful content. However, these methods are easily bypassed by
adversarial attacks, making them unreliable for ensuring the safety of
generated images. In this paper, we propose Direct Unlearning Optimization
(DUO), a novel framework for removing Not Safe For Work (NSFW) content from T2I
models while preserving their performance on unrelated topics. DUO employs a
preference optimization approach using curated paired image data, ensuring that
the model learns to remove unsafe visual concepts while retaining unrelated
features. Furthermore, we introduce an output-preserving regularization term to
maintain the model's generative capabilities on safe content. Extensive
experiments demonstrate that DUO can robustly defend against various
state-of-the-art red teaming methods without significant performance
degradation on unrelated topics, as measured by FID and CLIP scores. Our work
contributes to the development of safer and more reliable T2I models, paving
the way for their responsible deployment in both closed-source and open-source
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Distortion Guided <span class="highlight-title">Transformer</span> for Omnidirectional Image
  Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cuixin Yang, Rongkang Dong, Jun Xiao, Cong Zhang, Kin-Man Lam, Fei Zhou, Guoping Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As virtual and augmented reality applications gain popularity,
omnidirectional image (ODI) super-resolution has become increasingly important.
Unlike 2D plain images that are formed on a plane, ODIs are projected onto
spherical surfaces. Applying established image super-resolution methods to
ODIs, therefore, requires performing equirectangular projection (ERP) to map
the ODIs onto a plane. ODI super-resolution needs to take into account
geometric distortion resulting from ERP. However, without considering such
geometric distortion of ERP images, previous deep-learning-based methods only
utilize a limited range of pixels and may easily miss self-similar textures for
reconstruction. In this paper, we introduce a novel Geometric Distortion Guided
Transformer for Omnidirectional image Super-Resolution (GDGT-OSR).
Specifically, a distortion modulated rectangle-window self-attention mechanism,
integrated with deformable self-attention, is proposed to better perceive the
distortion and thus involve more self-similar textures. Distortion modulation
is achieved through a newly devised distortion guidance generator that produces
guidance by exploiting the variability of distortion across latitudes.
Furthermore, we propose a dynamic feature aggregation scheme to adaptively fuse
the features from different self-attention modules. We present extensive
experimental results on public datasets and show that the new GDGT-OSR
outperforms methods in existing literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures, journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iFADIT: Invertible Face Anonymization via Disentangled Identity
  Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Yuan, Kai Liang, Xiong Li, Tao Wu, Nannan Wang, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face anonymization aims to conceal the visual identity of a face to safeguard
the individual's privacy. Traditional methods like blurring and pixelation can
largely remove identifying features, but these techniques significantly degrade
image quality and are vulnerable to deep reconstruction attacks. Generative
models have emerged as a promising solution for anonymizing faces while
preserving a natural appearance. However, many still face limitations in visual
quality and often overlook the potential to recover the original face from the
anonymized version, which can be valuable in specific contexts such as image
forensics. This paper proposes a novel framework named iFADIT, an acronym for
Invertible Face Anonymization via Disentangled Identity Transform. The
framework features a disentanglement architecture coupled with a secure
flow-based model: the former decouples identity information from
non-identifying attributes, while the latter transforms the decoupled identity
into an anonymized version in an invertible manner controlled by a secret key.
The anonymized face can then be reconstructed based on a pre-trained StyleGAN
that ensures high image quality and realistic facial details. Recovery of the
original face (aka de-anonymization) is possible upon the availability of the
matching secret, by inverting the anonymization process based on the same set
of model parameters. Furthermore, a dedicated secret-key mechanism along with a
dual-phase training strategy is devised to ensure the desired properties of
face anonymization. Qualitative and quantitative experiments demonstrate the
superiority of the proposed approach in anonymity, reversibility, security,
diversity, and interpretability over competing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using
  Real-Time Warped Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Burgert, Yuancheng Xu, Wenqi Xian, Oliver Pilarski, Pascal Clausen, Mingming He, Li Ma, Yitong Deng, Lingxiao Li, Mohsen Mousavi, Michael Ryoo, Paul Debevec, Ning Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling aims to transform random noise into structured outputs.
In this work, we enhance video diffusion models by allowing motion control via
structured latent noise sampling. This is achieved by just a change in data: we
pre-process training videos to yield structured noise. Consequently, our method
is agnostic to diffusion model design, requiring no changes to model
architectures or training pipelines. Specifically, we propose a novel noise
warping algorithm, fast enough to run in real time, that replaces random
temporal Gaussianity with correlated warped noise derived from optical flow
fields, while preserving the spatial Gaussianity. The efficiency of our
algorithm enables us to fine-tune modern video diffusion base models using
warped noise with minimal overhead, and provide a one-stop solution for a wide
range of user-friendly motion control: local object motion control, global
camera movement control, and motion transfer. The harmonization between
temporal coherence and spatial Gaussianity in our warped noise leads to
effective motion control while maintaining per-frame pixel quality. Extensive
experiments and user studies demonstrate the advantages of our method, making
it a robust and scalable approach for controlling motion in video diffusion
models. Video results are available on our webpage:
https://vgenai-netflix-eyeline-research.github.io/Go-with-the-Flow. Source code
and model checkpoints are available on GitHub:
https://github.com/VGenAI-Netflix-Eyeline-Research/Go-with-the-Flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point-PRC: A <span class="highlight-title">Prompt</span> Learning Based Regulation Framework for
  Generalizable Point Cloud Analysis <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20406v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20406v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Sun, Qiuhong Ke, Yongcai Wang, Wang Chen, Kang Yang, Deying Li, Jianfei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the 3D domain generalization (3DDG) ability of large
3D models based on prevalent prompt learning. Recent works demonstrate the
performances of 3D point cloud recognition can be boosted remarkably by
parameter-efficient prompt tuning. However, we observe that the improvement on
downstream tasks comes at the expense of a severe drop in 3D domain
generalization. To resolve this challenge, we present a comprehensive
regulation framework that allows the learnable prompts to actively interact
with the well-learned general knowledge in large 3D models to maintain good
generalization. Specifically, the proposed framework imposes multiple explicit
constraints on the prompt learning trajectory by maximizing the mutual
agreement between task-specific predictions and task-agnostic knowledge. We
design the regulation framework as a plug-and-play module to embed into
existing representative large 3D models. Surprisingly, our method not only
realizes consistently increasing generalization ability but also enhances
task-specific 3D recognition performances across various 3DDG benchmarks by a
clear margin. Considering the lack of study and evaluation on 3DDG, we also
create three new benchmarks, namely base-to-new, cross-dataset and few-shot
generalization benchmarks, to enrich the field and inspire future research.
Code and benchmarks are available at
\url{https://github.com/auniquesun/Point-PRC}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 figures, 14 tables; accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting
  Universal Machine Learning for Accelerated Cardiac MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Wang, Fanwen Wang, Chen Qin, Jun Lyu, Cheng Ouyang, Shuo Wang, Yan Li, Mengyao Yu, Haoyu Zhang, Kunyuan Guo, Zhang Shi, Qirong Li, Ziqiang Xu, Yajing Zhang, Hao Li, Sha Hua, Binghua Chen, Longyu Sun, Mengting Sun, Qin Li, Ying-Hua Chu, Wenjia Bai, Jing Qin, Xiahai Zhuang, Claudia Prieto, Alistair Young, Michael Markl, He Wang, Lianming Wu, Guang Yang, Xiaobo Qu, Chengyan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac magnetic resonance imaging (MRI) has emerged as a clinically
gold-standard technique for diagnosing cardiac diseases, thanks to its ability
to provide diverse information with multiple modalities and anatomical views.
Accelerated cardiac MRI is highly expected to achieve time-efficient and
patient-friendly imaging, and then advanced image reconstruction approaches are
required to recover high-quality, clinically interpretable images from
undersampled measurements. However, the lack of publicly available cardiac MRI
k-space dataset in terms of both quantity and diversity has severely hindered
substantial technological progress, particularly for data-driven artificial
intelligence. Here, we provide a standardized, diverse, and high-quality
CMRxRecon2024 dataset to facilitate the technical development, fair evaluation,
and clinical transfer of cardiac MRI reconstruction approaches, towards
promoting the universal frameworks that enable fast and robust reconstructions
across different cardiac MRI protocols in clinical practice. To the best of our
knowledge, the CMRxRecon2024 dataset is the largest and most protocal-diverse
publicly available cardiac k-space dataset. It is acquired from 330 healthy
volunteers, covering commonly used modalities, anatomical views, and
acquisition trajectories in clinical cardiac MRI workflows. Besides, an open
platform with tutorials, benchmarks, and data processing tools is provided to
facilitate data usage, advanced method development, and fair performance
evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLG-CBM: Training Concept Bottleneck Models with Vision-Language
  Guidance <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01432v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01432v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyansh Srivastava, Ge Yan, Tsui-Wei Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept Bottleneck Models (CBMs) provide interpretable prediction by
introducing an intermediate Concept Bottleneck Layer (CBL), which encodes
human-understandable concepts to explain models' decision. Recent works
proposed to utilize Large Language Models and pre-trained Vision-Language
Models to automate the training of CBMs, making it more scalable and automated.
However, existing approaches still fall short in two aspects: First, the
concepts predicted by CBL often mismatch the input image, raising doubts about
the faithfulness of interpretation. Second, it has been shown that concept
values encode unintended information: even a set of random concepts could
achieve comparable test accuracy to state-of-the-art CBMs. To address these
critical limitations, in this work, we propose a novel framework called
Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful
interpretability with the benefits of boosted performance. Our method leverages
off-the-shelf open-domain grounded object detectors to provide visually
grounded concept annotation, which largely enhances the faithfulness of concept
prediction while further improving the model performance. In addition, we
propose a new metric called Number of Effective Concepts (NEC) to control the
information leakage and provide better interpretability. Extensive evaluations
across five standard benchmarks show that our method, VLG-CBM, outperforms
existing methods by at least 4.27% and up to 51.09% on Accuracy at NEC=5
(denoted as ANEC-5), and by at least 0.45% and up to 29.78% on average accuracy
(denoted as ANEC-avg), while preserving both faithfulness and interpretability
of the learned concepts as demonstrated in extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing Forestry Images Conditioned on Plant Phenotype Using a
  Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasmita Pal, Arun Ross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plant phenology and phenotype prediction using remote sensing data are
increasingly gaining attention within the plant science community as a
promising approach to enhance agricultural productivity. This work focuses on
generating synthetic forestry images that satisfy certain phenotypic
attributes, viz. canopy greenness. We harness a Generative Adversarial Network
(GAN) to synthesize biologically plausible and phenotypically stable forestry
images conditioned on the greenness of vegetation (a continuous attribute) over
a specific region of interest, describing a particular vegetation type in a
mixed forest. The training data is based on the automated digital camera
imagery provided by the National Ecological Observatory Network (NEON) and
processed by the PhenoCam Network. Our method helps render the appearance of
forest sites specific to a greenness value. The synthetic images are
subsequently utilized to predict another phenotypic attribute, viz., redness of
plants. The quality of the synthetic images is assessed using the Structural
SIMilarity (SSIM) index and Fr\'echet Inception Distance (FID). Further, the
greenness and redness indices of the synthetic images are compared against
those of the original images using Root Mean Squared Percentage Error (RMSPE)
to evaluate their accuracy and integrity. The generalizability and scalability
of our proposed GAN model are established by effectively transforming it to
generate synthetic images for other forest sites and vegetation types. From a
broader perspective, this approach could be leveraged to visualize forestry
based on different phenotypic attributes in the context of various
environmental parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Pattern Recognition journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BRIGHT-VO: Brightness-Guided Hybrid <span class="highlight-title">Transformer</span> for Visual Odometry with
  Multi-modality Refinement Module 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongzhihan Wang, Yang Yang, Liang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual odometry (VO) plays a crucial role in autonomous driving, robotic
navigation, and other related tasks by estimating the position and orientation
of a camera based on visual input. Significant progress has been made in
data-driven VO methods, particularly those leveraging deep learning techniques
to extract image features and estimate camera poses. However, these methods
often struggle in low-light conditions because of the reduced visibility of
features and the increased difficulty of matching keypoints. To address this
limitation, we introduce BrightVO, a novel VO model based on Transformer
architecture, which not only performs front-end visual feature extraction, but
also incorporates a multi-modality refinement module in the back-end that
integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization,
this module iteratively refines pose estimates to reduce errors and improve
both accuracy and robustness. Furthermore, we create a synthetic low-light
dataset, KiC4R, which includes a variety of lighting conditions to facilitate
the training and evaluation of VO frameworks in challenging environments.
Experimental results demonstrate that BrightVO achieves state-of-the-art
performance on both the KiC4R dataset and the KITTI benchmarks. Specifically,
it provides an average improvement of 20% in pose estimation accuracy in normal
outdoor environments and 259% in low-light conditions, outperforming existing
methods. For widespread use and further development, the research work is fully
open-source at https://github.com/Anastasiawd/BrightVO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We have identified significant issues in the methodology and data
  analysis that impact the validity of our conclusions</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">23</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tale of Two Models: Understanding Data Workers' Internal and External
  Representations of Complex Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Scully-Allison, Katy Williams, Stephanie Brink, Olga Pearce, Katherine E. Isaacs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data workers may have a a different mental model of their data that the one
reified in code. Understanding the organization of their data is necessary for
analyzing data, be it through scripting, visualization or abstract thought.
More complicated organizations, such as tables with attached hierarchies, may
tax people's ability to think about and interact with data. To better
understand and ultimately design for these situations, we conduct a study
across a team of ten people work ing with the same reified data model. Through
interviews and sketching, we conduct a study across a team of ten people
working with the same reified data model. Through interviews and sketching, we
probed their conception of the data model and developed themes through
reflexive data analysis. Participants had diverse data models that differed
from the reified data model, even among team members who had designed the
model, resulting in parallel hazards limiting their ability to reason about the
data. From these observations, we suggest potential design interventions for
data analysis processes and tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Automated Feedback Systems for Tutor Training in Low-Resource
  Scenarios through Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chentianye Xu, Jionghao Lin, Tongshuang Wu, Vincent Aleven, Kenneth R. Koedinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tutoring is an effective instructional method for enhancing student learning,
yet its success relies on the skill and experience of the tutors. This reliance
presents challenges for the widespread implementation of tutoring, particularly
in training novice tutors. To support tutor training programs, real-time
automated feedback systems are essential for efficiently training large numbers
of tutors. Lin et al.'s previous study employed Generative Pre-Trained
Transformers (GPT) for sequence labeling to identify desirable and undesirable
praise components in a tutor training dataset, providing explanatory feedback.
However, this approach requires a significant amount of labeled data for
fine-tuning, which is both labor-intensive and dependent on expert input. To
address the challenges associated with extensive data labeling, the current
study explores the use of prompting more advanced GPT models like GPT-4o to
generate synthetic datasets for augmenting labeled response data, followed by
fine-tuning a GPT-3.5 model. Our results demonstrate that our data augmentation
approach generalizes effectively to identify other types of praise, compared to
the same model fine-tuned without augmentation. These findings suggest that for
data-intensive tasks, synthetic data generated through GPT model prompting can
substantially enhance fine-tuned model performance in low-resource scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMPLest-X: Ultimate Scaling for Expressive Human Pose and Shape
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanqi Yin, Zhongang Cai, Ruisi Wang, Ailing Zeng, Chen Wei, Qingping Sun, Haiyi Mei, Yanjun Wang, Hui En Pang, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Atsushi Yamashita, Lei Yang, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressive human pose and shape estimation (EHPS) unifies body, hands, and
face motion capture with numerous applications. Despite encouraging progress,
current state-of-the-art methods focus on training innovative architectural
designs on confined datasets. In this work, we investigate the impact of
scaling up EHPS towards a family of generalist foundation models. 1) For data
scaling, we perform a systematic investigation on 40 EHPS datasets,
encompassing a wide range of scenarios that a model trained on any single
dataset cannot handle. More importantly, capitalizing on insights obtained from
the extensive benchmarking process, we optimize our training scheme and select
datasets that lead to a significant leap in EHPS capabilities. Ultimately, we
achieve diminishing returns at 10M training instances from diverse data
sources. 2) For model scaling, we take advantage of vision transformers (up to
ViT-Huge as the backbone) to study the scaling law of model sizes in EHPS. To
exclude the influence of algorithmic design, we base our experiments on two
minimalist architectures: SMPLer-X, which consists of an intermediate step for
hand and face localization, and SMPLest-X, an even simpler version that reduces
the network to its bare essentials and highlights significant advances in the
capture of articulated hands. With big data and the large model, the foundation
models exhibit strong performance across diverse test benchmarks and excellent
transferability to even unseen environments. Moreover, our finetuning strategy
turns the generalist into specialist models, allowing them to achieve further
performance boosts. Notably, our foundation models consistently deliver
state-of-the-art results on seven benchmarks such as AGORA, UBody, EgoBody, and
our proposed SynHand dataset for comprehensive hand evaluation. (Code is
available at: https://github.com/wqyin/SMPLest-X).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extension of SMPLer-X [arXiv:2309.17448]. Homepage:
  https://caizhongang.com/projects/SMPLer-X/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniThink: Expanding Knowledge Boundaries in Machine Writing through
  Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine writing with large language models often relies on
retrieval-augmented generation. However, these approaches remain confined
within the boundaries of the model's predefined scope, limiting the generation
of content with rich information. Specifically, vanilla-retrieved information
tends to lack depth, utility, and suffers from redundancy, which negatively
impacts the quality of generated articles, leading to shallow, repetitive, and
unoriginal outputs. To address these issues, we propose OmniThink, a machine
writing framework that emulates the human-like process of iterative expansion
and reflection. The core idea behind OmniThink is to simulate the cognitive
behavior of learners as they progressively deepen their knowledge of the
topics. Experimental results demonstrate that OmniThink improves the knowledge
density of generated articles without compromising metrics such as coherence
and depth. Human evaluations and expert feedback further highlight the
potential of OmniThink to address real-world challenges in the generation of
long-form articles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through
  Category-Bounding <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Kirmayr, Lukas Stappen, Phillip Schneider, Florian Matthes, Elisabeth André
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's assistant landscape, personalisation enhances interactions,
fosters long-term relationships, and deepens engagement. However, many systems
struggle with retaining user preferences, leading to repetitive user requests
and disengagement. Furthermore, the unregulated and opaque extraction of user
preferences in industry applications raises significant concerns about privacy
and trust, especially in regions with stringent regulations like Europe. In
response to these challenges, we propose a long-term memory system for voice
assistants, structured around predefined categories. This approach leverages
Large Language Models to efficiently extract, store, and retrieve preferences
within these categories, ensuring both personalisation and transparency. We
also introduce a synthetic multi-turn, multi-session conversation dataset
(CarMem), grounded in real industry data, tailored to an in-car voice assistant
setting. Benchmarked on the dataset, our system achieves an F1-score of .78 to
.95 in preference extraction, depending on category granularity. Our
maintenance strategy reduces redundant preferences by 95% and contradictory
ones by 92%, while the accuracy of optimal retrieval is at .87. Collectively,
the results demonstrate the system's suitability for industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the International Conference on
  Computational Linguistics (COLING 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make yourself comfortable: Nudging urban heat and noise mitigation with
  smartwatch-based Just-in-time Adaptive Interventions (JITAI) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clayton Miller, Yun Xuan Chua, Matias Quintana, Binyu Lei, Filip Biljecki, Mario Frei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can play a more active role in improving their comfort in the built
environment if given the right information at the right place and time. This
paper outlines the use of Just-in-Time Adaptive Interventions (JITAI)
implemented in the context of the built environment to provide information that
helps humans minimize the impact of heat and noise on their daily lives. This
framework builds upon the open-source Cozie iOS smartwatch platform. It
includes data collection through micro-surveys and intervention messages
triggered by environmental, contextual, and personal history conditions. An
eight-month deployment of the method was completed in Singapore with 103
participants who submitted over 12,000 micro-surveys and delivered over 3,600
JITAI intervention messages. A weekly survey conducted during two deployment
phases revealed an overall increase in perceived usefulness ranging from 8-19%
over the first three weeks of data collection. For noise-related interventions,
participants showed an overall increase in location changes ranging from 4-11%
and a 2-17% increase in earphone use to mitigate noise distractions. For
thermal comfort-related interventions, participants demonstrated a 3-13%
increase in adjustments to their location or thermostat to feel more
comfortable. The analysis found evidence that personality traits (such as
conscientiousness), gender, and environmental preferences could be factors in
determining the perceived helpfulness of JITAIs and influencing behavior
change. These findings underscore the importance of tailoring intervention
strategies to individual traits and environmental conditions, setting the stage
for future research to refine the delivery, timing, and content of intervention
messages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting a <span class="highlight-title">Large Language Model</span> with a Combination of Text and Visual
  Data for Conversational Visualization of Global Geospatial Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Mena, Alexandre Kouyoumdjian, Lonni Besançon, Michael Gleicher, Ivan Viola, Anders Ynnerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method for augmenting a Large Language Model (LLM) with a
combination of text and visual data to enable accurate question answering in
visualization of scientific data, making conversational visualization possible.
LLMs struggle with tasks like visual data interaction, as they lack contextual
visual information. We address this problem by merging a text description of a
visualization and dataset with snapshots of the visualization. We extract their
essential features into a structured text file, highly compact, yet descriptive
enough to appropriately augment the LLM with contextual information, without
any fine-tuning. This approach can be applied to any visualization that is
already finally rendered, as long as it is associated with some textual
description.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "A Great Start, But...": Evaluating LLM-Generated Mind Maps for
  Information Mapping in Video-Based Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhao He, Karthi Saravanan, Evangelos Niforatos, Gerd Kortuem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting concepts and understanding relationships from videos is essential
in Video-Based Design (VBD), where videos serve as a primary medium for
exploration but require significant effort in managing meta-information. Mind
maps, with their ability to visually organize complex data, offer a promising
approach for structuring and analysing video content. Recent advancements in
Large Language Models (LLMs) provide new opportunities for meta-information
processing and visual understanding in VBD, yet their application remains
underexplored. This study recruited 28 VBD practitioners to investigate the use
of prompt-tuned LLMs for generating mind maps from ethnographic videos.
Comparing LLM-generated mind maps with those created by professional designers,
we evaluated rated scores, design effectiveness, and user experience across two
contexts. Findings reveal that LLMs effectively capture central concepts but
struggle with hierarchical organization and contextual grounding. We discuss
trust, customization, and workflow integration as key factors to guide future
research on LLM-supported information mapping in VBD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effects of Social Contextual Variation Using Partner Avatars on Memory
  Acquisition and Retention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takato Mizuho, Takuji Narumi, Hideaki Kuzuoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates how partner avatar design affects learning and memory
when an avatar serves as a lecturer. Based on earlier research on the
environmental context dependency of memory, we hypothesize that the use of
diverse partner avatars results in a slower learning rate but better memory
retention than that of a constant partner avatar. Accordingly, participants
were tasked with memorizing Tagalog--Japanese word pairs. On the first day of
the experiment, they repeatedly learned the pairs over six sessions from a
partner avatar in an immersive virtual environment. One week later, on the
second day of the experiment, they underwent a recall test in a real
environment. We employed a between-participants design to compare the following
conditions: the varied avatar condition, in which each repetition used a
different avatar, and the constant avatar condition, in which the same avatar
was used throughout the experiment. Results showed that, compared to the
constant avatar condition, the varied avatar condition resulted in
significantly lower recall performance in the repeated learning trials
conducted on the first day. However, the avatar conditions showed no
significant differences in the final recall test on the second day. We discuss
these effects in relation to the social presence of the partner avatar. This
study opens up a novel approach to optimizing the effectiveness of instructor
avatars in immersive virtual environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChartInsighter: An Approach for Mitigating Hallucination in Time-series
  Chart Summary Generation with A Benchmark Dataset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fen Wang, Bomiao Wang, Xueli Shu, Zhen Liu, Zekai Shao, Chao Liu, Siming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective chart summary can significantly reduce the time and effort decision
makers spend interpreting charts, enabling precise and efficient communication
of data insights. Previous studies have faced challenges in generating accurate
and semantically rich summaries of time-series data charts. In this paper, we
identify summary elements and common hallucination types in the generation of
time-series chart summaries, which serve as our guidelines for automatic
generation. We introduce ChartInsighter, which automatically generates chart
summaries of time-series data, effectively reducing hallucinations in chart
summary generation. Specifically, we assign multiple agents to generate the
initial chart summary and collaborate iteratively, during which they invoke
external data analysis modules to extract insights and compile them into a
coherent summary. Additionally, we implement a self-consistency test method to
validate and correct our summary. We create a high-quality benchmark of charts
and summaries, with hallucination types annotated on a sentence-by-sentence
basis, facilitating the evaluation of the effectiveness of reducing
hallucinations. Our evaluations using our benchmark show that our method
surpasses state-of-the-art models, and that our summary hallucination rate is
the lowest, which effectively reduces various hallucinations and improves
summary quality. The benchmark is available at
https://github.com/wangfen01/ChartInsighter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Creating Virtual Environments with 3D Gaussian Splatting: A Comparative
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Qiu, Binzhu Xie, Qixuan Liu, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) has recently emerged as an innovative and
efficient 3D representation technique. While its potential for extended reality
(XR) applications is frequently highlighted, its practical effectiveness
remains underexplored. In this work, we examine three distinct 3DGS-based
approaches for virtual environment (VE) creation, leveraging their unique
strengths for efficient and visually compelling scene representation. By
conducting a comparable study, we evaluate the feasibility of 3DGS in creating
immersive VEs, identify its limitations in XR applications, and discuss future
research and development opportunities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE VR 2025 Posters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Spread of Virtual Gifting in Live Streaming: The Case of Twitch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Eun Kim, Seura Ha, Sangmi Kim, Libby Hemphill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines how gifting spreads among viewers on Twitch, one of the
largest live streaming platforms worldwide. Twitch users can give gift
subscriptions to other viewers in the chat room, with the majority of gifters
opting for community gifting, which is gifting to randomly selected viewers. We
identify the random nature of gift-receiving in our data as a natural
experiment setting. We investigate whether gift recipients pay it forward,
considering various gift types that may either promote or deter the spread of
gifting. Our findings reveal that Twitch viewers who receive gift subscriptions
are generally more likely to pay it forward than non-recipients, and the
positive impact of gift-receiving becomes stronger when the recipient is the
sole beneficiary of the giver's gifting behavior. However, we found that gifts
from frequent gifters discourage recipients from paying it forward, and gifts
from anonymous gifters do not influence the likelihood of viewers becoming
future gifters. This research contributes to the existing literature on the
spread of online prosocial behavior by providing robust evidence and suggests
practical strategies for promoting online gifting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalized Parsons Puzzles as Scaffolding Enhance Practice Engagement
  Over Just Showing LLM-Powered Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinying Hou, Zihan Wu, Xu Wang, Barbara J. Ericson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As generative AI products could generate code and assist students with
programming learning seamlessly, integrating AI into programming education
contexts has driven much attention. However, one emerging concern is that
students might get answers without learning from the LLM-generated content. In
this work, we deployed the LLM-powered personalized Parsons puzzles as
scaffolding to write-code practice in a Python learning classroom (PC
condition) and conducted an 80-minute randomized between-subjects study. Both
conditions received the same practice problems. The only difference was that
when requesting help, the control condition showed students a complete solution
(CC condition), simulating the most traditional LLM output. Results indicated
that students who received personalized Parsons puzzles as scaffolding engaged
in practicing significantly longer than those who received complete solutions
when struggling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncovering the Internet's Hidden Values: An Empirical Study of Desirable
  Behavior Using Highly-Upvoted Content on Reddit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agam Goyal, Charlotte Lambert, Yoshee Jain, Eshwar Chandrasekharan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major task for moderators of online spaces is norm-setting, essentially
creating shared norms for user behavior in their communities. Platform design
principles emphasize the importance of highlighting norm-adhering examples and
explicitly stating community norms. However, norms and values vary between
communities and go beyond content-level attributes, making it challenging for
platforms and researchers to provide automated ways to identify desirable
behavior to be highlighted. Current automated approaches to detect desirability
are limited to measures of prosocial behavior, but we do not know whether these
measures fully capture the spectrum of what communities value. In this paper,
we use upvotes, which express community approval, as a proxy for desirability
and examine 16,000 highly-upvoted comments across 80 popular sub-communities on
Reddit. Using a large language model, we extract values from these comments
across two years (2016 and 2022) and compile 64 and 72 $\textit{macro}$,
$\textit{meso}$, and $\textit{micro}$ values for 2016 and 2022 respectively,
based on their frequency across communities. Furthermore, we find that existing
computational models for measuring prosociality were inadequate to capture on
average $82\%$ of the values we extracted. Finally, we show that our approach
can not only extract most of the qualitatively-identified values from prior
taxonomies, but also uncover new values that are actually encouraged in
practice. Our findings highlight the need for nuanced models of desirability
that go beyond preexisting prosocial measures. This work has implications for
improving moderator understanding of their community values and provides a
framework that can supplement qualitative approaches with larger-scale content
analyses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint: 14 pages, 3 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AeroHaptix: A Wearable Vibrotactile Feedback System for Enhancing
  Collision Avoidance in UAV Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12105v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12105v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingjian Huang, Zhecheng Wang, Qilong Cheng, Siyi Ren, Hanfeng Cai, Antonio Alvarez Valdivia, Karthik Mahadevan, Daniel Wigdor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Haptic feedback enhances collision avoidance by providing directional
obstacle information to operators during unmanned aerial vehicle (UAV)
teleoperation. However, such feedback is often rendered via haptic joysticks,
which are unfamiliar to UAV operators and limited to single-direction force
feedback. Additionally, the direct coupling between the input device and the
feedback method diminishes operators' sense of control and induces oscillatory
movements. To overcome these limitations, we propose AeroHaptix, a wearable
haptic feedback system that uses spatial vibrations to simultaneously
communicate multiple obstacle directions to operators, without interfering with
their input control. The layout of vibrotactile actuators was optimized via a
perceptual study to eliminate perceptual biases and achieve uniform spatial
coverage. A novel rendering algorithm, MultiCBF, extended control barrier
functions to support multi-directional feedback. Our system evaluation showed
that compared to a no-feedback condition, AeroHaptix effectively reduced the
number of collisions and input disagreement. Furthermore, operators reported
that AeroHaptix was more helpful than force feedback, with improved situational
awareness and comparable workload.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Humanoid Robot RHP Friends: Seamless Combination of Autonomous and
  Teleoperated Tasks in a Nursing Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Benallegue, Guillaume Lorthioir, Antonin Dallard, Rafael Cisneros-Limón, Iori Kumagai, Mitsuharu Morisawa, Hiroshi Kaminaga, Masaki Murooka, Antoine Andre, Pierre Gergondet, Kenji Kaneko, Guillaume Caron, Fumio Kanehiro, Abderrahmane Kheddar, Soh Yukizaki, Junichi Karasuyama, Junichi Murakami, Masayuki Kamon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes RHP Friends, a social humanoid robot developed to enable
assistive robotic deployments in human-coexisting environments. As a use-case
application, we present its potential use in nursing by extending its
capabilities to operate human devices and tools according to the task and by
enabling remote assistance operations. To meet a wide variety of tasks and
situations in environments designed by and for humans, we developed a system
that seamlessly integrates the slim and lightweight robot and several
technologies: locomanipulation, multi-contact motion, teleoperation, and object
detection and tracking. We demonstrated the system's usage in a nursing
application. The robot efficiently performed the daily task of patient transfer
and a non-routine task, represented by a request to operate a circuit breaker.
This demonstration, held at the 2023 International Robot Exhibition (IREX),
conducted three times a day over three days.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Robotics and Automation Magazine, In press</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Assist Humans without Inferring Rewards <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Myers, Evan Ellis, Sergey Levine, Benjamin Eysenbach, Anca Dragan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive agents should make humans' lives easier. Classically, such
assistance is studied through the lens of inverse reinforcement learning, where
an assistive agent (e.g., a chatbot, a robot) infers a human's intention and
then selects actions to help the human reach that goal. This approach requires
inferring intentions, which can be difficult in high-dimensional settings. We
build upon prior work that studies assistance through the lens of empowerment:
an assistive agent aims to maximize the influence of the human's actions such
that they exert a greater control over the environmental outcomes and can solve
tasks in fewer steps. We lift the major limitation of prior work in this
area--scalability to high-dimensional settings--with contrastive successor
representations. We formally prove that these representations estimate a
similar notion of empowerment to that studied by prior work and provide a
ready-made mechanism for optimizing it. Empirically, our proposed method
outperforms prior methods on synthetic benchmarks, and scales to Overcooked, a
cooperative game setting. Theoretically, our work connects ideas from
information theory, neuroscience, and reinforcement learning, and charts a path
for representations to play a critical role in solving assistive problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Neural Information Processing Systems (NeurIPS), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PsyDI: Towards a Personalized and Progressively In-depth Chatbot for
  Psychological Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyan Li, Xinyan Chen, Yazhe Niu, Shuai Hu, Yu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of psychology, traditional assessment methods, such as
standardized scales, are frequently critiqued for their static nature, lack of
personalization, and reduced participant engagement, while comprehensive
counseling evaluations are often inaccessible. The complexity of quantifying
psychological traits further limits these methods. Despite advances with large
language models (LLMs), many still depend on single-round Question-and-Answer
interactions. To bridge this gap, we introduce PsyDI, a personalized and
progressively in-depth chatbot designed for psychological measurements,
exemplified by its application in the Myers-Briggs Type Indicator (MBTI)
framework. PsyDI leverages user-related multi-modal information and engages in
customized, multi-turn interactions to provide personalized, easily accessible
measurements, while ensuring precise MBTI type determination. To address the
challenge of unquantifiable psychological traits, we introduce a novel training
paradigm that involves learning the ranking of proxy variables associated with
these traits, culminating in a robust score model for MBTI measurements. The
score model enables PsyDI to conduct comprehensive and precise measurements
through multi-turn interactions within a unified estimation context. Through
various experiments, we validate the efficacy of both the score model and the
PsyDI pipeline, demonstrating its potential to serve as a general framework for
psychological measurements. Furthermore, the online deployment of PsyDI has
garnered substantial user engagement, with over 3,000 visits, resulting in the
collection of numerous multi-turn dialogues annotated with MBTI types, which
facilitates further research. The source code for the training and web service
components is publicly available as a part of OpenDILab at:
https://github.com/opendilab/PsyDI
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Gym: A Framework for Enabling and Evaluating Human-Agent
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Shao, Vinay Samuel, Yucheng Jiang, John Yang, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language models (LMs) have sparked growing interest in
developing LM agents. While fully autonomous agents could excel in many
scenarios, numerous use cases inherently require them to collaborate with
humans due to humans' latent preferences, domain expertise, or need for
control. To facilitate the study of human-agent collaboration, we present
Collaborative Gym (Co-Gym), a general framework enabling asynchronous,
tripartite interaction among agents, humans, and task environments. We
instantiate Co-Gym with three representative tasks in both simulated and
real-world conditions, and propose an evaluation framework that assesses both
the collaboration outcomes and processes. Our findings reveal that
collaborative agents consistently outperform their fully autonomous
counterparts in task performance within those delivered cases, achieving win
rates of 86% in Travel Planning, 74% in Tabular Analysis, and 66% in Related
Work when evaluated by real users. However, our study also highlights
significant challenges in developing collaborative agents, requiring
advancements in core aspects of intelligence -- communication capabilities,
situational awareness, and balancing autonomy and human control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Submerse: Visualizing Storm Surge Flooding Simulations in Immersive
  Display Ecologies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeed Boorboor, Yoonsang Kim, Ping Hu, Josef M. Moses, Brian A. Colle, Arie E. Kaufman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Submerse, an end-to-end framework for visualizing flooding
scenarios on large and immersive display ecologies. Specifically, we
reconstruct a surface mesh from input flood simulation data and generate a
to-scale 3D virtual scene by incorporating geographical data such as terrain,
textures, buildings, and additional scene objects. To optimize computation and
memory performance for large simulation datasets, we discretize the data on an
adaptive grid using dynamic quadtrees and support level-of-detail based
rendering. Moreover, to provide a perception of flooding direction for a time
instance, we animate the surface mesh by synthesizing water waves. As
interaction is key for effective decision-making and analysis, we introduce two
novel techniques for flood visualization in immersive systems: (1) an automatic
scene-navigation method using optimal camera viewpoints generated for marked
points-of-interest based on the display layout, and (2) an AR-based
focus+context technique using an auxiliary display system. Submerse is
developed in collaboration between computer scientists and atmospheric
scientists. We evaluate the effectiveness of our system and application by
conducting workshops with emergency managers, domain experts, and concerned
stakeholders in the Stony Brook Reality Deck, an immersive gigapixel facility,
to visualize a superstorm flooding scenario in New York City.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffMesh: A Motion-aware Diffusion Framework for Human Mesh Recovery
  from Videos <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13397v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13397v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ce Zheng, Xianpeng Liu, Qucheng Peng, Tianfu Wu, Pu Wang, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human mesh recovery (HMR) provides rich human body information for various
real-world applications. While image-based HMR methods have achieved impressive
results, they often struggle to recover humans in dynamic scenarios, leading to
temporal inconsistencies and non-smooth 3D motion predictions due to the
absence of human motion. In contrast, video-based approaches leverage temporal
information to mitigate this issue. In this paper, we present DiffMesh, an
innovative motion-aware Diffusion-like framework for video-based HMR. DiffMesh
establishes a bridge between diffusion models and human motion, efficiently
generating accurate and smooth output mesh sequences by incorporating human
motion within the forward process and reverse process in the diffusion model.
Extensive experiments are conducted on the widely used datasets (Human3.6M
\cite{h36m_pami} and 3DPW \cite{pw3d2018}), which demonstrate the effectiveness
and efficiency of our DiffMesh. Visual comparisons in real-world scenarios
further highlight DiffMesh's suitability for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Holoview: Interactive 3D visualization of medical data in AR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pankaj Kaushik, Anshul Goswami, Ojaswa Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HoloView, an innovative augmented reality (AR) system that
enhances interactive learning of human anatomical structures through immersive
visualization. Combining advanced rendering techniques with intuitive
gesture-based interactions, HoloView provides a comprehensive technical
solution for medical education. The system architecture features a distributed
rendering pipeline that offloads stereoscopic computations to a remote server,
optimizing performance and enabling high-quality visualization on less powerful
devices. To prioritize visual quality in the user's direct line of sight while
reducing computational load, we implement foveated rendering optimization,
enhancing the immersive experience. Additionally, a hybrid surface-volume
rendering technique is used to achieve faster rendering speeds without
sacrificing visual fidelity. Complemented by a carefully designed user
interface and gesture-based interaction system, HoloView allows users to
naturally manipulate holographic content and seamlessly navigate the learning
environment. HoloView significantly facilitates anatomical structure
visualization and promotes an engaging, user-centric learning experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaboration in Immersive Environments: Challenges and Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00689v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00689v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahin Doroudian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in
all engineering fields in order to avoid the use of physical prototypes, to
train in high-risk situations, and to interpret real or simulated results. In
order to complete a shared task or assign tasks to the agents in such immersive
environments, collaboration or Shared Cooperative Activities are a necessity.
Collaboration in immersive environments is an emerging field of research that
aims to study and enhance the ways in which people interact and work together
in Virtual and Augmented Reality settings. Collaboration in immersive
environments is a complex process that involves different factors such as
communication, coordination, and social presence. This paper provides an
overview of the current state of research on collaboration in immersive
environments. It discusses the different types of immersive environments,
including VR and AR, and the different forms of collaboration that can occur in
these environments. The paper also highlights the challenges and limitations of
collaboration in immersive environments, such as the lack of physical cues,
cost and usability and the need for further research in this area. Overall,
collaboration in immersive environments is a promising field with a wide range
of potential applications, from education to industry, and it can benefit both
individuals and groups by enhancing their ability to work together effectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added new references in Networking section</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">32</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLAP-S: Support Set Based Adaptation for Downstream Fiber-optic Acoustic
  Recognition <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingchen Sun, Shaobo Han, Wataru Kohno, Changyou Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Audio Pretraining (CLAP) models have demonstrated
unprecedented performance in various acoustic signal recognition tasks.
Fiber-optic-based acoustic recognition is one of the most important downstream
tasks and plays a significant role in environmental sensing. Adapting CLAP for
fiber-optic acoustic recognition has become an active research area. As a
non-conventional acoustic sensor, fiber-optic acoustic recognition presents a
challenging, domain-specific, low-shot deployment environment with significant
domain shifts due to unique frequency response and noise characteristics. To
address these challenges, we propose a support-based adaptation method, CLAP-S,
which linearly interpolates a CLAP Adapter with the Support Set, leveraging
both implicit knowledge through fine-tuning and explicit knowledge retrieved
from memory for cross-domain generalization. Experimental results show that our
method delivers competitive performance on both laboratory-recorded fiber-optic
ESC-50 datasets and a real-world fiber-optic gunshot-firework dataset. Our
research also provides valuable insights for other downstream acoustic
recognition tasks. The code and gunshot-firework dataset are available at
https://github.com/Jingchensun/clap-s.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry-Preserving En<span class="highlight-title">code</span>r/De<span class="highlight-title">code</span>r in Latent Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjun Lee, Riley C. W. O'Neill, Dongmian Zou, Jeff Calder, Gilad Lerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling aims to generate new data samples that resemble a given
dataset, with diffusion models recently becoming the most popular generative
model. One of the main challenges of diffusion models is solving the problem in
the input space, which tends to be very high-dimensional. Recently, solving
diffusion models in the latent space through an encoder that maps from the data
space to a lower-dimensional latent space has been considered to make the
training process more efficient and has shown state-of-the-art results. The
variational autoencoder (VAE) is the most commonly used encoder/decoder
framework in this domain, known for its ability to learn latent representations
and generate data samples. In this paper, we introduce a novel encoder/decoder
framework with theoretical properties distinct from those of the VAE,
specifically designed to preserve the geometric structure of the data
distribution. We demonstrate the significant advantages of this
geometry-preserving encoder in the training process of both the encoder and
decoder. Additionally, we provide theoretical results proving convergence of
the training process, including convergence guarantees for encoder training,
and results showing faster convergence of decoder training when using the
geometry-preserving encoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An LLM-Guided Tutoring System for Social Skills Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Guevarra, Indronil Bhattacharjee, Srijita Das, Christabel Wayllace, Carrie Demmans Epp, Matthew E. Taylor, Alan Tay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social skills training targets behaviors necessary for success in social
interactions. However, traditional classroom training for such skills is often
insufficient to teach effective communication -- one-to-one interaction in
real-world scenarios is preferred to lecture-style information delivery. This
paper introduces a framework that allows instructors to collaborate with large
language models to dynamically design realistic scenarios for students to
communicate. Our framework uses these scenarios to enable student rehearsal,
provide immediate feedback, and visualize performance for both students and
instructors. Unlike traditional intelligent tutoring systems, instructors can
easily co-create scenarios with a large language model without technical
skills. Additionally, the system generates new scenario branches in real time
when existing options do not fit the student's response.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Explainability to Interpretability: Interpretable Policies in
  Reinforcement Learning Via Model Explanation <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peilang Li, Umer Siddique, Yongcan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (RL) has shown remarkable success in complex
domains, however, the inherent black box nature of deep neural network policies
raises significant challenges in understanding and trusting the decision-making
processes. While existing explainable RL methods provide local insights, they
fail to deliver a global understanding of the model, particularly in
high-stakes applications. To overcome this limitation, we propose a novel
model-agnostic approach that bridges the gap between explainability and
interpretability by leveraging Shapley values to transform complex deep RL
policies into transparent representations. The proposed approach offers two key
contributions: a novel approach employing Shapley values to policy
interpretation beyond local explanations and a general framework applicable to
off-policy and on-policy algorithms. We evaluate our approach with three
existing deep RL algorithms and validate its performance in two classic control
environments. The results demonstrate that our approach not only preserves the
original models' performance but also generates more stable interpretable
policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Deployable AI (DAI) Workshop at the Thirty-Ninth AAAI
  Conference on Artificial Intelligence (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Noisy Halfspaces with a Margin: Massart is No Harder than
  Random <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gautam Chandrasekaran, Vasilis Kontonis, Konstantinos Stavropoulos, Kevin Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of PAC learning $\gamma$-margin halfspaces with Massart
noise. We propose a simple proper learning algorithm, the Perspectron, that has
sample complexity $\widetilde{O}((\epsilon\gamma)^{-2})$ and achieves
classification error at most $\eta+\epsilon$ where $\eta$ is the Massart noise
rate. Prior works [DGT19,CKMY20] came with worse sample complexity guarantees
(in both $\epsilon$ and $\gamma$) or could only handle random classification
noise [DDK+23,KIT+23] -- a much milder noise assumption. We also show that our
results extend to the more challenging setting of learning generalized linear
models with a known link function under Massart noise, achieving a similar
sample complexity to the halfspace case. This significantly improves upon the
prior state-of-the-art in this setting due to [CKMY20], who introduced this
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared in NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Code</span>d Deep Learning: Framework and Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        En-hui Yang, Shayan Mohajer Hamidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of deep learning (DL) is often achieved with large models and
high complexity during both training and post-training inferences, hindering
training in resource-limited settings. To alleviate these issues, this paper
introduces a new framework dubbed ``coded deep learning'' (CDL), which
integrates information-theoretic coding concepts into the inner workings of DL,
to significantly compress model weights and activations, reduce computational
complexity at both training and post-training inference stages, and enable
efficient model/data parallelism. Specifically, within CDL, (i) we first
propose a novel probabilistic method for quantizing both model weights and
activations, and its soft differentiable variant which offers an analytic
formula for gradient calculation during training; (ii) both the forward and
backward passes during training are executed over quantized weights and
activations, eliminating most floating-point operations and reducing training
complexity; (iii) during training, both weights and activations are entropy
constrained so that they are compressible in an information-theoretic sense
throughout training, thus reducing communication costs in model/data
parallelism; and (iv) the trained model in CDL is by default in a quantized
format with compressible quantized weights, reducing post-training inference
and storage complexity. Additionally, a variant of CDL, namely relaxed CDL
(R-CDL), is presented to further improve the trade-off between validation
accuracy and compression though requiring full precision in training with other
advantageous features of CDL intact. Extensive empirical results show that CDL
and R-CDL outperform the state-of-the-art algorithms in DNN compression in the
literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ pFedWN: A Personalized Federated Learning Framework for D2D Wireless
  Networks with Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhou Ni, Masoud Ghazikor, Morteza Hashemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Federated Learning (FL) approaches often struggle with data
heterogeneity across clients, leading to suboptimal model performance for
individual clients. To address this issue, Personalized Federated Learning
(PFL) emerges as a solution to the challenges posed by non-independent and
identically distributed (non-IID) and unbalanced data across clients.
Furthermore, in most existing decentralized machine learning works, a perfect
communication channel is considered for model parameter transmission between
clients and servers. However, decentralized PFL over wireless links introduces
new challenges, such as resource allocation and interference management. To
overcome these challenges, we formulate a joint optimization problem that
incorporates the underlying device-to-device (D2D) wireless channel conditions
into a server-free PFL approach. The proposed method, dubbed pFedWN, optimizes
the learning performance for each client while accounting for the variability
in D2D wireless channels. To tackle the formulated problem, we divide it into
two sub-problems: PFL neighbor selection and PFL weight assignment. The PFL
neighbor selection is addressed through channel-aware neighbor selection within
unlicensed spectrum bands such as ISM bands. Next, to assign PFL weights, we
utilize the Expectation-Maximization (EM) method to evaluate the similarity
between clients' data and obtain optimal weight distribution among the chosen
PFL neighbors. Empirical results show that pFedWN provides efficient and
personalized learning performance with non-IID and unbalanced datasets.
Furthermore, it outperforms the existing FL and PFL methods in terms of
learning efficacy and robustness, particularly under dynamic and unpredictable
wireless channel conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures, 3 tables, submitted to Transactions on
  Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BN-Pool: a Bayesian Nonparametric Approach to Graph Pooling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Castellana, Filippo Maria Bianchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce BN-Pool, the first clustering-based pooling method for Graph
Neural Networks (GNNs) that adaptively determines the number of supernodes in a
coarsened graph. By leveraging a Bayesian non-parametric framework, BN-Pool
employs a generative model capable of partitioning graph nodes into an
unbounded number of clusters. During training, we learn the node-to-cluster
assignments by combining the supervised loss of the downstream task with an
unsupervised auxiliary term, which encourages the reconstruction of the
original graph topology while penalizing unnecessary proliferation of clusters.
This adaptive strategy allows BN-Pool to automatically discover an optimal
coarsening level, offering enhanced flexibility and removing the need to
specify sensitive pooling ratios. We show that BN-Pool achieves superior
performance across diverse benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Generalization in Chain of Thought Reasoning for Smaller
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxwell J. Yin, Dingyi Jiang, Yongbing Chen, Boyu Wang, Charles Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning in smaller language models is a challenging
natural language process problem yet highly desirable in many real-life
applications. Existing CoT knowledge distillation methods often suffer from
overly conservative memorization in smaller LLMs, leading to low generalization
confidence. As fully preserving the CoT ability of teacher model is impossible,
we hypothesize that adversarial CoT fine-tuning is crucial for developing
smaller LLM with robust CoT generalization. To this end, we propose
\textit{PRompt-Assisted Domain-Adversarial fine-tuning} (PRADA), a principled
fine-tuning framework that integrates diverse CoT domains. Specifically, PRADA
pioneers two CoT improvements in smaller LLM: (1) Recovering the
domain-invariant feature insight which typically lost during distillation with
domain adversarial fine-tuning; (2) Enhancing the domain adaptability of CoT
prompt engineering by employing domain-adversarial approaches. We theoretically
demonstrate the effectiveness of our approach and empirically show that it
significantly outperforms the state of the arts in a wide range of tasks.
Moreover, our empirical findings reveal that the smaller LLM, when leveraging
PRADA, aligns closely with domain knowledge, thereby improving the
explainability of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks for Travel Distance Estimation and Route
  Recommendation Under Probabilistic Hazards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Liu, Hadi Meidani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the shortest travel time and providing route recommendation
between different locations in a city or region can quantitatively measure the
conditions of the transportation network during or after extreme events. One
common approach is to use Dijkstra's Algorithm, which produces the shortest
path as well as the shortest distance. However, this option is computationally
expensive when applied to large-scale networks. This paper proposes a novel
fast framework based on graph neural networks (GNNs) which approximate the
single-source shortest distance between pairs of locations, and predict the
single-source shortest path subsequently. We conduct multiple experiments on
synthetic graphs of different size to demonstrate the feasibility and
computational efficiency of the proposed model. In real-world case studies, we
also applied the proposed method of flood risk analysis of coastal urban areas
to calculate delays in evacuation to public shelters during hurricanes. The
results indicate the accuracy and computational efficiency of the GNN model,
and its potential for effective implementation in emergency planning and
management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical
  Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexi Du, Jiazhen Zhang, Tal Zeevi, Nicha C. Dvornek, John A. Onofrey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional neural networks (CNNs) are essential tools for computer vision
tasks, but they lack traditionally desired properties of extracted features
that could further improve model performance, e.g., rotational equivariance.
Such properties are ubiquitous in biomedical images, which often lack explicit
orientation. While current work largely relies on data augmentation or explicit
modules to capture orientation information, this comes at the expense of
increased training costs or ineffective approximations of the desired
equivariance. To overcome these challenges, we propose a novel and efficient
implementation of the Symmetric Rotation-Equivariant (SRE) Convolution
(SRE-Conv) kernel, designed to learn rotation-invariant features while
simultaneously compressing the model size. The SRE-Conv kernel can easily be
incorporated into any CNN backbone. We validate the ability of a deep SRE-CNN
to capture equivariance to rotation using the public MedMNISTv2 dataset (16
total tasks). SRE-Conv-CNN demonstrated improved rotated image classification
performance accuracy on all 16 test datasets in both 2D and 3D images, all
while increasing efficiency with fewer parameters and reduced memory footprint.
The code is available at https://github.com/XYPB/SRE-Conv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ISBI 2025 4-page paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OmniThink: Expanding Knowledge Boundaries in Machine Writing through
  Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine writing with large language models often relies on
retrieval-augmented generation. However, these approaches remain confined
within the boundaries of the model's predefined scope, limiting the generation
of content with rich information. Specifically, vanilla-retrieved information
tends to lack depth, utility, and suffers from redundancy, which negatively
impacts the quality of generated articles, leading to shallow, repetitive, and
unoriginal outputs. To address these issues, we propose OmniThink, a machine
writing framework that emulates the human-like process of iterative expansion
and reflection. The core idea behind OmniThink is to simulate the cognitive
behavior of learners as they progressively deepen their knowledge of the
topics. Experimental results demonstrate that OmniThink improves the knowledge
density of generated articles without compromising metrics such as coherence
and depth. Human evaluations and expert feedback further highlight the
potential of OmniThink to address real-world challenges in the generation of
long-form articles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAST: Efficient Action Tokenization for Vision-Language-Action Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive sequence models, such as Transformer-based vision-language
action (VLA) policies, can be tremendously effective for capturing complex and
generalizable robotic behaviors. However, such models require us to choose a
tokenization of our continuous action signals, which determines how the
discrete symbols predicted by the model map to continuous robot actions. We
find that current approaches for robot action tokenization, based on simple
per-dimension, per-timestep binning schemes, typically perform poorly when
learning dexterous skills from high-frequency robot data. To address this
challenge, we propose a new compression-based tokenization scheme for robot
actions, based on the discrete cosine transform. Our tokenization approach,
Frequency-space Action Sequence Tokenization (FAST), enables us to train
autoregressive VLAs for highly dexterous and high-frequency tasks where
standard discretization methods fail completely. Based on FAST, we release
FAST+, a universal robot action tokenizer, trained on 1M real robot action
trajectories. It can be used as a black-box tokenizer for a wide range of robot
action sequences, with diverse action spaces and control frequencies. Finally,
we show that, when combined with the pi0 VLA, our method can scale to training
on 10k hours of robot data and match the performance of diffusion VLAs, while
reducing training time by up to 5x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://www.pi.website/research/fast</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Suggesting <span class="highlight-title">Code</span> Edits in Interactive Machine Learning Notebooks Using
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bihui Jin, Jiayue Wang, Pengyu Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning developers frequently use interactive computational
notebooks, such as Jupyter notebooks, to host code for data processing and
model training. Jupyter notebooks provide a convenient tool for writing machine
learning pipelines and interactively observing outputs, however, maintaining
Jupyter notebooks, e.g., to add new features or fix bugs, can be challenging
due to the length and complexity of the notebooks. Moreover, there is no
existing benchmark related to developer edits on Jupyter notebooks. To address
this, we present the first dataset of 48,398 Jupyter notebook edits derived
from 20,095 revisions of 792 machine learning repositories on GitHub, and
perform the first study of the using LLMs to predict code edits in Jupyter
notebooks. Our dataset captures granular details of cell-level and line-level
modifications, offering a foundation for understanding real-world maintenance
patterns in machine learning workflows. We observed that the edits on Jupyter
notebooks are highly localized, with changes averaging only 166 lines of code
in repositories. While larger models outperform smaller counterparts in code
editing, all models have low accuracy on our dataset even after finetuning,
demonstrating the complexity of real-world machine learning maintenance tasks.
Our findings emphasize the critical role of contextual information in improving
model performance and point toward promising avenues for advancing large
language models' capabilities in engineering machine learning code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random Subspace Cubic-Regularization Methods, with Applications to
  Low-Rank Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Coralia Cartis, Zhen Shao, Edward Tansley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose and analyze random subspace variants of the second-order Adaptive
Regularization using Cubics (ARC) algorithm. These methods iteratively restrict
the search space to some random subspace of the parameters, constructing and
minimizing a local model only within this subspace. Thus, our variants only
require access to (small-dimensional) projections of first- and second-order
problem derivatives and calculate a reduced step inexpensively. Under suitable
assumptions, the ensuing methods maintain the optimal first-order, and
second-order, global rates of convergence of (full-dimensional) cubic
regularization, while showing improved scalability both theoretically and
numerically, particularly when applied to low-rank functions. When applied to
the latter, our adaptive variant naturally adapts the subspace size to the true
rank of the function, without knowing it a priori.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictions as Surrogates: Revisiting Surrogate Outcomes in the Age of
  AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Ji, Lihua Lei, Tijana Zrnic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish a formal connection between the decades-old surrogate outcome
model in biostatistics and economics and the emerging field of
prediction-powered inference (PPI). The connection treats predictions from
pre-trained models, prevalent in the age of AI, as cost-effective surrogates
for expensive outcomes. Building on the surrogate outcomes literature, we
develop recalibrated prediction-powered inference, a more efficient approach to
statistical inference than existing PPI proposals. Our method departs from the
existing proposals by using flexible machine learning techniques to learn the
optimal ``imputed loss'' through a step we call recalibration. Importantly, the
method always improves upon the estimator that relies solely on the data with
available true outcomes, even when the optimal imputed loss is estimated
imperfectly, and it achieves the smallest asymptotic variance among PPI
estimators if the estimate is consistent. Computationally, our optimization
objective is convex whenever the loss function that defines the target
parameter is convex. We further analyze the benefits of recalibration, both
theoretically and numerically, in several common scenarios where machine
learning predictions systematically deviate from the outcome of interest. We
demonstrate significant gains in effective sample size over existing PPI
proposals via three applications leveraging state-of-the-art machine
learning/AI models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating particle physics Lagrangians with <span class="highlight-title">transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Sheng Koay, Rikard Enberg, Stefano Moretti, Eliel Camargo-Molina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In physics, Lagrangians provide a systematic way to describe laws governing
physical systems. In the context of particle physics, they encode the
interactions and behavior of the fundamental building blocks of our universe.
By treating Lagrangians as complex, rule-based constructs similar to linguistic
expressions, we trained a transformer model -- proven to be effective in
natural language tasks -- to predict the Lagrangian corresponding to a given
list of particles. We report on the transformer's performance in constructing
Lagrangians respecting the Standard Model $\mathrm{SU}(3)\times
\mathrm{SU}(2)\times \mathrm{U}(1)$ gauge symmetries. The resulting model is
shown to achieve high accuracies (over 90\%) with Lagrangians up to six matter
fields, with the capacity to generalize beyond the training distribution,
albeit within architectural constraints. We show through an analysis of input
embeddings that the model has internalized concepts such as group
representations and conjugation operations as it learned to generate
Lagrangians. We make the model and training datasets available to the
community. An interactive demonstration can be found at:
\url{https://huggingface.co/spaces/JoseEliel/generate-lagrangians}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 11 figues, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention based Bidirectional GRU hybrid model for inappropriate content
  detection in Urdu language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezzah Shoukat, Rabia Irfan, Iqra Basharat, Muhammad Ali Tahir, Sameen Shaukat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increased use of the internet and social networks for online
discussions, the spread of toxic and inappropriate content on social networking
sites has also increased. Several studies have been conducted in different
languages. However, there is less work done for South Asian languages for
inappropriate content identification using deep learning techniques. In Urdu
language, the spellings are not unique, and people write different common
spellings for the same word, while mixing it other languages, like English in
the text makes it more challenging, and limited research work is available to
process such language with the finest algorithms. The use of attention layer
with a deep learning model can help handling the long-term dependencies and
increase its efficiency . To explore the effects of the attention layer, this
study proposes attention-based Bidirectional GRU hybrid model for identifying
inappropriate content in Urdu Unicode text language. Four different baseline
deep learning models; LSTM, Bi-LSTM, GRU, and TCN, are used to compare the
performance of the proposed model. The results of these models were compared
based on evaluation metrics, dataset size, and impact of the word embedding
layer. The pre-trained Urdu word2Vec embeddings were utilized for our case. Our
proposed model BiGRU-A outperformed all other baseline models by yielding 84\%
accuracy without using pre-trained word2Vec layer. From our experiments, we
have established that the attention layer improves the model's efficiency, and
pre-trained word2Vec embedding does not work well with an inappropriate content
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical Continual Forgetting for <span class="highlight-title">Pre-train</span>ed Vision Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Zhao, Fei Zhu, Bolin Ni, Feng Zhu, Gaofeng Meng, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For privacy and security concerns, the need to erase unwanted information
from pre-trained vision models is becoming evident nowadays. In real-world
scenarios, erasure requests originate at any time from both users and model
owners, and these requests usually form a sequence. Therefore, under such a
setting, selective information is expected to be continuously removed from a
pre-trained model while maintaining the rest. We define this problem as
continual forgetting and identify three key challenges. (i) For unwanted
knowledge, efficient and effective deleting is crucial. (ii) For remaining
knowledge, the impact brought by the forgetting procedure should be minimal.
(iii) In real-world scenarios, the training samples may be scarce or partially
missing during the process of forgetting. To address them, we first propose
Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA
modules to fine-tune the FFN layers in Transformer blocks for each forgetting
task independently, and towards (ii), a simple group sparse regularization is
adopted, enabling automatic selection of specific LoRA groups and zeroing out
the others. To further extend GS-LoRA to more practical scenarios, we
incorporate prototype information as additional supervision and introduce a
more practical approach, GS-LoRA++. For each forgotten class, we move the
logits away from its original prototype. For the remaining classes, we pull the
logits closer to their respective prototypes. We conduct extensive experiments
on face recognition, object detection and image classification and demonstrate
that our method manages to forget specific classes with minimal impact on other
classes. Codes have been released on https://github.com/bjzhb666/GS-LoRA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cueless EEG imagined speech for subject identification: dataset and
  benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Derakhshesh, Zahra Dehghanian, Reza Ebrahimpour, Hamid R. Rabiee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalogram (EEG) signals have emerged as a promising modality for
biometric identification. While previous studies have explored the use of
imagined speech with semantically meaningful words for subject identification,
most have relied on additional visual or auditory cues. In this study, we
introduce a cueless EEG-based imagined speech paradigm, where subjects imagine
the pronunciation of semantically meaningful words without any external cues.
This innovative approach addresses the limitations of prior methods by
requiring subjects to select and imagine words from a predefined list
naturally. The dataset comprises over 4,350 trials from 11 subjects across five
sessions. We assess a variety of classification methods, including traditional
machine learning techniques such as Support Vector Machines (SVM) and XGBoost,
as well as time-series foundation models and deep learning architectures
specifically designed for EEG classification, such as EEG Conformer and Shallow
ConvNet. A session-based hold-out validation strategy was employed to ensure
reliable evaluation and prevent data leakage. Our results demonstrate
outstanding classification accuracy, reaching 97.93%. These findings highlight
the potential of cueless EEG paradigms for secure and reliable subject
identification in real-world applications, such as brain-computer interfaces
(BCIs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Near-optimal Algorithm for Learning Margin Halfspaces with Massart
  Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Diakonikolas, Nikos Zarifis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of PAC learning $\gamma$-margin halfspaces in the
presence of Massart noise. Without computational considerations, the sample
complexity of this learning problem is known to be
$\widetilde{\Theta}(1/(\gamma^2 \epsilon))$. Prior computationally efficient
algorithms for the problem incur sample complexity $\tilde{O}(1/(\gamma^4
\epsilon^3))$ and achieve 0-1 error of $\eta+\epsilon$, where $\eta<1/2$ is the
upper bound on the noise rate. Recent work gave evidence of an
information-computation tradeoff, suggesting that a quadratic dependence on
$1/\epsilon$ is required for computationally efficient algorithms. Our main
result is a computationally efficient learner with sample complexity
$\widetilde{\Theta}(1/(\gamma^2 \epsilon^2))$, nearly matching this lower
bound. In addition, our algorithm is simple and practical, relying on online
SGD on a carefully selected sequence of convex losses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer
  Depression Detection <span class="chip">ML4H</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaee Cheong, Aditya Bangar, Sinan Kalkan, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning bias in mental health is becoming an increasingly pertinent
challenge. Despite promising efforts indicating that multitask approaches often
work better than unitask approaches, there is minimal work investigating the
impact of multitask learning on performance and fairness in depression
detection nor leveraged it to achieve fairer prediction outcomes. In this work,
we undertake a systematic investigation of using a multitask approach to
improve performance and fairness for depression detection. We propose a novel
gender-based task-reweighting method using uncertainty grounded in how the
PHQ-8 questionnaire is structured. Our results indicate that, although a
multitask approach improves performance and fairness compared to a unitask
approach, the results are not always consistent and we see evidence of negative
transfer and a reduction in the Pareto frontier, which is concerning given the
high-stake healthcare setting. Our proposed approach of gender-based
reweighting with uncertainty improves performance and fairness and alleviates
both challenges to a certain extent. Our findings on each PHQ-8 subitem task
difficulty are also in agreement with the largest study conducted on the PHQ-8
subitem discrimination capacity, thus providing the very first tangible
evidence linking ML findings with large-scale empirical population studies
conducted on the PHQ-8.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at the Proceedings of Machine Learning Research 259, 1-14,
  2024 as part of the Machine Learning for Health (ML4H) Symposium 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reward-Guided Controlled Generation for Inference-Time Alignment in
  Diffusion Models: Tutorial and <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masatoshi Uehara, Yulai Zhao, Chenyu Wang, Xiner Li, Aviv Regev, Sergey Levine, Tommaso Biancalani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This tutorial provides an in-depth guide on inference-time guidance and
alignment methods for optimizing downstream reward functions in diffusion
models. While diffusion models are renowned for their generative modeling
capabilities, practical applications in fields such as biology often require
sample generation that maximizes specific metrics (e.g., stability, affinity in
proteins, closeness to target structures). In these scenarios, diffusion models
can be adapted not only to generate realistic samples but also to explicitly
maximize desired measures at inference time without fine-tuning. This tutorial
explores the foundational aspects of such inference-time algorithms. We review
these methods from a unified perspective, demonstrating that current techniques
-- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling,
and classifier guidance -- aim to approximate soft optimal denoising processes
(a.k.a. policies in RL) that combine pre-trained denoising processes with value
functions serving as look-ahead functions that predict from intermediate states
to terminal rewards. Within this framework, we present several novel algorithms
not yet covered in the literature. Furthermore, we discuss (1) fine-tuning
methods combined with inference-time techniques, (2) inference-time algorithms
based on search algorithms such as Monte Carlo tree search, which have received
limited attention in current research, and (3) connections between
inference-time algorithms in language models and diffusion models. The code of
this tutorial on protein design is available at
https://github.com/masa-ue/AlignInversePro
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We plan to add more content/codes. Please let us know if there are
  any comments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Dark Side of Rich Rewards: Understanding and Mitigating Noise in VLM
  Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15922v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15922v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sukai Huang, Shu-Wei Liu, Nir Lipovetzky, Trevor Cohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Vision-Language Models (VLMs) are increasingly used to generate reward
signals for training embodied agents to follow instructions, our research
reveals that agents guided by VLM rewards often underperform compared to those
employing only intrinsic (exploration-driven) rewards, contradicting
expectations set by recent work. We hypothesize that false positive rewards --
instances where unintended trajectories are incorrectly rewarded -- are more
detrimental than false negatives. Our analysis confirms this hypothesis,
revealing that the widely used cosine similarity metric is prone to false
positive reward estimates. To address this, we introduce BiMI ({Bi}nary
{M}utual {I}nformation), a novel reward function designed to mitigate noise.
BiMI significantly enhances learning efficiency across diverse and challenging
embodied navigation environments. Our findings offer a nuanced understanding of
how different types of reward noise impact agent learning and highlight the
importance of addressing multimodal reward signal noise when training embodied
agents
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 main body pages, 21 appendix pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Mechanistic Explanatory Strategy for XAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01332v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01332v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Rabiza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in XAI, scholars note a persistent lack of
solid conceptual foundations and integration with broader scientific discourse
on explanation. In response, emerging XAI research draws on explanatory
strategies from various sciences and philosophy of science literature to fill
these gaps. This paper outlines a mechanistic strategy for explaining the
functional organization of deep learning systems, situating recent advancements
in AI explainability within a broader philosophical context. According to the
mechanistic approach, the explanation of opaque AI systems involves identifying
mechanisms that drive decision-making. For deep neural networks, this means
discerning functionally relevant components -- such as neurons, layers,
circuits, or activation patterns -- and understanding their roles through
decomposition, localization, and recomposition. Proof-of-principle case studies
from image recognition and language modeling align these theoretical approaches
with the latest research from AI labs like OpenAI and Anthropic. This research
suggests that a systematic approach to studying model organization can reveal
elements that simpler (or ''more modest'') explainability techniques might
miss, fostering more thoroughly explainable AI. The paper concludes with a
discussion on the epistemic relevance of the mechanistic approach positioned in
the context of selected philosophical debates on XAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming in M\"uller, V. C., Dewey, A. R., Dung, L., & L\"ohr, G.
  (Eds.), Philosophy of Artificial Intelligence: The State of the Art, Synthese
  Library, Berlin: Springer Nature. Please cite the published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference-based Pure Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apurv Shukla, Debabrota Basu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the preference-based pure exploration problem for bandits with
vector-valued rewards. The rewards are ordered using a (given) preference cone
$\mathcal{C}$ and our goal is to identify the set of Pareto optimal arms.
First, to quantify the impact of preferences, we derive a novel lower bound on
sample complexity for identifying the most preferred policy with a confidence
level $1-\delta$. Our lower bound elicits the role played by the geometry of
the preference cone and punctuates the difference in hardness compared to
existing best-arm identification variants of the problem. We further explicate
this geometry when the rewards follow Gaussian distributions. We then provide a
convex relaxation of the lower bound and leverage it to design the
Preference-based Track and Stop (PreTS) algorithm that identifies the most
preferred policy. Finally, we show that the sample complexity of PreTS is
asymptotically tight by deriving a new concentration inequality for
vector-valued rewards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intelligent Icing Detection Model of Wind Turbine Blades Based on SCADA
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.07914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.07914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqian Jiang, Junyang Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diagnosis of ice accretion on wind turbine blades is all the time a hard nut
to crack in condition monitoring of wind farms. Existing methods focus on
mechanism analysis of icing process, deviation degree analysis of feature
engineering. However, there have not been deep researches of neural networks
applied in this field at present. Supervisory control and data acquisition
(SCADA) makes it possible to train networks through continuously providing not
only operation parameters and performance parameters of wind turbines but also
environmental parameters and operation modes. This paper explores the
possibility that using convolutional neural networks (CNNs), generative
adversarial networks (GANs) and domain adaption learning to establish
intelligent diagnosis frameworks under different training scenarios.
Specifically, PGANC and PGANT are proposed for sufficient and non-sufficient
target wind turbine labeled data, respectively. The basic idea is that we
consider a two-stage training with parallel GANs, which are aimed at capturing
intrinsic features for normal and icing samples, followed by classification CNN
or domain adaption module in various training cases. Model validation on three
wind turbine SCADA data shows that two-stage training can effectively improve
the model performance. Besides, if there is no sufficient labeled data for a
target turbine, which is an extremely common phenomenon in real industrial
practices, the addition of domain adaption learning makes the trained model
show better performance. Overall, our proposed intelligent diagnosis frameworks
can achieve more accurate detection on the same wind turbine and more
generalized capability on a new wind turbine, compared with other machine
learning models and conventional CNNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-hop Upstream Anticipatory Traffic Signal Control with Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07271v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07271v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaocan Li, Xiaoyu Wang, Ilia Smirnov, Scott Sanner, Baher Abdulhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordination in traffic signal control is crucial for managing congestion in
urban networks. Existing pressure-based control methods focus only on immediate
upstream links, leading to suboptimal green time allocation and increased
network delays. However, effective signal control inherently requires
coordination across a broader spatial scope, as the effect of upstream traffic
should influence signal control decisions at downstream intersections,
impacting a large area in the traffic network. Although agent communication
using neural network-based feature extraction can implicitly enhance spatial
awareness, it significantly increases the learning complexity, adding an
additional layer of difficulty to the challenging task of control in deep
reinforcement learning. To address the issue of learning complexity and myopic
traffic pressure definition, our work introduces a novel concept based on
Markov chain theory, namely \textit{multi-hop upstream pressure}, which
generalizes the conventional pressure to account for traffic conditions beyond
the immediate upstream links. This farsighted and compact metric informs the
deep reinforcement learning agent to preemptively clear the multi-hop upstream
queues, guiding the agent to optimize signal timings with a broader spatial
awareness. Simulations on synthetic and realistic (Toronto) scenarios
demonstrate controllers utilizing multi-hop upstream pressure significantly
reduce overall network delay by prioritizing traffic movements based on a
broader understanding of upstream congestion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 tables, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Alignment Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satchel Grant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When can we say that two neural systems are the same? The answer to this
question is goal-dependent, and it is often addressed through correlative
methods such as Representational Similarity Analysis (RSA) and Centered Kernel
Alignment (CKA). What do we miss when we forgo causal explorations, and how can
we target specific types of similarity? In this work, we introduce Model
Alignment Search (MAS), a method for causally exploring distributed
representational similarity. The method learns invertible linear
transformations that align a subspace between two distributed networks'
representations where causal information can be freely interchanged. We first
show that the method can be used to transfer specific causal variables, such as
the number of items in a counting task, between networks with different
training seeds. We then explore open questions in number cognition by comparing
different types of numeric representations in models trained on structurally
different numeric tasks. We then explore differences between MAS vs preexisting
causal similarity methods, and lastly, we introduce a counterfactual latent
auxiliary loss function that helps shape causally relevant alignments even in
cases where we do not have causal access to one of the two models for training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforcement learning with non-ergodic reward increments: robustness
  via ergodicity transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11335v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11335v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Baumann, Erfaun Noorani, James Price, Ole Peters, Colm Connaughton, Thomas B. Schön
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Envisioned application areas for reinforcement learning (RL) include
autonomous driving, precision agriculture, and finance, which all require RL
agents to make decisions in the real world. A significant challenge hindering
the adoption of RL methods in these domains is the non-robustness of
conventional algorithms. In particular, the focus of RL is typically on the
expected value of the return. The expected value is the average over the
statistical ensemble of infinitely many trajectories, which can be
uninformative about the performance of the average individual. For instance,
when we have a heavy-tailed return distribution, the ensemble average can be
dominated by rare extreme events. Consequently, optimizing the expected value
can lead to policies that yield exceptionally high returns with a probability
that approaches zero but almost surely result in catastrophic outcomes in
single long trajectories. In this paper, we develop an algorithm that lets RL
agents optimize the long-term performance of individual trajectories. The
algorithm enables the agents to learn robust policies, which we show in an
instructive example with a heavy-tailed return distribution and standard RL
benchmarks. The key element of the algorithm is a transformation that we learn
from data. This transformation turns the time series of collected returns into
one for whose increments expected value and the average over a long trajectory
coincide. Optimizing these increments results in robust policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted final version to appear in the Transactions on Machine
  Learning Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithmic Collective Action in Recommender Systems: Promoting Songs by
  Reordering Playlists <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joachim Baumann, Celestine Mendler-Dünner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate algorithmic collective action in transformer-based recommender
systems. Our use case is a music streaming platform where a collective of fans
aims to promote the visibility of an underrepresented artist by strategically
placing one of their songs in the existing playlists they control. We introduce
two easily implementable strategies to select the position at which to insert
the song with the goal to boost recommendations at test time. The strategies
exploit statistical properties of the learner by targeting discontinuities in
the recommendations, and leveraging the long-tail nature of song distributions.
We evaluate the efficacy of our strategies using a publicly available
recommender system model released by a major music streaming platform. Our
findings reveal that through strategic placement even small collectives
(controlling less than 0.01\% of the training data) can achieve up to
$40\times$ more test time recommendations than an average song with the same
number of training set occurrences. Focusing on the externalities of the
strategy, we find that the recommendations of other songs are largely
preserved, and the newly gained recommendations are distributed across various
artists. Together, our findings demonstrate how carefully designed collective
action strategies can be effective while not necessarily being adversarial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at NeurIPS 2024, camera-ready updates</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Machine Learning to Discover Parsimonious and
  Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff
  Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan-Heng Wang, Hoshin V. Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the excellent real-world predictive performance of modern machine
learning (ML) methods, many scientists remain hesitant to discard traditional
physical-conceptual (PC) approaches due mainly to their relative
interpretability, which contributes to credibility during decision-making. In
this context, a currently underexplored aspect of ML is how to develop
minimally-optimal representations that can facilitate better insight regarding
system functioning. Regardless of how this is achieved, it is arguably true
that parsimonious representations better support the advancement of scientific
understanding. Our own view is that ML-based modeling of geoscientific systems
should be based in the use of computational units that are fundamentally
interpretable by design.
  This paper continues our exploration of how the strengths of ML can be
exploited in the service of better understanding via scientific investigation.
Here, we use the Mass Conserving Perceptron (MCP) as the fundamental
computational unit in a generic network architecture consisting of nodes
arranged in series and parallel to explore several generic and important issues
related to the use of observational data for constructing input-state-output
models of dynamical systems. In the context of lumped catchment modeling, we
show that physical interpretability and excellent predictive performance can
both be achieved using a relatively parsimonious distributed-state
multiple-flow-path network with context-dependent gating and information
sharing across the nodes, suggesting that MCP-based modeling can play a
significant role in application of ML to geoscientific investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>74 Pages, 4 Tables, 13 Figures, 11 Tables and 11 Figures in
  Supplementary Materials</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-15T00:00:00Z">2025-01-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">27</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Veln(ia)s is in the Details: Evaluating LLM Judgment on Latvian and
  Lithuanian Short Answer Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yevhen Kostiuk, Oxana Vitman, Łukasz Gagała, Artur Kiulian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we address the challenge of evaluating large language models
(LLMs) on the short answer matching task for Latvian and Lithuanian languages.
We introduce novel datasets consisting of 502 Latvian and 690 Lithuanian
question-answer pairs. For each question-answer pair, we generated matched and
non-matched answers using a set of alteration rules specifically designed to
introduce small but meaningful changes in the text. These generated answers
serve as test cases to assess the ability of LLMs to detect subtle differences
in matching of the original answers. A subset of the datasets was manually
verified for quality and accuracy. Our results show that while larger LLMs,
such as QWEN2.5 72b and LLaMa3.1 70b, demonstrate near-perfect performance in
distinguishing matched and non-matched answers, smaller models show more
variance. For instance, LLaMa3.1 8b and EuroLLM 9b benefited from few-shot
examples, while Mistral Nemo 12b underperformed on detection of subtle text
alteration, particularly in Lithuanian, even with additional examples. QWEN2.5
7b and Mistral 7b were able to obtain a strong and comparable performance to
the larger 70b models in zero and few shot experiments. Moreover, the
performance of Mistral 7b was weaker in few shot experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating GenAI for Simplifying Texts for Education: Improving Accuracy
  and Consistency for Enhanced Readability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephanie L. Day, Jacapo Cirica, Steven R. Clapp, Veronika Penkova, Amy E. Giroux, Abbey Banta, Catherine Bordeau, Poojitha Mutteneni, Ben D. Sawyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence (GenAI) holds great promise as a tool to
support personalized learning. Teachers need tools to efficiently and
effectively enhance content readability of educational texts so that they are
matched to individual students reading levels, while retaining key details.
Large Language Models (LLMs) show potential to fill this need, but previous
research notes multiple shortcomings in current approaches. In this study, we
introduced a generalized approach and metrics for the systematic evaluation of
the accuracy and consistency in which LLMs, prompting techniques, and a novel
multi-agent architecture to simplify sixty informational reading passages,
reducing each from the twelfth grade level down to the eighth, sixth, and
fourth grade levels. We calculated the degree to which each LLM and prompting
technique accurately achieved the targeted grade level for each passage,
percentage change in word count, and consistency in maintaining keywords and
key phrases (semantic similarity). One-sample t-tests and multiple regression
models revealed significant differences in the best performing LLM and prompt
technique for each of the four metrics. Both LLMs and prompting techniques
demonstrated variable utility in grade level accuracy and consistency of
keywords and key phrases when attempting to level content down to the fourth
grade reading level. These results demonstrate the promise of the application
of LLMs for efficient and precise automated text simplification, the
shortcomings of current models and prompting methods in attaining an ideal
balance across various evaluation criteria, and a generalizable method to
evaluate future systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages, 9 tables, 6 figures, and supplemental materials</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VCRScore: Image captioning metric based on V\&L <span class="highlight-title">Transformer</span>s, CLIP, and
  precision-recall 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Ruiz, Tania Ramírez, Daniela Moctezuma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning has become an essential Vision & Language research task. It
is about predicting the most accurate caption given a specific image or video.
The research community has achieved impressive results by continuously
proposing new models and approaches to improve the overall model's performance.
Nevertheless, despite increasing proposals, the performance metrics used to
measure their advances have remained practically untouched through the years. A
probe of that, nowadays metrics like BLEU, METEOR, CIDEr, and ROUGE are still
very used, aside from more sophisticated metrics such as BertScore and
ClipScore.
  Hence, it is essential to adjust how are measure the advances, limitations,
and scopes of the new image captioning proposals, as well as to adapt new
metrics to these new advanced image captioning approaches.
  This work proposes a new evaluation metric for the image captioning problem.
To do that, first, it was generated a human-labeled dataset to assess to which
degree the captions correlate with the image's content. Taking these human
scores as ground truth, we propose a new metric, and compare it with several
well-known metrics, from classical to newer ones. Outperformed results were
also found, and interesting insights were presented and discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Multilingual LLM Evaluation for Baltic and Nordic languages: A
  study on Lithuanian History 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yevhen Kostiuk, Oxana Vitman, Łukasz Gagała, Artur Kiulian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we evaluated Lithuanian and general history knowledge of
multilingual Large Language Models (LLMs) on a multiple-choice
question-answering task. The models were tested on a dataset of Lithuanian
national and general history questions translated into Baltic, Nordic, and
other languages (English, Ukrainian, Arabic) to assess the knowledge sharing
from culturally and historically connected groups. We evaluated GPT-4o,
LLaMa3.1 8b and 70b, QWEN2.5 7b and 72b, Mistral Nemo 12b, LLaMa3 8b, Mistral
7b, LLaMa3.2 3b, and Nordic fine-tuned models (GPT-SW3 and LLaMa3 8b).
  Our results show that GPT-4o consistently outperformed all other models
across language groups, with slightly better results for Baltic and Nordic
languages. Larger open-source models like QWEN2.5 72b and LLaMa3.1 70b
performed well but showed weaker alignment with Baltic languages. Smaller
models (Mistral Nemo 12b, LLaMa3.2 3b, QWEN 7B, LLaMa3.1 8B, and LLaMa3 8b)
demonstrated gaps with LT-related alignment with Baltic languages while
performing better on Nordic and other languages. The Nordic fine-tuned models
did not surpass multilingual models, indicating that shared cultural or
historical context alone does not guarantee better performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agentic Retrieval-Augmented Generation: A <span class="highlight-title">Survey</span> on Agentic RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized artificial intelligence (AI)
by enabling human like text generation and natural language understanding.
However, their reliance on static training data limits their ability to respond
to dynamic, real time queries, resulting in outdated or inaccurate outputs.
Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs
by integrating real time data retrieval to provide contextually relevant and
up-to-date responses. Despite its promise, traditional RAG systems are
constrained by static workflows and lack the adaptability required for
multistep reasoning and complex task management.
  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these
limitations by embedding autonomous AI agents into the RAG pipeline. These
agents leverage agentic design patterns reflection, planning, tool use, and
multiagent collaboration to dynamically manage retrieval strategies,
iteratively refine contextual understanding, and adapt workflows to meet
complex task requirements. This integration enables Agentic RAG systems to
deliver unparalleled flexibility, scalability, and context awareness across
diverse applications.
  This survey provides a comprehensive exploration of Agentic RAG, beginning
with its foundational principles and the evolution of RAG paradigms. It
presents a detailed taxonomy of Agentic RAG architectures, highlights key
applications in industries such as healthcare, finance, and education, and
examines practical implementation strategies. Additionally, it addresses
challenges in scaling these systems, ensuring ethical decision making, and
optimizing performance for real-world applications, while providing detailed
insights into frameworks and tools for implementing Agentic RAG
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Robustness of Contrastive Learning Models for Medical
  Image-Report Retrieval <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Demetrio Deanda, Yuktha Priya Masupalli, Jeong Yang, Young Lee, Zechun Cao, Gongbo Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical images and reports offer invaluable insights into patient health. The
heterogeneity and complexity of these data hinder effective analysis. To bridge
this gap, we investigate contrastive learning models for cross-domain
retrieval, which associates medical images with their corresponding clinical
reports. This study benchmarks the robustness of four state-of-the-art
contrastive learning models: CLIP, CXR-RePaiR, MedCLIP, and CXR-CLIP. We
introduce an occlusion retrieval task to evaluate model performance under
varying levels of image corruption. Our findings reveal that all evaluated
models are highly sensitive to out-of-distribution data, as evidenced by the
proportional decrease in performance with increasing occlusion levels. While
MedCLIP exhibits slightly more robustness, its overall performance remains
significantly behind CXR-CLIP and CXR-RePaiR. CLIP, trained on a
general-purpose dataset, struggles with medical image-report retrieval,
highlighting the importance of domain-specific training data. The evaluation of
this work suggests that more effort needs to be spent on improving the
robustness of these models. By addressing these limitations, we can develop
more reliable cross-domain retrieval models for medical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is accepted to AAAI 2025 Workshop -- the 9th International
  Workshop on Health Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual LLMs Struggle to Link Orthography and Semantics in
  Bilingual Word Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eshaan Tanwar, Gayatri Oke, Tanmoy Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilingual lexical processing is shaped by the complex interplay of
phonological, orthographic, and semantic features of two languages within an
integrated mental lexicon. In humans, this is evident in the ease with which
cognate words - words similar in both orthographic form and meaning (e.g.,
blind, meaning "sightless" in both English and German) - are processed,
compared to the challenges posed by interlingual homographs, which share
orthographic form but differ in meaning (e.g., gift, meaning "present" in
English but "poison" in German). We investigate how multilingual Large Language
Models (LLMs) handle such phenomena, focusing on English-Spanish,
English-French, and English-German cognates, non-cognate, and interlingual
homographs. Specifically, we evaluate their ability to disambiguate meanings
and make semantic judgments, both when these word types are presented in
isolation or within sentence contexts. Our findings reveal that while certain
LLMs demonstrate strong performance in recognizing cognates and non-cognates in
isolation, they exhibit significant difficulty in disambiguating interlingual
homographs, often performing below random baselines. This suggests LLMs tend to
rely heavily on orthographic similarities rather than semantic understanding
when interpreting interlingual homographs. Further, we find LLMs exhibit
difficulty in retrieving word meanings, with performance in isolative
disambiguation tasks having no correlation with semantic understanding.
Finally, we study how the LLM processes interlingual homographs in incongruent
sentences. We find models to opt for different strategies in understanding
English and non-English homographs, highlighting a lack of a unified approach
to handling cross-lingual ambiguities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at:
  https://github.com/EshaanT/Bilingual_processing_LLMs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting Human-Annotated Training Data with <span class="highlight-title">Large Language Model</span>
  Generation and Distillation in Open-Response Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conrad Borchers, Danielle R. Thomas, Jionghao Lin, Ralph Abboud, Kenneth R. Koedinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like GPT-4o can help automate text
classification tasks at low cost and scale. However, there are major concerns
about the validity and reliability of LLM outputs. By contrast, human coding is
generally more reliable but expensive to procure at scale. In this study, we
propose a hybrid solution to leverage the strengths of both. We combine
human-coded data and synthetic LLM-produced data to fine-tune a classical
machine learning classifier, distilling both into a smaller BERT model. We
evaluate our method on a human-coded test set as a validity measure for LLM
output quality. In three experiments, we systematically vary LLM-generated
samples' size, variety, and consistency, informed by best practices in LLM
tuning. Our findings indicate that augmenting datasets with synthetic samples
improves classifier performance, with optimal results achieved at an 80%
synthetic to 20% human-coded data ratio. Lower temperature settings of 0.3,
corresponding to less variability in LLM generations, produced more stable
improvements but also limited model learning from augmented samples. In
contrast, higher temperature settings (0.7 and above) introduced greater
variability in performance estimates and, at times, lower performance. Hence,
LLMs may produce more uniform output that classifiers overfit to earlier or
produce more diverse output that runs the risk of deteriorating model
performance through information irrelevant to the prediction task. Filtering
out inconsistent synthetic samples did not enhance performance. We conclude
that integrating human and LLM-generated data to improve text classification
models in assessment offers a scalable solution that leverages both the
accuracy of human coding and the variety of LLM outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript accepted to the Second Workshop on Generative AI for
  Learning Analytics (GenAI-LA) at LAK25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteLLA: A Structured Grading System Using LLMs with RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hefei Qiu, Brian White, Ashley Ding, Reinaldo Costa, Ali Hachem, Wei Ding, Ping Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown strong general capabilities in many
applications. However, how to make them reliable tools for some specific tasks
such as automated short answer grading (ASAG) remains a challenge. We present
SteLLA (Structured Grading System Using LLMs with RAG) in which a) Retrieval
Augmented Generation (RAG) approach is used to empower LLMs specifically on the
ASAG task by extracting structured information from the highly relevant and
reliable external knowledge based on the instructor-provided reference answer
and rubric, b) an LLM performs a structured and question-answering-based
evaluation of student answers to provide analytical grades and feedback. A
real-world dataset that contains students' answers in an exam was collected
from a college-level Biology course. Experiments show that our proposed system
can achieve substantial agreement with the human grader while providing
break-down grades and feedback on all the knowledge points examined in the
problem. A qualitative and error analysis of the feedback generated by GPT4
shows that GPT4 is good at capturing facts while may be prone to inferring too
much implication from the given text in the grading task which provides
insights into the usage of LLMs in the ASAG system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal LLMs Can Reason about Aesthetics in Zero-Shot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Jiang, Changwen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability
shall be elicited to evaluate the aesthetics of artworks. To facilitate this
investigation, we construct MM-StyleBench, a novel high-quality dataset for
benchmarking artistic stylization. We then develop a principled method for
human preference modeling and perform a systematic correlation analysis between
MLLMs' responses and human preference. Our experiments reveal an inherent
hallucination issue of MLLMs in art evaluation, associated with response
subjectivity. ArtCoT is proposed, demonstrating that art-specific task
decomposition and the use of concrete language boost MLLMs' reasoning ability
for aesthetics. Our findings offer valuable insights into MLLMs for art and can
benefit a wide range of downstream applications, such as style transfer and
artistic image generation. Code available at
https://github.com/songrise/MLLM4Art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WIP, Homepage https://github.com/songrise/MLLM4Art</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decompose-ToM: Enhancing Theory of Mind Reasoning in Large Language
  Models through Simulation and Task Decomposition <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sneheel Sarangi, Maha Elgarf, Hanan Salam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theory of Mind (ToM) is the ability to understand and reflect on the mental
states of others. Although this capability is crucial for human interaction,
testing on Large Language Models (LLMs) reveals that they possess only a
rudimentary understanding of it. Although the most capable closed-source LLMs
have come close to human performance on some ToM tasks, they still perform
poorly on complex variations of the task that involve more structured
reasoning. In this work, we utilize the concept of "pretend-play", or
``Simulation Theory'' from cognitive psychology to propose ``Decompose-ToM'':
an LLM-based inference algorithm that improves model performance on complex ToM
tasks. We recursively simulate user perspectives and decompose the ToM task
into a simpler set of functions: subject identification, question-reframing,
world model updation, and knowledge availability. We test the algorithm on
higher-order ToM tasks and a task testing for ToM capabilities in a
conversational setting, demonstrating that our approach shows significant
improvement across models compared to baseline methods while requiring minimal
prompt tuning across tasks and no additional model training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment
  of LLM Guardrails 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian Rebedea, Jibin Rajan Varghese, Christopher Parisien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) and generative AI become increasingly
widespread, concerns about content safety have grown in parallel. Currently,
there is a clear lack of high-quality, human-annotated datasets that address
the full spectrum of LLM-related safety risks and are usable for commercial
applications. To bridge this gap, we propose a comprehensive and adaptable
taxonomy for categorizing safety risks, structured into 12 top-level hazard
categories with an extension to 9 fine-grained subcategories. This taxonomy is
designed to meet the diverse requirements of downstream users, offering more
granular and flexible tools for managing various risk types. Using a hybrid
data generation pipeline that combines human annotations with a multi-LLM
"jury" system to assess the safety of responses, we obtain Aegis 2.0, a
carefully curated collection of 34,248 samples of human-LLM interactions,
annotated according to our proposed taxonomy. To validate its effectiveness, we
demonstrate that several lightweight models, trained using parameter-efficient
techniques on Aegis 2.0, achieve performance competitive with leading safety
models fully fine-tuned on much larger, non-commercial datasets. In addition,
we introduce a novel training blend that combines safety with topic following
data.This approach enhances the adaptability of guard models, enabling them to
generalize to new risk categories defined during inference. We plan to
open-source Aegis 2.0 data and models to the research community to aid in the
safety guardrailing of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2404.05993</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personality Modeling for Persuasion of Misinformation using AI Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianmin Lou, Wentao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of misinformation on social media platforms has highlighted
the need to understand how individual personality traits influence
susceptibility to and propagation of misinformation. This study employs an
innovative agent-based modeling approach to investigate the relationship
between personality traits and misinformation dynamics. Using six AI agents
embodying different dimensions of the Big Five personality traits
(Extraversion, Agreeableness, and Neuroticism), we simulated interactions
across six diverse misinformation topics. The experiment, implemented through
the AgentScope framework using the GLM-4-Flash model, generated 90 unique
interactions, revealing complex patterns in how personality combinations affect
persuasion and resistance to misinformation. Our findings demonstrate that
analytical and critical personality traits enhance effectiveness in
evidence-based discussions, while non-aggressive persuasion strategies show
unexpected success in misinformation correction. Notably, agents with critical
traits achieved a 59.4% success rate in HIV-related misinformation discussions,
while those employing non-aggressive approaches maintained consistent
persuasion rates above 40% across different personality combinations. The study
also revealed a non-transitive pattern in persuasion effectiveness, challenging
conventional assumptions about personality-based influence. These results
provide crucial insights for developing personality-aware interventions in
digital environments and suggest that effective misinformation countermeasures
should prioritize emotional connection and trust-building over confrontational
approaches. The findings contribute to both theoretical understanding of
personality-misinformation dynamics and practical strategies for combating
misinformation in social media contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Extract Cross-Domain Aspects and Understanding Sentiments
  Using <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karukriti Kaushik Ghosh, Chiranjib Sur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment
analysis that aims to extract and classify sentiments based on specific aspects
or features of a product, service, or entity. Unlike traditional sentiment
analysis, which assigns a general sentiment score to entire reviews or texts,
ABSA focuses on breaking down the text into individual components or aspects
(e.g., quality, price, service) and evaluating the sentiment towards each. This
allows for a more granular level of understanding of customer opinions,
enabling businesses to pinpoint specific areas of strength and improvement. The
process involves several key steps, including aspect extraction, sentiment
classification, and aspect-level sentiment aggregation for a review paragraph
or any other form that the users have provided. ABSA has significant
applications in areas such as product reviews, social media monitoring,
customer feedback analysis, and market research. By leveraging techniques from
natural language processing (NLP) and machine learning, ABSA facilitates the
extraction of valuable insights, enabling companies to make data-driven
decisions that enhance customer satisfaction and optimize offerings. As ABSA
evolves, it holds the potential to greatly improve personalized customer
experiences by providing a deeper understanding of sentiment across various
product aspects. In this work, we have analyzed the strength of LLMs for a
complete cross-domain aspect-based sentiment analysis with the aim of defining
the framework for certain products and using it for other similar situations.
We argue that it is possible to that at an effectiveness of 92\% accuracy for
the Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying General Turn-taking Models to Conversational Human-Robot
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Skantze, Bahar Irfan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Turn-taking is a fundamental aspect of conversation, but current Human-Robot
Interaction (HRI) systems often rely on simplistic, silence-based models,
leading to unnatural pauses and interruptions. This paper investigates, for the
first time, the application of general turn-taking models, specifically TurnGPT
and Voice Activity Projection (VAP), to improve conversational dynamics in HRI.
These models are trained on human-human dialogue data using self-supervised
learning objectives, without requiring domain-specific fine-tuning. We propose
methods for using these models in tandem to predict when a robot should begin
preparing responses, take turns, and handle potential interruptions. We
evaluated the proposed system in a within-subject study against a traditional
baseline system, using the Furhat robot with 39 adults in a conversational
setting, in combination with a large language model for autonomous response
generation. The results show that participants significantly prefer the
proposed system, and it significantly reduces response delays and
interruptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at HRI 2025 (the IEEE/ACM International Conference on
  Human-Robot Interaction)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Exploration of <span class="highlight-title">Large Language Model</span>s by Optimal
  Exploitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Grams, Patrick Betz, Christian Bartelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploration is a crucial skill for self-improvement and open-ended
problem-solving. However, it remains uncertain whether large language models
can effectively explore the state-space. Existing evaluations predominantly
focus on the trade-off between exploration and exploitation, often assessed in
multi-armed bandit problems. In contrast, this work isolates exploration as the
sole objective, tasking the agent with delivering information that enhances
future returns. For the evaluation, we propose to decompose missing rewards
into exploration and exploitation components by measuring the optimal
achievable return for the states already explored. Our experiments with various
LLMs reveal that most models struggle to sufficiently explore the state-space
and that weak exploration is insufficient. We observe a positive correlation
between model size and exploration performance, with larger models
demonstrating superior capabilities. Furthermore, we show that our
decomposition provides insights into differences in behaviors driven by agent
instructions during prompt engineering, offering a valuable tool for refining
LLM performance in exploratory tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text
  Detection Challenge <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Dugan, Andrew Zhu, Firoj Alam, Preslav Nakov, Marianna Apidianaki, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there have been many shared tasks targeting the detection of
generated text from Large Language Models (LLMs). However, these shared tasks
tend to focus either on cases where text is limited to one particular domain or
cases where text can be from many domains, some of which may not be seen during
test time. In this shared task, using the newly released RAID benchmark, we aim
to answer whether or not models can detect generated text from a large, yet
fixed, number of domains and LLMs, all of which are seen during training. Over
the course of three months, our task was attempted by 9 teams with 23 detector
submissions. We find that multiple participants were able to obtain accuracies
of over 99% on machine-generated text from RAID while maintaining a 5% False
Positive Rate -- suggesting that detectors are able to robustly detect text
from many domains and models simultaneously. We discuss potential
interpretations of this result and provide directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToMATO: Verbalizing the Mental States of Role-Playing LLMs for
  Benchmarking Theory of Mind <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazutoshi Shinoda, Nobukatsu Hojo, Kyosuke Nishida, Saki Mizuno, Keita Suzuki, Ryo Masumura, Hiroaki Sugiyama, Kuniko Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in
three aspects: 1) they assess a limited range of mental states such as beliefs,
2) false beliefs are not comprehensively explored, and 3) the diverse
personality traits of characters are overlooked. To address these challenges,
we introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over
conversations. ToMATO is generated via LLM-LLM conversations featuring
information asymmetry. By employing a prompting method that requires
role-playing LLMs to verbalize their thoughts before each utterance, we capture
both first- and second-order mental states across five categories: belief,
intention, desire, emotion, and knowledge. These verbalized thoughts serve as
answers to questions designed to assess the mental states of characters within
conversations. Furthermore, the information asymmetry introduced by hiding
thoughts from others induces the generation of false beliefs about various
mental states. Assigning distinct personality traits to LLMs further
diversifies both utterances and thoughts. ToMATO consists of 5.4k questions,
753 conversations, and 15 personality trait patterns. Our analysis shows that
this dataset construction approach frequently generates false beliefs due to
the information asymmetry between role-playing LLMs, and effectively reflects
diverse personalities. We evaluate nine LLMs on ToMATO and find that even
GPT-4o mini lags behind human performance, especially in understanding false
beliefs, and lacks robustness to various personality traits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuicai Dong, Yujing Chang, Xin Deik Goh, Dexun Li, Ruiming Tang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal document retrieval is designed to identify and retrieve various
forms of multi-modal content, such as figures, tables, charts, and layout
information from extensive documents. Despite its significance, there is a
notable lack of a robust benchmark to effectively evaluate the performance of
systems in multi-modal document retrieval. To address this gap, this work
introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks:
page-level and layout-level retrieval. The former focuses on localizing the
most relevant pages within a long document, while the latter targets the
detection of specific layouts, offering a more fine-grained granularity than
whole-page analysis. A layout can refer to a variety of elements such as
textual paragraphs, equations, figures, tables, or charts. The MMDocIR
benchmark comprises a rich dataset featuring expertly annotated labels for
1,685 questions and bootstrapped labels for 173,843 questions, making it a
pivotal resource for advancing multi-modal document retrieval for both training
and evaluation. Through rigorous experiments, we reveal that (i) visual
retrievers significantly outperform their text counterparts, (ii) MMDocIR train
set can effectively benefit the training process of multi-modal document
retrieval and (iii) text retrievers leveraging on VLM-text perform much better
than those using OCR-text. These findings underscores the potential advantages
of integrating visual elements for multi-modal document retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://huggingface.co/MMDocIR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAIF: A Comprehensive Framework for Evaluating the Risks of Generative
  AI in the Public Sector <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyeongryul Lee, Heehyeon Kim, Joyce Jiyoung Whang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid adoption of generative AI in the public sector, encompassing
diverse applications ranging from automated public assistance to welfare
services and immigration processes, highlights its transformative potential
while underscoring the pressing need for thorough risk assessments. Despite its
growing presence, evaluations of risks associated with AI-driven systems in the
public sector remain insufficiently explored. Building upon an established
taxonomy of AI risks derived from diverse government policies and corporate
guidelines, we investigate the critical risks posed by generative AI in the
public sector while extending the scope to account for its multimodal
capabilities. In addition, we propose a Systematic dAta generatIon Framework
for evaluating the risks of generative AI (SAIF). SAIF involves four key
stages: breaking down risks, designing scenarios, applying jailbreak methods,
and exploring prompt types. It ensures the systematic and consistent generation
of prompt data, facilitating a comprehensive evaluation while providing a solid
foundation for mitigating the risks. Furthermore, SAIF is designed to
accommodate emerging jailbreak methods and evolving prompt types, thereby
enabling effective responses to unforeseen risk scenarios. We believe that this
study can play a crucial role in fostering the safe and responsible integration
of generative AI into the public sector.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, 1 tables. AI for Public Missions (AIPM) Workshop
  at the 39th AAAI Conference on Artificial Intelligence (AAAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking the Imposters: How Censorship and Domain Adaptation Affect the
  Detection of Machine-Generated Tweets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17967v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17967v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan E. Tuck, Rakesh M. Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has significantly
improved the generation of fluent and convincing text, raising concerns about
their potential misuse on social media platforms. We present a comprehensive
methodology for creating nine Twitter datasets to examine the generative
capabilities of four prominent LLMs: Llama 3, Mistral, Qwen2, and GPT4o. These
datasets encompass four censored and five uncensored model configurations,
including 7B and 8B parameter base-instruction models of the three open-source
LLMs. Additionally, we perform a data quality analysis to assess the
characteristics of textual outputs from human, "censored," and "uncensored"
models, employing semantic meaning, lexical richness, structural patterns,
content characteristics, and detector performance metrics to identify
differences and similarities. Our evaluation demonstrates that "uncensored"
models significantly undermine the effectiveness of automated detection
methods. This study addresses a critical gap by exploring smaller open-source
models and the ramifications of "uncensoring," providing valuable insights into
how domain adaptation and content moderation strategies influence both the
detectability and structural characteristics of machine-generated text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PASS: Presentation Automation for Slide Generation and Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tushar Aggarwal, Aarohi Bhand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's fast-paced world, effective presentations have become an essential
tool for communication in both online and offline meetings. The crafting of a
compelling presentation requires significant time and effort, from gathering
key insights to designing slides that convey information clearly and concisely.
However, despite the wealth of resources available, people often find
themselves manually extracting crucial points, analyzing data, and organizing
content in a way that ensures clarity and impact. Furthermore, a successful
presentation goes beyond just the slides; it demands rehearsal and the ability
to weave a captivating narrative to fully engage the audience. Although there
has been some exploration of automating document-to-slide generation, existing
research is largely centered on converting research papers. In addition,
automation of the delivery of these presentations has yet to be addressed. We
introduce PASS, a pipeline used to generate slides from general Word documents,
going beyond just research papers, which also automates the oral delivery of
the generated slides. PASS analyzes user documents to create a dynamic,
engaging presentation with an AI-generated voice. Additionally, we developed an
LLM-based evaluation metric to assess our pipeline across three critical
dimensions of presentations: relevance, coherence, and redundancy. The data and
codes are available at https://github.com/AggarwalTushar/PASS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextual Evaluation of <span class="highlight-title">Large Language Model</span>s for Classifying Tropical
  and Infectious Diseases <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09201v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09201v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mercy Asiedu, Nenad Tomasev, Chintan Ghate, Tiya Tiyasirichokchai, Awa Dieng, Oluwatosin Akande, Geoffrey Siwo, Steve Adudans, Sylvanus Aitkins, Odianosen Ehiakhamen, Eric Ndombi, Katherine Heller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have shown promise for medical question
answering, there is limited work focused on tropical and infectious
disease-specific exploration. We build on an opensource tropical and infectious
diseases (TRINDs) dataset, expanding it to include demographic and semantic
clinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM
performance on these, comparing generalist and medical LLMs, as well as LLM
outcomes to human experts. We demonstrate through systematic experimentation,
the benefit of contextual information such as demographics, location, gender,
risk factors for optimal LLM response. Finally we develop a prototype of
TRINDs-LM, a research tool that provides a playground to navigate how context
impacts LLM outputs for health.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2 NeurIPS 2024 workshops: Generative AI for Health
  Workshop and Workshop on Advancements In Medical Foundation Models:
  Explainability, Robustness, Security, and Beyond</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency of Responses and Continuations Generated by Large Language
  Models on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate remarkable capabilities in text
generation, yet their emotional consistency and semantic coherence in social
media contexts remain insufficiently understood. This study investigates how
LLMs handle emotional content and maintain semantic relationships through
continuation and response tasks using two open-source models: Gemma and Llama.
By analyzing climate change discussions from Twitter and Reddit, we examine
emotional transitions, intensity patterns, and semantic similarity between
human-authored and LLM-generated content. Our findings reveal that while both
models maintain high semantic coherence, they exhibit distinct emotional
patterns: Gemma shows a tendency toward negative emotion amplification,
particularly anger, while maintaining certain positive emotions like optimism.
Llama demonstrates superior emotional preservation across a broader spectrum of
affects. Both models systematically generate responses with attenuated
emotional intensity compared to human-authored content and show a bias toward
positive emotions in response tasks. Additionally, both models maintain strong
semantic similarity with original texts, though performance varies between
continuation and response tasks. These findings provide insights into LLMs'
emotional and semantic processing capabilities, with implications for their
deployment in social media contexts and human-AI interaction design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSA-ASR: Efficient Multilingual Speaker Attribution with frozen ASR
  Models <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18152v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18152v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thai-Binh Nguyen, Alexander Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speaker-attributed automatic speech recognition (SA-ASR) aims to transcribe
speech while assigning transcripts to the corresponding speakers accurately.
Existing methods often rely on complex modular systems or require extensive
fine-tuning of joint modules, limiting their adaptability and general
efficiency. This paper introduces a novel approach, leveraging a frozen
multilingual ASR model to incorporate speaker attribution into the
transcriptions, using only standard monolingual ASR datasets. Our method
involves training a speaker module to predict speaker embeddings based on weak
labels without requiring additional ASR model modifications. Despite being
trained exclusively with non-overlapping monolingual data, our approach
effectively extracts speaker attributes across diverse multilingual datasets,
including those with overlapping speech. Experimental results demonstrate
competitive performance compared to strong baselines, highlighting the model's
robustness and potential for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets
  and Languages for Open Named Entity Recognition <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuming Yang, Wantong Zhao, Caishuang Huang, Junjie Ye, Xiao Wang, Huiyuan Zheng, Yang Nan, Yuran Wang, Xueying Xu, Kaixin Huang, Yunke Zhang, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Named Entity Recognition (NER), which involves identifying arbitrary
types of entities from arbitrary domains, remains challenging for Large
Language Models (LLMs). Recent studies suggest that fine-tuning LLMs on
extensive NER data can boost their performance. However, training directly on
existing datasets neglects their inconsistent entity definitions and redundant
data, limiting LLMs to dataset-specific learning and hindering out-of-domain
adaptation. To address this, we present B2NERD, a compact dataset designed to
guide LLMs' generalization in Open NER under a universal entity taxonomy.
B2NERD is refined from 54 existing English and Chinese datasets using a
two-step process. First, we detect inconsistent entity definitions across
datasets and clarify them by distinguishable label names to construct a
universal taxonomy of 400+ entity types. Second, we address redundancy using a
data pruning strategy that selects fewer samples with greater category and
semantic diversity. Comprehensive evaluation shows that B2NERD significantly
enhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,
outperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3
out-of-domain benchmarks across 15 datasets and 6 languages. The data, models,
and code are publicly available at https://github.com/UmeanNever/B2NER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025. Camera-ready version updated. Project page:
  https://github.com/UmeanNever/B2NER</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IndoNLP 2025: Shared Task on Real-Time Reverse Transliteration for
  Romanized Indo-Aryan languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deshan Sumanathilaka, Isuri Anuradha, Ruvan Weerasinghe, Nicholas Micallef, Julian Hough
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper overviews the shared task on Real-Time Reverse Transliteration for
Romanized Indo-Aryan languages. It focuses on the reverse transliteration of
low-resourced languages in the Indo-Aryan family to their native scripts.
Typing Romanized Indo-Aryan languages using ad-hoc transliterals and achieving
accurate native scripts are complex and often inaccurate processes with the
current keyboard systems. This task aims to introduce and evaluate a real-time
reverse transliterator that converts Romanized Indo-Aryan languages to their
native scripts, improving the typing experience for users. Out of 11 registered
teams, four teams participated in the final evaluation phase with
transliteration models for Sinhala, Hindi and Malayalam. These proposed
solutions not only solve the issue of ad-hoc transliteration but also empower
low-resource language usability in the digital arena.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, 1 Figure, 3 Tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Vulnerabilities in Encrypted Software <span class="highlight-title">Code</span> while Ensuring <span class="highlight-title">Code</span>
  Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Martins, David Dantas, Rafael Ramires, Bernardo Ferreira, Ibéria Medeiros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software vulnerabilities continue to be the main cause of occurrence for
cyber attacks. In an attempt to reduce them and improve software quality,
software code analysis has emerged as a service offered by companies
specialising in software testing. However, this service requires software
companies to provide access to their software's code, which raises concerns
about code privacy and intellectual property theft. This paper presents a novel
approach to Software Quality and Privacy, in which testing companies can
perform code analysis tasks on encrypted software code provided by software
companies while code privacy is preserved. The approach combines Static Code
Analysis and Searchable Symmetric Encryption in order to process the source
code and build an encrypted inverted index that represents its data and control
flows. The index is then used to discover vulnerabilities by carrying out
static analysis tasks in a confidential way. With this approach, this paper
also defines a new research field -- Confidential Code Analysis --, from which
other types of code analysis tasks and approaches can be derived. We
implemented the approach in a new tool called CoCoA and evaluated it
experimentally with synthetic and real PHP web applications. The results show
that the tool has similar precision as standard (non-confidential) static
analysis tools and a modest average performance overhead of 42.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures, 13 pages without refs, 4 appendixes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Blockchain-Enabled Approach to Cross-Border Compliance and Trust 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikram Kulothungan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As artificial intelligence (AI) systems become increasingly integral to
critical infrastructure and global operations, the need for a unified,
trustworthy governance framework is more urgent that ever. This paper proposes
a novel approach to AI governance, utilizing blockchain and distributed ledger
technologies (DLT) to establish a decentralized, globally recognized framework
that ensures security, privacy, and trustworthiness of AI systems across
borders. The paper presents specific implementation scenarios within the
financial sector, outlines a phased deployment timeline over the next decade,
and addresses potential challenges with solutions grounded in current research.
By synthesizing advancements in blockchain, AI ethics, and cybersecurity, this
paper offers a comprehensive roadmap for a decentralized AI governance
framework capable of adapting to the complex and evolving landscape of global
AI regulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint of paper that has been accepted for Publication at
  2024 IEEE International Conference on Trust, Privacy and Security in
  Intelligent Systems, and Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAFix: History-Augmented <span class="highlight-title">Large Language Model</span>s for Bug Fixing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Shi, Abdul Ali Bangash, Emad Fallahzadeh, Bram Adams, Ahmed E. Hassan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have explored the performance of Large Language Models (LLMs)
on various Software Engineering (SE) tasks, such as code generation and bug
fixing. However, these approaches typically rely on the context data from the
current snapshot of the project, overlooking the potential of rich historical
data from real-world software repositories. Additionally, the impact of prompt
styles on LLM performance within a historical context remains underexplored. To
address these gaps, we propose HAFix, which stands for History-Augmented LLMs
on Bug Fixing, a novel approach that leverages individual historical heuristics
associated with bugs and aggregates the results of these heuristics (HAFix-Agg)
to enhance LLMs' bug-fixing capabilities. To empirically evaluate HAFix, we
employ Code Llama on a dataset of 51 single-line bugs, sourced from 11
open-source projects, by mining the historical context data of bugs and
operationalizing this context in the form of seven heuristics. Our evaluation
demonstrates that historical heuristics significantly enhance bug-fixing
performance. For example, the FLN-all heuristic achieves a 10% improvement in
performance compared to a non-historical baseline inspired by GitHub Copilot.
Furthermore, HAFix-Agg fixes 45% more bugs than the baseline, outperforming
FLN-all and demonstrating the best performance overall. Moreover, within the
context of historical heuristics, we identify the Instruction style prompt as
the most effective template for LLMs in bug fixing. Finally, we provide a
pragmatic trade-off analysis of bug-fixing performance, cost, and time
efficiency, offering valuable insights for the practical deployment of our
approach in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do Generative Models Draw a Software Engineer? A Case Study on
  Stable Diffusion Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tosin Fadahunsi, Giordano d'Aloisio, Antinisca Di Marco, Federica Sarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models are nowadays widely used to generate graphical content used
for multiple purposes, e.g. web, art, advertisement. However, it has been shown
that the images generated by these models could reinforce societal biases
already existing in specific contexts. In this paper, we focus on understanding
if this is the case when one generates images related to various software
engineering tasks. In fact, the Software Engineering (SE) community is not
immune from gender and ethnicity disparities, which could be amplified by the
use of these models. Hence, if used without consciousness, artificially
generated images could reinforce these biases in the SE domain. Specifically,
we perform an extensive empirical evaluation of the gender and ethnicity bias
exposed by three versions of the Stable Diffusion (SD) model (a very popular
open-source text-to-image model) - SD 2, SD XL, and SD 3 - towards SE tasks. We
obtain 6,720 images by feeding each model with two sets of prompts describing
different software-related tasks: one set includes the Software Engineer
keyword, and one set does not include any specification of the person
performing the task. Next, we evaluate the gender and ethnicity disparities in
the generated images. Results show how all models are significantly biased
towards male figures when representing software engineers. On the contrary,
while SD 2 and SD XL are strongly biased towards White figures, SD 3 is
slightly more biased towards Asian figures. Nevertheless, all models
significantly under-represent Black and Arab figures, regardless of the prompt
style used. The results of our analysis highlight severe concerns about
adopting those models to generate content for SE tasks and open the field for
future research on bias mitigation in this context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taint Analysis for Graph APIs Focusing on Broken Access Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leen Lambers, Lucas Sakizloglou, Taisiya Khakharova, Fernando Orejas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph APIs are capable of flexibly retrieving or manipulating
graph-structured data over the web. This rather novel type of APIs presents new
challenges when it comes to properly securing the APIs against the usual web
application security risks, e.g., broken access control. A prominent security
testing approach is taint analysis, which traces tainted, i.e.,
security-relevant, data from sources (where tainted data is inserted) to sinks
(where the use of tainted data may lead to a security risk), over the
information flow in an application.
  We present a first systematic approach to static and dynamic taint analysis
for Graph APIs focusing on broken access control. The approach comprises the
following. We taint nodes in the Graph API if they represent data requiring
specific privileges in order to be retrieved or manipulated, and identify API
calls which are related to sources and sinks. Then, we statically analyze
whether tainted information flow between API source and sink calls occurs. To
this end, we model the API calls using graph transformation rules. We
subsequently use critical pair analysis to automatically analyze potential
dependencies between rules representing source calls and rules representing
sink calls. We distinguish direct from indirect tainted information flow and
argue under which conditions the CPA is able to detect not only direct, but
also indirect tainted flow. The static taint analysis (i) identifies flows that
need to be further reviewed, since tainted nodes may be created by an API call
and used or manipulated by another API call later without having the necessary
privileges, and (ii) can be used to systematically design dynamic security
tests for broken access control. The dynamic taint analysis checks if potential
broken access control risks detected during the static taint analysis really
occur. We apply the approach to a part of the GitHub GraphQL API.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Intermediate preprint for submission to ICGT 24 Special Issue in LMCS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Software Testing for Extended Reality Applications: A Systematic Mapping
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhen Gu, José Miguel Rojas, Donghwan Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extended Reality (XR) is an emerging technology spanning diverse application
domains and offering immersive user experiences. However, its unique
characteristics, such as six degrees of freedom interactions, present
significant testing challenges distinct from traditional 2D GUI applications,
demanding novel testing techniques to build high-quality XR applications. This
paper presents the first systematic mapping study on software testing for XR
applications. We selected 34 studies focusing on techniques and empirical
approaches in XR software testing for detailed examination. The studies are
classified and reviewed to address the current research landscape, test facets,
and evaluation methodologies in the XR testing domain. Additionally, we provide
a repository summarising the mapping study, including datasets and tools
referenced in the selected studies, to support future research and practical
applications. Our study highlights open challenges in XR testing and proposes
actionable future research directions to address the gaps and advance the field
of XR software testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Uncertainty Leads to Unsafety: Empirical Insights into the Role of
  Uncertainty in Unmanned Aerial Vehicle Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sajad Khatiri, Fatemeh Mohammadi Amin, Sebastiano Panichella, Paolo Tonella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent developments in obstacle avoidance and other safety
features, autonomous Unmanned Aerial Vehicles (UAVs) continue to face safety
challenges. No previous work investigated the relationship between the
behavioral uncertainty of a UAV and the unsafety of its flight. By quantifying
uncertainty, it is possible to develop a predictor for unsafety, which acts as
a flight supervisor. We conducted a large-scale empirical investigation of
safety violations using PX4-Autopilot, an open-source UAV software platform.
Our dataset of over 5,000 simulated flights, created to challenge obstacle
avoidance, allowed us to explore the relation between uncertain UAV decisions
and safety violations: up to 89% of unsafe UAV states exhibit significant
decision uncertainty, and up to 74% of uncertain decisions lead to unsafe
states. Based on these findings, we implemented Superialist (Supervising
Autonomous Aerial Vehicles), a runtime uncertainty detector based on
autoencoders, the state-of-the-art technology for anomaly detection.
Superialist achieved high performance in detecting uncertain behaviors with up
to 96% precision and 93% recall. Despite the observed performance degradation
when using the same approach for predicting unsafety (up to 74% precision and
87% recall), Superialist enabled early prediction of unsafe states up to 50
seconds in advance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CveBinarySheet: A Comprehensive Pre-built Binaries Database for IoT
  Vulnerability Analysis <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Binary Static Code Analysis (BSCA) is a pivotal area in software
vulnerability research, focusing on the precise localization of vulnerabilities
within binary executables. Despite advancements in BSCA techniques, there is a
notable scarcity of comprehensive and readily usable vulnerability datasets
tailored for diverse environments such as IoT, UEFI, and MCU firmware. To
address this gap, we present CveBinarySheet, a meticulously curated database
containing 1033 CVE entries spanning from 1999 to 2024. Our dataset encompasses
16 essential third-party components, including busybox and curl, and supports
five CPU architectures: x86-64, i386, MIPS, ARMv7, and RISC-V64. Each
precompiled binary is available at two compiler optimization levels (O0 and
O3), facilitating comprehensive vulnerability analysis under different
compilation scenarios. By providing detailed metadata and diverse binary
samples, CveBinarySheet aims to accelerate the development of state-of-the-art
BSCA tools, binary similarity analysis, and vulnerability matching
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, dataset for binary SCA training</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smart Contract Fuzzing Towards Profitable Vulnerabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiao Kong, Cen Zhang, Maoyi Xie, Ming Hu, Yue Xue, Ye Liu, Haijun Wang, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Billions of dollars are transacted through smart contracts, making
vulnerabilities a major financial risk. One focus in the security arms race is
on profitable vulnerabilities that attackers can exploit. Fuzzing is a key
method for identifying these vulnerabilities. However, current solutions face
two main limitations: a lack of profit-centric techniques for expediting
detection, and insufficient automation in maximizing the profitability of
discovered vulnerabilities, leaving the analysis to human experts. To address
these gaps, we have developed VERITE, a profit-centric smart contract fuzzing
framework that not only effectively detects those profitable vulnerabilities
but also maximizes the exploited profits.
  VERITE has three key features: 1) DeFi action-based mutators for boosting the
exploration of transactions with different fund flows; 2) potentially
profitable candidates identification criteria, which checks whether the input
has caused abnormal fund flow properties during testing; 3) a gradient
descent-based profit maximization strategy for these identified candidates.
  VERITE is fully developed from scratch and evaluated on a dataset consisting
of 61 exploited real-world DeFi projects with an average of over 1.1 million
dollars loss. The results show that VERITE can automatically extract more than
18 million dollars in total and is significantly better than state-of-the-art
fuzzer ITYFUZZ in both detection (29/9) and exploitation (58 times more profits
gained on average). Remarkbly, in 12 targets, it gains more profits than
real-world attacking exploits (1.01 to 11.45 times more). VERITE is also
applied by auditors in contract auditing, where 6 (5 high severity) zero-day
vulnerabilities are found with over $2,500 bounty rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submission version without revisions requested from peer reviews. The
  camera-ready version will be available soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in
  Software Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Treude, Marco A. Gerosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI), including large language models and generative
AI, is emerging as a significant force in software development, offering
developers powerful tools that span the entire development lifecycle. Although
software engineering research has extensively studied AI tools in software
development, the specific types of interactions between developers and these
AI-powered tools have only recently begun to receive attention. Understanding
and improving these interactions has the potential to improve productivity,
trust, and efficiency in AI-driven workflows. In this paper, we propose a
taxonomy of interaction types between developers and AI tools, identifying
eleven distinct interaction types, such as auto-complete code suggestions,
command-driven actions, and conversational assistance. Building on this
taxonomy, we outline a research agenda focused on optimizing AI interactions,
improving developer control, and addressing trust and usability challenges in
AI-assisted development. By establishing a structured foundation for studying
developer-AI interactions, this paper aims to stimulate research on creating
more effective, adaptive AI tools for software development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2nd ACM International Conference on AI Foundation Models
  and Software Engineering (FORGE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging LLM Agents for Translating Network Configurations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Wei, Xiaohui Xie, Yiwei Zuo, Tianshuo Hu, Xinyi Chen, Kaiwen Chi, Yong Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Configuration translation is a critical and frequent task in network
operations. When a network device is damaged or outdated, administrators need
to replace it to maintain service continuity. The replacement devices may
originate from different vendors, necessitating configuration translation to
ensure seamless network operation. However, translating configurations manually
is a labor-intensive and error-prone process. In this paper, we propose an
intent-based framework for translating network configuration with Large
Language Model (LLM) Agents. The core of our approach is an Intent-based
Retrieval Augmented Generation (IRAG) module that systematically splits a
configuration file into fragments, extracts intents, and generates accurate
translations. We also design a two-stage verification method to validate the
syntax and semantics correctness of the translated configurations. We implement
and evaluate the proposed method on real-world network configurations.
Experimental results show that our method achieves 97.74% syntax correctness,
outperforming state-of-the-art methods in translation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting Smart Contract Decompiler Output through Fine-grained
  Dependency Analysis and LLM-facilitated Semantic Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqin Liao, Yuhong Nan, Zixu Gao, Henglong Liang, Sicheng Hao, Peifan Reng, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decompiler is a specialized type of reverse engineering tool extensively
employed in program analysis tasks, particularly in program comprehension and
vulnerability detection. However, current Solidity smart contract decompilers
face significant limitations in reconstructing the original source code. In
particular, the bottleneck of SOTA decompilers lies in inaccurate method
identification, incorrect variable type recovery, and missing contract
attributes. These deficiencies hinder downstream tasks and understanding of the
program logic. To address these challenges, we propose SmartHalo, a new
framework that enhances decompiler output by combining static analysis (SA) and
large language models (LLM). SmartHalo leverages the complementary strengths of
SA's accuracy in control and data flow analysis and LLM's capability in
semantic prediction. More specifically, \system{} constructs a new data
structure - Dependency Graph (DG), to extract semantic dependencies via static
analysis. Then, it takes DG to create prompts for LLM optimization. Finally,
the correctness of LLM outputs is validated through symbolic execution and
formal verification. Evaluation on a dataset consisting of 465 randomly
selected smart contract methods shows that SmartHalo significantly improves the
quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse).
Notably, integrating GPT-4o with SmartHalo further enhances its performance,
achieving precision rates of 87.39% for method boundaries, 90.39% for variable
types, and 80.65% for contract attributes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoRestTest: A Tool for Automated REST API Testing Using LLMs and MARL <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Stennett, Myeongsoo Kim, Saurabh Sinha, Alessandro Orso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As REST APIs have become widespread in modern web services, comprehensive
testing of these APIs has become increasingly crucial. Due to the vast search
space consisting of operations, parameters, and parameter values along with
their complex dependencies and constraints, current testing tools suffer from
low code coverage, leading to suboptimal fault detection. To address this
limitation, we present a novel tool, AutoRestTest, which integrates the
Semantic Operation Dependency Graph (SODG) with Multi-Agent Reinforcement
Learning (MARL) and large language models (LLMs) for effective REST API
testing. AutoRestTest determines operation-dependent parameters using the SODG
and employs five specialized agents (operation, parameter, value, dependency,
and header) to identify dependencies of operations and generate operation
sequences, parameter combinations, and values. AutoRestTest provides a
command-line interface and continuous telemetry on successful operation count,
unique server errors detected, and time elapsed. Upon completion, AutoRestTest
generates a detailed report highlighting errors detected and operations
exercised. In this paper, we introduce our tool and present preliminary
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the 47th IEEE/ACM International Conference on
  Software Engineering - Demonstration Track (ICSE-Demo 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LlamaRestTest: Effective REST API Testing with Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myeongsoo Kim, Saurabh Sinha, Alessandro Orso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern web services rely heavily on REST APIs, typically documented using the
OpenAPI specification. The widespread adoption of this standard has resulted in
the development of many black-box testing tools that generate tests based on
these specifications. Recent advancements in Natural Language Processing (NLP),
particularly with Large Language Models (LLMs), have enhanced REST API testing
by extracting actionable rules and generating input values from the
human-readable portions of the specification. However, these advancements
overlook the potential of continuously refining the identified rules and test
inputs based on server responses. To address this limitation, we present
LlamaRestTest, a novel approach that employs two custom LLMs to generate
realistic test inputs and uncover parameter dependencies during the testing
process by incorporating server responses. These LLMs are created by
fine-tuning the Llama3-8b model, using mined datasets of REST API example
values and inter-parameter dependencies. We evaluated LlamaRestTest on 12
real-world services (including popular services such as Spotify), comparing it
against RESTGPT, a GPT-powered specification-enhancement tool, as well as
several state-of-the-art REST API testing tools, including RESTler, MoRest,
EvoMaster, and ARAT-RL. Our results show that fine-tuning enables smaller LLMs
to outperform larger models in detecting actionable rules and generating inputs
for REST API testing. We evaluated configurations from the base Llama3-8B to
fine-tuned versions and explored 2-bit, 4-bit, and 8-bit quantization for
efficiency. LlamaRestTest surpasses state-of-the-art tools in code coverage and
error detection, even with RESTGPT-enhanced specifications, and an ablation
study highlights the impact of its novel components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the ACM International Conference on the
  Foundations of Software Engineering (FSE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formal Model Guided Conformance Testing for Blockchains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Drobnjakovic, Amir Kashapov, Matija Kupresanin, Bernhard Scholz, Pavle Subotic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern blockchains increasingly consist of multiple clients that implement
the blockchain protocol. If there is a semantic mismatch between the protocol
implementations, the blockchain can permanently split and introduce new attack
vectors. Current ad-hoc test suites for client implementations are not
sufficient to ensure a high degree of protocol conformance. As an alternative,
we present a framework that performs protocol conformance testing using a
formal model of the protocol and an implementation running inside a
deterministic blockchain simulator. Our framework consists of two complementary
workflows that use the components as trace generators and checkers. Our insight
is that both workflows are needed to detect all types of violations. We have
applied and demonstrated the utility of our framework on an industrial strength
consensus protocol.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Similarity-Aware Test Suite Minimization with Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijia Gu, Ali Mesbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Multi-Criteria Test Suite Minimization (MCTSM) problem aims to remove
redundant test cases, guided by adequacy criteria such as code coverage or
fault detection capability. However, current techniques either exhibit a high
loss of fault detection ability or face scalability challenges due to the
NP-hard nature of the problem, which limits their practical utility. We propose
TripRL, a novel technique that integrates traditional criteria such as
statement coverage and fault detection ability with test coverage similarity
into an Integer Linear Program (ILP), to produce a diverse reduced test suite
with high test effectiveness. TripRL leverages bipartite graph representation
and its embedding for concise ILP formulation and combines ILP with effective
reinforcement learning (RL) training. This combination renders large-scale test
suite minimization more scalable and enhances test effectiveness. Our empirical
evaluations demonstrate that TripRL's runtime scales linearly with the
magnitude of the MCTSM problem. Notably, for large test suites from the
Defects4j dataset where existing approaches fail to provide solutions within a
reasonable time frame, our technique consistently delivers solutions in less
than 47 minutes. The reduced test suites produced by TripRL also maintain the
original statement coverage and fault detection ability while having a higher
potential to detect unknown faults.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASTER: Natural and Multi-language Unit Test Generation with LLMs <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rangeet Pan, Myeongsoo Kim, Rahul Krishna, Raju Pavuluri, Saurabh Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implementing automated unit tests is an important but time-consuming activity
in software development. To assist developers in this task, many techniques for
automating unit test generation have been developed. However, despite this
effort, usable tools exist for very few programming languages. Moreover,
studies have found that automatically generated tests suffer poor readability
and do not resemble developer-written tests. In this work, we present a
rigorous investigation of how large language models (LLMs) can help bridge the
gap. We describe a generic pipeline that incorporates static analysis to guide
LLMs in generating compilable and high-coverage test cases. We illustrate how
the pipeline can be applied to different programming languages, specifically
Java and Python, and to complex software requiring environment mocking. We
conducted an empirical study to assess the quality of the generated tests in
terms of code coverage and test naturalness -- evaluating them on standard as
well as enterprise Java applications and a large Python benchmark. Our results
demonstrate that LLM-based test generation, when guided by static analysis, can
be competitive with, and even outperform, state-of-the-art test-generation
techniques in coverage achieved while also producing considerably more natural
test cases that developers find easy to understand. We also present the results
of a user study, conducted with 161 professional developers, that highlights
the naturalness characteristics of the tests generated by our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICSE-SEIP, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unseen Horizons: Unveiling the Real Capability of LLM <span class="highlight-title">Code</span> Generation
  Beyond the Familiar <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanliang Zhang, Yifan Xie, Shanshan Li, Ke Liu, Chong Wang, Zhouyang Jia, Xiangbing Huang, Jie Song, Chaopeng Luo, Zhizheng Zheng, Rulin Xu, Yitong Liu, Si Zheng, Xiangke Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large language models (LLMs) have shown strong potential in code
generation tasks. However, there are still gaps before they can be fully
applied in actual software development processes. Accurately assessing the code
generation capabilities of large language models has become an important basis
for evaluating and improving the models. Some existing works have constructed
datasets to evaluate the capabilities of these models. However, the current
evaluation process may encounter the illusion of "Specialist in Familiarity",
primarily due to three gaps: the exposure of target code, case timeliness, and
dependency availability. The fundamental reason for these gaps is that the code
in current datasets may have been extensively exposed and exercised during the
training phase, and due to the continuous training and development of LLM,
their timeliness has been severely compromised. The key to solve the problem is
to, as much as possible, evaluate the LLMs using code that they have not
encountered before. Thus, the fundamental idea in this paper is to draw on the
concept of code obfuscation, changing code at different levels while ensuring
the functionality and output. To this end, we build a code-obfuscation based
benchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world
projects, including function description and code. Then we use three-level
strategy (symbol, structure and semantic) to obfuscate descriptions, code and
context dependencies. We evaluate four LLMs on OBFU- SEVAL and compared the
effectiveness of different obfuscation strategy. We use official test suites of
these projects to evaluate the generated code. The results show that after
obfuscation, the average decrease ratio of test pass rate can up to 62.5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 47th International Conference on Software Engineering
  (ICSE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Outlines for <span class="highlight-title">Code</span>: Literate Programming in the LLM Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kensen Shi, Deniz Altınbüken, Saswat Anand, Mihai Christodorescu, Katja Grünwedel, Alexa Koenings, Sai Naidu, Anurag Pathak, Marc Rasi, Fredde Ribeiro, Brandon Ruffin, Siddhant Sanyam, Maxim Tabachnyk, Sara Toth, Roy Tu, Tobias Welp, Pengcheng Yin, Manzil Zaheer, Satish Chandra, Charles Sutton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose using natural language outlines as a novel modality and
interaction surface for providing AI assistance to developers throughout the
software development process. An NL outline for a code function comprises
multiple statements written in concise prose, which partition the code and
summarize its main ideas in the style of literate programming. Crucially, we
find that modern LLMs can generate accurate and high-quality NL outlines in
practice. Moreover, NL outlines enable a bidirectional sync between code and
NL, allowing changes in one to be automatically reflected in the other. We
discuss many use cases for NL outlines: they can accelerate understanding and
navigation of code and diffs, simplify code maintenance, augment code search,
steer code generation, and more. We then propose and compare multiple LLM
prompting techniques for generating outlines and ask professional developers to
judge outline quality. Finally, we present two case studies applying NL
outlines toward code review and malware detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Ability of <span class="highlight-title">Pre-train</span>ed Language Model by Imparting Large
  Language Model's Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Yin, Chao Ni, Xiaodan Xu, Xinrui Li, Xiaohu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) and pre-trained Language Models (LMs) have
achieved impressive success on many software engineering tasks (e.g., code
completion and code generation). By leveraging huge existing code corpora
(e.g., GitHub), these models can understand the patterns in source code and use
these patterns to predict code properties. However, LLMs under few-shot
learning perform poorly on non-generative tasks (e.g., fault localization and
vulnerability localization), and fine-tuning LLMs is time-consuming and costly
for end users and small organizations. Furthermore, the performance of
fine-tuning LMs for non-generative tasks is impressive, yet it heavily depends
on the amount and quality of data. As a result, the current lack of data and
the high cost of collecting it in real-world scenarios further limit the
applicability of LMs. In this paper, we leverage the powerful generation
capabilities of LLMs to enhance pre-trained LMs. Specifically, we use LLMs to
generate domain-specific data, thereby improving the performance of pre-trained
LMs on the target tasks. We conduct experiments by combining different LLMs in
our generation phase and introducing various LMs to learn from the
LLM-generated data. Then, we compare the performance of these LMs before and
after learning the data. We find that LLM-generated data significantly enhances
the performance of LMs. The improvement can reach up to 58.36% for fault
localization and up to 6.09% for clone detection.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Printed Maps and Icons for Inclusion: Testing in the Wild by People
  who are Blind or have Low Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leona Holloway, Kim Marriott, Matthew Butler, Samuel Reinders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The difficulty and consequent fear of travel is one of the most disabling
consequences of blindness and severe vision impairment, affecting confidence
and quality of life. Traditional tactile graphics are vital in the Orientation
and Mobility training process, however 3D printing may have the capacity to
enable production of more meaningful and inclusive maps. This study explored
the use of 3D printed maps on site at a public event to examine their
suitability and to identify guidelines for the design of future 3D maps. An
iterative design process was used in the production of the 3D maps, with
feedback from visitors who are blind or have low vision informing the
recommendations for their design and use. For example, it was found that many
representational 3D icons could be recognised by touch without the need for a
key and that such a map helped form mental models of the event space. Complex
maps, however, require time to explore and should be made available before an
event or at the entrance in a comfortable position. The maps were found to
support the orientation and mobility process, and importantly to also promote a
positive message about inclusion and accessibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper presented at ACM ASSETS 2019: Proceedings of the 21st
  International ACM SIGACCESS Conference on Computers and Accessibility, ACM,
  New York, October 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Breaking Barriers or Building Dependency? Exploring Team-LLM
  Collaboration in AI-infused Classroom Debate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Zhang, Black Sun, Pengcheng An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classroom debates are a unique form of collaborative learning characterized
by fast-paced, high-intensity interactions that foster critical thinking and
teamwork. Despite the recognized importance of debates, the role of AI tools,
particularly LLM-based systems, in supporting this dynamic learning environment
has been under-explored in HCI. This study addresses this opportunity by
investigating the integration of LLM-based AI into real-time classroom debates.
Over four weeks, 22 students in a Design History course participated in three
rounds of debates with support from ChatGPT. The findings reveal how learners
prompted the AI to offer insights, collaboratively processed its outputs, and
divided labor in team-AI interactions. The study also surfaces key advantages
of AI usage, reducing social anxiety, breaking communication barriers, and
providing scaffolding for novices, alongside risks, such as information
overload and cognitive dependency, which could limit learners' autonomy. We
thereby discuss a set of nuanced implications for future HCI exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Drama Llama: An LLM-Powered Storylets Framework for Authorable
  Responsiveness in Interactive Narrative 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqian Sun, Phoebe J. Wang, John Joon Young Chung, Melissa Roemmele, Taewook Kim, Max Kreminski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present Drama Llama, an LLM-powered storylets framework
that supports the authoring of responsive, open-ended interactive stories. DL
combines the structural benefits of storylet-based systems with the generative
capabilities of large language models, enabling authors to create responsive
interactive narratives while maintaining narrative control. Rather than
crafting complex logical preconditions in a general-purpose or domain-specific
programming language, authors define triggers in natural language that fire at
appropriate moments in the story. Through a preliminary authoring study with
six content authors, we present initial evidence that DL can generate coherent
and meaningful narratives with believable character interactions. This work
suggests directions for hybrid approaches that enhance authorial control while
supporting emergent narrative generation through LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 photos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Processing and Analyzing Real-World Driving Data: Insights on Trips,
  Scenarios, and Human Driving Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihun Han, Dominik Karbowski, Ayman Moawad, Namdoo Kim, Aymeric Rousseau, Shihong Fan, Jason Hoon Lee, Jinho Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing large volumes of real-world driving data is essential for providing
meaningful and reliable insights into real-world trips, scenarios, and human
driving behaviors. To this end, we developed a multi-level data processing
approach that adds new information, segments data, and extracts desired
parameters. Leveraging a confidential but extensive dataset (over 1 million
km), this approach leads to three levels of in-depth analysis: trip, scenario,
and driving. The trip-level analysis explains representative properties
observed in real-world trips, while the scenario-level analysis focuses on
scenario conditions resulting from road events that reduce vehicle speed. The
driving-level analysis identifies the cause of driving regimes for specific
situations and characterizes typical human driving behaviors. Such analyses can
support the design of both trip- and scenario-based tests, the modeling of
human drivers, and the establishment of guidelines for connected and automated
vehicles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The New Calculator? Practices, Norms, and Implications of Generative AI
  in Higher Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Auste Simkute, Viktor Kewenig, Abigail Sellen, Sean Rintel, Lev Tankelevitch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI (GenAI) has introduced myriad opportunities and challenges for
higher education. Anticipating this potential transformation requires
understanding students' contextualised practices and norms around GenAI. We
conducted semi-structured interviews with 26 students and 11 educators from
diverse departments across two universities. Grounded in Strong Structuration
Theory, we find diversity in students' uses and motivations for GenAI.
Occurring in the context of unclear university guidelines, institutional
fixation on plagiarism, and inconsistent educator communication, students'
practices are informed by unspoken rules around appropriate use, GenAI
limitations and reliance strategies, and consideration of agency and skills.
Perceived impacts include changes in confidence, and concerns about skill
development, relationships with educators, and plagiarism. Both groups envision
changes in universities' attitude to GenAI, responsible use training,
assessments, and integration of GenAI into education. We discuss
socio-technical implications in terms of current and anticipated changes in the
external and internal structures that contextualise students' GenAI use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in
  Software Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Treude, Marco A. Gerosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI), including large language models and generative
AI, is emerging as a significant force in software development, offering
developers powerful tools that span the entire development lifecycle. Although
software engineering research has extensively studied AI tools in software
development, the specific types of interactions between developers and these
AI-powered tools have only recently begun to receive attention. Understanding
and improving these interactions has the potential to improve productivity,
trust, and efficiency in AI-driven workflows. In this paper, we propose a
taxonomy of interaction types between developers and AI tools, identifying
eleven distinct interaction types, such as auto-complete code suggestions,
command-driven actions, and conversational assistance. Building on this
taxonomy, we outline a research agenda focused on optimizing AI interactions,
improving developer control, and addressing trust and usability challenges in
AI-assisted development. By establishing a structured foundation for studying
developer-AI interactions, this paper aims to stimulate research on creating
more effective, adaptive AI tools for software development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2nd ACM International Conference on AI Foundation Models
  and Software Engineering (FORGE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subject Disentanglement Neural Network for Speech Envelope
  Reconstruction from EEG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhang, Jiyao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing speech envelopes from EEG signals is essential for exploring
neural mechanisms underlying speech perception. Yet, EEG variability across
subjects and physiological artifacts complicate accurate reconstruction. To
address this problem, we introduce Subject Disentangling Neural Network
(SDN-Net), which disentangles subject identity information from reconstructed
speech envelopes to enhance cross-subject reconstruction accuracy. SDN-Net
integrates three key components: MLA-Codec, MPN-MI, and CTA-MTDNN. The
MLA-Codec, a fully convolutional neural network, decodes EEG signals into
speech envelopes. The CTA-MTDNN module, a multi-scale time-delay neural network
with channel and temporal attention, extracts subject identity features from
EEG signals. Lastly, the MPN-MI module, a mutual information estimator with a
multi-layer perceptron, supervises the removal of subject identity information
from the reconstructed speech envelope. Experiments on the Auditory EEG
Decoding Dataset demonstrate that SDN-Net achieves superior performance in
inner- and cross-subject speech envelope reconstruction compared to recent
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Learning Algorithm That Attains the Human Optimum in a Repeated
  Human-Machine Interaction Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason T. Isa, Lillian J. Ratliff, Samuel A. Burden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When humans interact with learning-based control systems, a common goal is to
minimize a cost function known only to the human. For instance, an exoskeleton
may adapt its assistance in an effort to minimize the human's metabolic
cost-of-transport. Conventional approaches to synthesizing the learning
algorithm solve an inverse problem to infer the human's cost. However, these
problems can be ill-posed, hard to solve, or sensitive to problem data. Here we
show a game-theoretic learning algorithm that works solely by observing human
actions to find the cost minimum, avoiding the need to solve an inverse
problem. We evaluate the performance of our algorithm in an extensive set of
human subjects experiments, demonstrating consistent convergence to the minimum
of a prescribed human cost function in scalar and multidimensional
instantiations of the game. We conclude by outlining future directions for
theoretical and empirical extensions of our results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANSR-DT: An Adaptive Neuro-Symbolic Learning and Reasoning Framework for
  Digital Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safayat Bin Hakim, Muhammad Adil, Alvaro Velasquez, Houbing Herbert Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an Adaptive Neuro-Symbolic Learning Framework for
digital twin technology called ``ANSR-DT." Our approach combines pattern
recognition algorithms with reinforcement learning and symbolic reasoning to
enable real-time learning and adaptive intelligence. This integration enhances
the understanding of the environment and promotes continuous learning, leading
to better and more effective decision-making in real-time for applications that
require human-machine collaboration. We evaluated the \textit{ANSR-DT}
framework for its ability to learn and adapt to dynamic patterns, observing
significant improvements in decision accuracy, reliability, and
interpretability when compared to existing state-of-the-art methods. However,
challenges still exist in extracting and integrating symbolic rules in complex
environments, which limits the full potential of our framework in heterogeneous
settings. Moreover, our ongoing research aims to address this issue in the
future by ensuring seamless integration of neural models at large. In addition,
our open-source implementation promotes reproducibility and encourages future
research to build on our foundational work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LAMS: LLM-Driven Automatic Mode Switching for Assistive Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Tao, Jehan Yang, Dan Ding, Zackory Erickson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teleoperating high degrees-of-freedom (DoF) robotic manipulators via low-DoF
controllers like joysticks often requires frequent switching between control
modes, where each mode maps controller movements to specific robot actions.
Manually performing this frequent switching can make teleoperation cumbersome
and inefficient. On the other hand, existing automatic mode-switching
solutions, such as heuristic-based or learning-based methods, are often
task-specific and lack generalizability. In this paper, we introduce LLM-Driven
Automatic Mode Switching (LAMS), a novel approach that leverages Large Language
Models (LLMs) to automatically switch control modes based on task context.
Unlike existing methods, LAMS requires no prior task demonstrations and
incrementally improves by integrating user-generated mode-switching examples.
We validate LAMS through an ablation study and a user study with 10
participants on complex, long-horizon tasks, demonstrating that LAMS
effectively reduces manual mode switches, is preferred over alternative
methods, and improves performance over time. The project website with
supplementary materials is at https://lams-assistance.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning-Enhanced Procedural Generation for Dynamic
  Narrative-Driven AR Experiences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruddha Srinivas Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural Content Generation (PCG) is widely used to create scalable and
diverse environments in games. However, existing methods, such as the Wave
Function Collapse (WFC) algorithm, are often limited to static scenarios and
lack the adaptability required for dynamic, narrative-driven applications,
particularly in augmented reality (AR) games. This paper presents a
reinforcement learning-enhanced WFC framework designed for mobile AR
environments. By integrating environment-specific rules and dynamic tile weight
adjustments informed by reinforcement learning (RL), the proposed method
generates maps that are both contextually coherent and responsive to gameplay
needs. Comparative evaluations and user studies demonstrate that the framework
achieves superior map quality and delivers immersive experiences, making it
well-suited for narrative-driven AR games. Additionally, the method holds
promise for broader applications in education, simulation training, and
immersive extended reality (XR) experiences, where dynamic and adaptive
environments are critical.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Number of pages: 13, Number of figures: 4. Accepted for presentation
  at GRAPP 2025 - 20th International Conference on Computer Graphics Theory and
  Applications (for additional details on the conference visit
  https://grapp.scitevents.org). Disclaimer: This preprint may differ from the
  final version published in the conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Easing Seasickness through Attention Redirection with a
  Mindfulness-Based Brain--Computer Interface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Bao, Kailin Xu, Jiawei Zhu, Haiyun Huang, Kangning Li, Qiyun Huang, Yuanqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Seasickness is a prevalent issue that adversely impacts both passenger
experiences and the operational efficiency of maritime crews. While techniques
that redirect attention have proven effective in alleviating motion sickness
symptoms in terrestrial environments, applying similar strategies to manage
seasickness poses unique challenges due to the prolonged and intense motion
environment associated with maritime travel. In this study, we propose a
mindfulness brain-computer interface (BCI), specifically designed to redirect
attention with the aim of mitigating seasickness symptoms in real-world
settings. Our system utilizes a single-channel headband to capture prefrontal
EEG signals, which are then wirelessly transmitted to computing devices for the
assessment of mindfulness states. The results are transferred into real-time
feedback as mindfulness scores and audiovisual stimuli, facilitating a shift in
attentional focus from physiological discomfort to mindfulness practices. A
total of 43 individuals participated in a real-world maritime experiment
consisted of three sessions: a real-feedback mindfulness session, a resting
session, and a pseudofeedback mindfulness session. Notably, 81.39% of
participants reported that the mindfulness BCI intervention was effective, and
there was a significant reduction in the severity of seasickness, as measured
by the Misery Scale (MISC). Furthermore, EEG analysis revealed a decrease in
the theta/beta ratio, corresponding with the alleviation of seasickness
symptoms. A decrease in overall EEG band power during the real-feedback
mindfulness session suggests that the mindfulness BCI fosters a more tranquil
and downregulated state of brain activity. Together, this study presents a
novel nonpharmacological, portable, and effective approach for seasickness
intervention, with the potential to enhance the cruising experience for both
passengers and crews.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for Dynamic Situational Awareness in Human Robot Teams: An
  Interview Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hashini Senaratne, Leimin Tian, Pavan Sikka, Jason Williams, David Howard, Dana Kulić, Cécile Paris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In human-robot teams, human situational awareness is the operator's conscious
knowledge of the team's states, actions, plans and their environment.
Appropriate human situational awareness is critical to successful human-robot
collaboration. In human-robot teaming, it is often assumed that the best and
required level of situational awareness is knowing everything at all times.
This view is problematic, because what a human needs to know for optimal team
performance varies given the dynamic environmental conditions, task context and
roles and capabilities of team members. We explore this topic by interviewing
16 participants with active and repeated experience in diverse human-robot
teaming applications. Based on analysis of these interviews, we derive a
framework explaining the dynamic nature of required situational awareness in
human-robot teaming. In addition, we identify a range of factors affecting the
dynamic nature of required and actual levels of situational awareness (i.e.,
dynamic situational awareness), types of situational awareness inefficiencies
resulting from gaps between actual and required situational awareness, and
their main consequences. We also reveal various strategies, initiated by humans
and robots, that assist in maintaining the required situational awareness. Our
findings inform the implementation of accurate estimates of dynamic situational
awareness and the design of user-adaptive human-robot interfaces. Therefore,
this work contributes to the future design of more collaborative and effective
human-robot teams.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Network Analysis in Immersive Environments: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Joos, Maximilian T. Fischer, Julius Rauscher, Daniel A. Keim, Tim Dwyer, Falk Schreiber, Karsten Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity and volume of network data demand effective
analysis approaches, with visual exploration proving particularly beneficial.
Immersive technologies, such as augmented reality, virtual reality, and large
display walls, have enabled the emerging field of immersive analytics, offering
new opportunities to enhance user engagement, spatial awareness, and
problem-solving. A growing body of work explores immersive environments for
network visualisation, ranging from design studies to fully integrated
applications across various domains. Despite these advancements, the field
remains fragmented, with diverse methodologies, hardware setups, and evaluation
criteria, often lacking clear connections to prior work. This fragmentation
complicates the comparability and generalisability of findings. To address
this, we present a structured survey of visual network analysis in immersive
environments. We systematically categorise and analyse existing approaches,
revealing connections and coverage within the design space. By synthesising
findings of experiments and evaluating current applications, we identify key
achievements, challenges, and research gaps. Additionally, we provide an
interactive online resource for exploring and updating results, aiming to guide
researchers and practitioners in advancing the field. This work provides a
comprehensive overview of the research landscape and proposes actionable
insights to foster innovation in immersive network analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Refreshable Tactile Displays Meet Conversational Agents:
  Investigating Accessible Data Presentation and Analysis with Touch and Speech <span class="chip">IEEE VIS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Reinders, Matthew Butler, Ingrid Zukerman, Bongshin Lee, Lizhen Qu, Kim Marriott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent surge of research efforts to make data visualizations
accessible to people who are blind or have low vision (BLV), how to support BLV
people's data analysis remains an important and challenging question. As
refreshable tactile displays (RTDs) become cheaper and conversational agents
continue to improve, their combination provides a promising approach to support
BLV people's interactive data exploration and analysis. To understand how BLV
people would use and react to a system combining an RTD with a conversational
agent, we conducted a Wizard-of-Oz study with 11 BLV participants, where they
interacted with line charts, bar charts, and isarithmic maps. Our analysis of
participants' interactions led to the identification of nine distinct patterns.
We also learned that the choice of modalities depended on the type of task and
prior experience with tactile graphics, and that participants strongly
preferred the combination of RTD and speech to a single modality. In addition,
participants with more tactile experience described how tactile images
facilitated a deeper engagement with the data and supported independent
interpretation. Our findings will inform the design of interfaces for such
interactive mixed-modality systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to be presented at IEEE VIS 2024 (Honorable Mention Award)
  and published in IEEE TVCG; Replacement: typos corrected, external DOI added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Long-Term Student Outcomes from Short-Term EdTech Log Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Gao, Amelia Leon, Andrea Jetten, Jasmine Turner, Husni Almoubayyed, Stephen Fancsali, Emma Brunskill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Educational stakeholders are often particularly interested in sparse, delayed
student outcomes, like end-of-year statewide exams. The rare occurrence of such
assessments makes it harder to identify students likely to fail such
assessments, as well as making it slow for researchers and educators to be able
to assess the effectiveness of particular educational tools. Prior work has
primarily focused on using logs from students full usage (e.g. year-long) of an
educational product to predict outcomes, or considered predictive accuracy
using a few minutes to predict outcomes after a short (e.g. 1 hour) session. In
contrast, we investigate machine learning predictors using students' logs
during their first few hours of usage can provide useful predictive insight
into those students' end-of-school year external assessment. We do this on
three diverse datasets: from students in Uganda using a literacy game product,
and from students in the US using two mathematics intelligent tutoring systems.
We consider various measures of the accuracy of the resulting predictors,
including its ability to identify students at different parts along the
assessment performance distribution. Our findings suggest that short-term log
usage data, from 2-5 hours, can be used to provide valuable signal about
students' long-term external performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 15th International Learning Analytics and Knowledge
  Conference (LAK2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistency of Responses and Continuations Generated by Large Language
  Models on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate remarkable capabilities in text
generation, yet their emotional consistency and semantic coherence in social
media contexts remain insufficiently understood. This study investigates how
LLMs handle emotional content and maintain semantic relationships through
continuation and response tasks using two open-source models: Gemma and Llama.
By analyzing climate change discussions from Twitter and Reddit, we examine
emotional transitions, intensity patterns, and semantic similarity between
human-authored and LLM-generated content. Our findings reveal that while both
models maintain high semantic coherence, they exhibit distinct emotional
patterns: Gemma shows a tendency toward negative emotion amplification,
particularly anger, while maintaining certain positive emotions like optimism.
Llama demonstrates superior emotional preservation across a broader spectrum of
affects. Both models systematically generate responses with attenuated
emotional intensity compared to human-authored content and show a bias toward
positive emotions in response tasks. Additionally, both models maintain strong
semantic similarity with original texts, though performance varies between
continuation and response tasks. These findings provide insights into LLMs'
emotional and semantic processing capabilities, with implications for their
deployment in social media contexts and human-AI interaction design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cultivating a Supportive Sphere: Designing Technology to Increase Social
  Support for Foster-Involved Youth <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ila Kumar, Craig Ferguson, Jiayi Wu, Rosalind W Picard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximately 400,000 youth in the US are living in foster care due to
experiences with abuse or neglect at home. For multiple reasons, these youth
often don't receive adequate social support from those around them. Despite
technology's potential, very little work has explored how these tools can
provide more support to foster-involved youth. To begin to fill this gap, we
worked with current and former foster-involved youth to develop the first
digital tool that aims to increase social support for this population, creating
a novel system in which users complete reflective check-ins in an online
community setting. We then conducted a pilot study with 15 current and former
foster-involved youth, comparing the effect of using the app for two weeks to
two weeks of no intervention. We collected qualitative and quantitative data,
which demonstrated that this type of interface can provide youth with types of
social support that are often not provided by foster care services and other
digital interventions. The paper details the motivation behind the app, the
trauma-informed design process, and insights gained from this initial
evaluation study. Finally, the paper concludes with recommendations for
designing digital tools that effectively provide social support to
foster-involved youth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the 28th ACM SIGCHI Conference on
  Computer-Supported Cooperative Work & Social Computing (CSCW 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To Trust or Distrust Trust Measures: Validating Questionnaires for Trust
  in AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Scharowski, Sebastian A. C. Perrig, Lena Fanya Aeschbach, Nick von Felten, Klaus Opwis, Philipp Wintersberger, Florian Brühlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the importance of trust in human-AI interactions, researchers must
adopt questionnaires from other disciplines that lack validation in the AI
context. Motivated by the need for reliable and valid measures, we investigated
the psychometric quality of two trust questionnaires, the Trust between People
and Automation scale (TPA) by Jian et al. (2000) and the Trust Scale for the AI
Context (TAI) by Hoffman et al. (2023). In a pre-registered online experiment
(N = 1485), participants observed interactions with trustworthy and
untrustworthy AI (autonomous vehicle and chatbot). Results support the
psychometric quality of the TAI while revealing opportunities to improve the
TPA, which we outline in our recommendations for using the two questionnaires.
Furthermore, our findings provide additional empirical evidence of trust and
distrust as two distinct constructs that may coexist independently. Building on
our findings, we highlight the opportunities and added value of measuring both
trust and distrust in human-AI research and advocate for further work on both
constructs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Narrative Player: Reviving Data Narratives with Visuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekai Shao, Leixian Shen, Haotian Li, Yi Shan, Huamin Qu, Yun Wang, Siming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-rich documents are commonly found across various fields such as
business, finance, and science. However, a general limitation of these
documents for reading is their reliance on text to convey data and facts.
Visual representation of text aids in providing a satisfactory reading
experience in comprehension and engagement. However, existing work emphasizes
presenting the insights of local text context, rather than fully conveying data
stories within the whole paragraphs and engaging readers. To provide readers
with satisfactory data stories, this paper presents Narrative Player, a novel
method that automatically revives data narratives with consistent and
contextualized visuals. Specifically, it accepts a paragraph and corresponding
data table as input and leverages LLMs to characterize the clauses and extract
contextualized data facts. Subsequently, the facts are transformed into a
coherent visualization sequence with a carefully designed optimization-based
approach. Animations are also assigned between adjacent visualizations to
enable seamless transitions. Finally, the visualization sequence, transition
animations, and audio narration generated by text-to-speech technologies are
rendered into a data video. The evaluation results showed that the
automatic-generated data videos were well-received by participants and experts
for enhancing reading.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TVCG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Outlines for <span class="highlight-title">Code</span>: Literate Programming in the LLM Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kensen Shi, Deniz Altınbüken, Saswat Anand, Mihai Christodorescu, Katja Grünwedel, Alexa Koenings, Sai Naidu, Anurag Pathak, Marc Rasi, Fredde Ribeiro, Brandon Ruffin, Siddhant Sanyam, Maxim Tabachnyk, Sara Toth, Roy Tu, Tobias Welp, Pengcheng Yin, Manzil Zaheer, Satish Chandra, Charles Sutton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose using natural language outlines as a novel modality and
interaction surface for providing AI assistance to developers throughout the
software development process. An NL outline for a code function comprises
multiple statements written in concise prose, which partition the code and
summarize its main ideas in the style of literate programming. Crucially, we
find that modern LLMs can generate accurate and high-quality NL outlines in
practice. Moreover, NL outlines enable a bidirectional sync between code and
NL, allowing changes in one to be automatically reflected in the other. We
discuss many use cases for NL outlines: they can accelerate understanding and
navigation of code and diffs, simplify code maintenance, augment code search,
steer code generation, and more. We then propose and compare multiple LLM
prompting techniques for generating outlines and ask professional developers to
judge outline quality. Finally, we present two case studies applying NL
outlines toward code review and malware detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction
  Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Fang, Xinle Deng, Kangwei Liu, Ningyu Zhang, Jingyang Qian, Penghui Yang, Xiaohui Fan, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models excel at interpreting complex natural language
instructions, enabling them to perform a wide range of tasks. In the life
sciences, single-cell RNA sequencing (scRNA-seq) data serves as the "language
of cellular biology", capturing intricate gene expression patterns at the
single-cell level. However, interacting with this "language" through
conventional tools is often inefficient and unintuitive, posing challenges for
researchers. To address these limitations, we present InstructCell, a
multi-modal AI copilot that leverages natural language as a medium for more
direct and flexible single-cell analysis. We construct a comprehensive
multi-modal instruction dataset that pairs text-based instructions with
scRNA-seq profiles from diverse tissues and species. Building on this, we
develop a multi-modal cell language architecture capable of simultaneously
interpreting and processing both modalities. InstructCell empowers researchers
to accomplish critical tasks-such as cell type annotation, conditional
pseudo-cell generation, and drug sensitivity prediction-using straightforward
natural language commands. Extensive evaluations demonstrate that InstructCell
consistently meets or exceeds the performance of existing single-cell
foundation models, while adapting to diverse experimental conditions. More
importantly, InstructCell provides an accessible and intuitive tool for
exploring complex single-cell data, lowering technical barriers and enabling
deeper biological insights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages; 13 figures; Code: https://github.com/zjunlp/Instructcell,
  Models: https://huggingface.co/zjunlp/Instructcell-chat,
  https://huggingface.co/zjunlp/InstructCell-instruct</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-14T00:00:00Z">2025-01-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Quality Challenges in Deep Learning: The Role of MLOps and
  Domain Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago del Rey, Adrià Medina, Xavier Franch, Silverio Martínez-Fernández
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) systems present unique challenges in software engineering,
especially concerning quality attributes like correctness and resource
efficiency. While DL models achieve exceptional performance in specific tasks,
engineering DL-based systems is still essential. The effort, cost, and
potential diminishing returns of continual improvements must be carefully
evaluated, as software engineers often face the critical decision of when to
stop refining a system relative to its quality attributes. This experience
paper explores the role of MLOps practices -- such as monitoring and experiment
tracking -- in creating transparent and reproducible experimentation
environments that enable teams to assess and justify the impact of design
decisions on quality attributes. Furthermore, we report on experiences
addressing the quality challenges by embedding domain knowledge into the design
of a DL model and its integration within a larger system. The findings offer
actionable insights into not only the benefits of domain knowledge and MLOps
but also the strategic consideration of when to limit further optimizations in
DL projects to maximize overall system quality and reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, accepted to the 4th International Conference on AI
  Engineering - Software Engineering for AI (CAIN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engineering LLM Powered Multi-agent Framework for Autonomous CloudOps <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kannan Parthasarathy, Karthik Vaidhyanathan, Rudra Dhar, Venkat Krishnamachari, Basil Muhammed, Adyansh Kakran, Sreemaee Akshathala, Shrikara Arun, Sumant Dubey, Mohan Veerubhotla, Amey Karan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud Operations (CloudOps) is a rapidly growing field focused on the
automated management and optimization of cloud infrastructure which is
essential for organizations navigating increasingly complex cloud environments.
MontyCloud Inc. is one of the major companies in the CloudOps domain that
leverages autonomous bots to manage cloud compliance, security, and continuous
operations. To make the platform more accessible and effective to the
customers, we leveraged the use of GenAI.
  Developing a GenAI-based solution for autonomous CloudOps for the existing
MontyCloud system presented us with various challenges such as i) diverse data
sources; ii) orchestration of multiple processes; and iii) handling complex
workflows to automate routine tasks. To this end, we developed MOYA, a
multi-agent framework that leverages GenAI and balances autonomy with the
necessary human control. This framework integrates various internal and
external systems and is optimized for factors like task orchestration,
security, and error mitigation while producing accurate, reliable, and relevant
insights by utilizing Retrieval Augmented Generation (RAG). Evaluations of our
multi-agent system with the help of practitioners as well as using automated
checks demonstrate enhanced accuracy, responsiveness, and effectiveness over
non-agentic approaches across complex workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted as full paper to CAIN 2025
  (https://conf.researchr.org/home/cain-2025), co-located with ICSE 2025
  (https://conf.researchr.org/home/icse-2025). The paper was submitted to CAIN
  for review on 9 November 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CWEval: Outcome-driven Evaluation on Functionality and Security of LLM
  <span class="highlight-title">Code</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinjun Peng, Leyi Cui, Kele Huang, Junfeng Yang, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have significantly aided developers by
generating or assisting in code writing, enhancing productivity across various
tasks. While identifying incorrect code is often straightforward, detecting
vulnerabilities in functionally correct code is more challenging, especially
for developers with limited security knowledge, which poses considerable
security risks of using LLM-generated code and underscores the need for robust
evaluation benchmarks that assess both functional correctness and security.
Current benchmarks like CyberSecEval and SecurityEval attempt to solve it but
are hindered by unclear and impractical specifications, failing to assess both
functionality and security accurately. To tackle these deficiencies, we
introduce CWEval, a novel outcome-driven evaluation framework designed to
enhance the evaluation of secure code generation by LLMs. This framework not
only assesses code functionality but also its security simultaneously with
high-quality task specifications and outcome-driven test oracles which provides
high accuracy. Coupled with CWEval-bench, a multilingual, security-critical
coding benchmark, CWEval provides a rigorous empirical security evaluation on
LLM-generated code, overcoming previous benchmarks' shortcomings. Through our
evaluations, CWEval reveals a notable portion of functional but insecure code
produced by LLMs, and shows a serious inaccuracy of previous evaluations,
ultimately contributing significantly to the field of secure code generation.
We open-source our artifact at: https://github.com/Co1lin/CWEval .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in LLM4Code 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Executable Multi-Layered Software 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Radosky, Ivan Polasek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel software visualisation and animation method,
manifested in a prototype software tool - AnimArch. The introduced method is
based on model fusion of static and dynamic models. The static model is
represented by class diagram while the dynamic model is represented by source
code written in high-level Object Action Language from xUML (executable UML).
The class diagram defines architecture that is animated in response to
real-time execution of the source code. Moreover, additional object diagram
layer represents all object instances present in runtime. The AnimArch also
features source code generation to Python, to bridge the gap from design to
implementation. This paper provides detailed description of the modelling
method and screenshots of the accompanying software tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I Can Find You in Seconds! Leveraging <span class="highlight-title">Large Language Model</span>s for <span class="highlight-title">Code</span>
  Authorship Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soohyeon Choi, Yong Kiam Tan, Mark Huasong Meng, Mohamed Ragab, Soumik Mondal, David Mohaisen, Khin Mi Mi Aung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source code authorship attribution is important in software forensics,
plagiarism detection, and protecting software patch integrity. Existing
techniques often rely on supervised machine learning, which struggles with
generalization across different programming languages and coding styles due to
the need for large labeled datasets. Inspired by recent advances in natural
language authorship analysis using large language models (LLMs), which have
shown exceptional performance without task-specific tuning, this paper explores
the use of LLMs for source code authorship attribution.
  We present a comprehensive study demonstrating that state-of-the-art LLMs can
successfully attribute source code authorship across different languages. LLMs
can determine whether two code snippets are written by the same author with
zero-shot prompting, achieving a Matthews Correlation Coefficient (MCC) of
0.78, and can attribute code authorship from a small set of reference code
snippets via few-shot learning, achieving MCC of 0.77. Additionally, LLMs show
some adversarial robustness against misattribution attacks.
  Despite these capabilities, we found that naive prompting of LLMs does not
scale well with a large number of authors due to input token limitations. To
address this, we propose a tournament-style approach for large-scale
attribution. Evaluating this approach on datasets of C++ (500 authors, 26,355
samples) and Java (686 authors, 55,267 samples) code from GitHub, we achieve
classification accuracy of up to 65% for C++ and 68.7% for Java using only one
reference per author. These results open new possibilities for applying LLMs to
code authorship attribution in cybersecurity and software engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automating Explanation Need Management in App <span class="highlight-title">Review</span>s: A Case Study from
  the Navigation App Industry <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Obaidi, Nicolas Voß, Jakob Droste, Hannah Deters, Marc Herrmann, Jannik Fischbach, Kurt Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing explanations in response to user reviews is a time-consuming and
repetitive task for companies, as many reviews present similar issues requiring
nearly identical responses. To improve efficiency, this paper proposes a
semi-automated approach to managing explanation needs in user reviews. The
approach leverages taxonomy categories to classify reviews and assign them to
relevant internal teams or sources for responses. 2,366 app reviews from the
Google Play Store and Apple App Store were scraped and analyzed using a word
and phrase filtering system to detect explanation needs. The detected needs
were categorized and assigned to specific internal teams at the company
Graphmasters GmbH, using a hierarchical assignment strategy that prioritizes
the most relevant teams. Additionally, external sources, such as existing
support articles and past review responses, were integrated to provide
comprehensive explanations. The system was evaluated through interviews and
surveys with the Graphmasters support team, which consists of four employees.
The results showed that the hierarchical assignment method improved the
accuracy of team assignments, with correct teams being identified in 79.2% of
cases. However, challenges in interrater agreement and the need for new
responses in certain cases, particularly for Apple App Store reviews, were
noted. Future work will focus on refining the taxonomy and enhancing the
automation process to reduce manual intervention further.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at the Software Engineering in Practice
  (SEIP) track of the 47th International Conference on Software Engineering
  (ICSE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Ehnanced Holonic Architecture for Ad-Hoc Scalable SoS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Ashfaq, Ahmed R. Sadik, Tommi Mikkonen, Muhammad Waseem, Niko Mäkitalo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As modern system of systems (SoS) become increasingly adaptive and human
centred, traditional architectures often struggle to support interoperability,
reconfigurability, and effective human system interaction. This paper addresses
these challenges by advancing the state of the art holonic architecture for
SoS, offering two main contributions to support these adaptive needs. First, we
propose a layered architecture for holons, which includes reasoning,
communication, and capabilities layers. This design facilitates seamless
interoperability among heterogeneous constituent systems by improving data
exchange and integration. Second, inspired by principles of intelligent
manufacturing, we introduce specialised holons namely, supervisor, planner,
task, and resource holons aimed at enhancing the adaptability and
reconfigurability of SoS. These specialised holons utilise large language
models within their reasoning layers to support decision making and ensure real
time adaptability. We demonstrate our approach through a 3D mobility case study
focused on smart city transportation, showcasing its potential for managing
complex, multimodal SoS environments. Additionally, we propose evaluation
methods to assess the architecture efficiency and scalability,laying the
groundwork for future empirical validations through simulations and real world
implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Many-Objective Neuroevolution for Testing Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patric Feldmeier, Katrin Schmelz, Gordon Fraser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating tests for games is challenging due to the high degree of
randomisation inherent to games and hard-to-reach program states that require
sophisticated gameplay. The test generator NEATEST tackles these challenges by
combining search-based software testing principles with neuroevolution to
optimise neural networks that serve as test cases. However, since NEATEST is
designed as a single-objective algorithm, it may require a long time to cover
fairly simple program states or may even get stuck trying to reach unreachable
program states. In order to resolve these shortcomings of NEATEST, this work
aims to transform the algorithm into a many-objective search algorithm that
targets several program states simultaneously. To this end, we combine the
neuroevolution algorithm NEATEST with the two established search-based software
testing algorithms, MIO and MOSA. Moreover, we adapt the existing
many-objective neuroevolution algorithm NEWS/D to serve as a test generator.
Our experiments on a dataset of 20 SCRATCH programs show that extending NEATEST
to target several objectives simultaneously increases the average branch
coverage from 75.88% to 81.33% while reducing the required search time by
93.28%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 18th IEEE International Conference on Software Testing,
  Verification and Validation (ICST) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Metamemory Mechanisms for Enhanced Data-Free <span class="highlight-title">Code</span> Generation
  in LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Wang, Liang Ding, Yibing Zhan, Yong Luo, Zheng He, Dapeng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated code generation using large language models (LLMs) has gained
attention due to its efficiency and adaptability. However, real-world coding
tasks or benchmarks like HumanEval and StudentEval often lack dedicated
training datasets, challenging existing few-shot prompting approaches that rely
on reference examples. Inspired by human metamemory-a cognitive process
involving recall and evaluation-we present a novel framework (namely M^2WF) for
improving LLMs' one-time code generation. This approach enables LLMs to
autonomously generate, evaluate, and utilize synthetic examples to enhance
reliability and performance. Unlike prior methods, it minimizes dependency on
curated data and adapts flexibly to various coding scenarios. Our experiments
demonstrate significant improvements in coding benchmarks, offering a scalable
and robust solution for data-free environments. The code and framework will be
publicly available on GitHub and HuggingFace.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Repository-Level <span class="highlight-title">Code</span> Summarization for Business
  Applications Using Local LLMs <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nilesh Dhulshette, Sapan Shah, Vinay Kulkarni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large-scale software development, understanding the functionality and
intent behind complex codebases is critical for effective development and
maintenance. While code summarization has been widely studied, existing methods
primarily focus on smaller code units, such as functions, and struggle with
larger code artifacts like files and packages. Additionally, current
summarization models tend to emphasize low-level implementation details, often
overlooking the domain and business context that are crucial for real-world
applications. This paper proposes a two-step hierarchical approach for
repository-level code summarization, tailored to business applications. First,
smaller code units such as functions and variables are identified using syntax
analysis and summarized with local LLMs. These summaries are then aggregated to
generate higher-level file and package summaries. To ensure the summaries are
grounded in business context, we design custom prompts that capture the
intended purpose of code artifacts based on the domain and problem context of
the business application. We evaluate our approach on a business support system
(BSS) for the telecommunications domain, showing that syntax analysis-based
hierarchical summarization improves coverage, while business-context grounding
enhances the relevance of the generated summaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at LLM4Code@ICSE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Provider Bias in <span class="highlight-title">Large Language Model</span>s for <span class="highlight-title">Code</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Zhang, Juan Zhai, Shiqing Ma, Qingshuang Bao, Weipeng Jiang, Chao Shen, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as the new recommendation engines,
outperforming traditional methods in both capability and scope, particularly in
code generation applications. Our research reveals a novel provider bias in
LLMs, namely without explicit input prompts, these models show systematic
preferences for services from specific providers in their recommendations
(e.g., favoring Google Cloud over Microsoft Azure). This bias holds significant
implications for market dynamics and societal equilibrium, potentially
promoting digital monopolies. It may also deceive users and violate their
expectations, leading to various consequences. This paper presents the first
comprehensive empirical study of provider bias in LLM code generation. We
develop a systematic methodology encompassing an automated pipeline for dataset
generation, incorporating 6 distinct coding task categories and 30 real-world
application scenarios. Our analysis encompasses over 600,000 LLM-generated
responses across seven state-of-the-art models, utilizing approximately 500
million tokens (equivalent to \$5,000+ in computational costs). The study
evaluates both the generated code snippets and their embedded service provider
selections to quantify provider bias. Additionally, we conduct a comparative
analysis of seven debiasing prompting techniques to assess their efficacy in
mitigating these biases. Our findings demonstrate that LLMs exhibit significant
provider preferences, predominantly favoring services from Google and Amazon,
and can autonomously modify input code to incorporate their preferred providers
without users' requests. Notably, we observe discrepancies between providers
recommended in conversational contexts versus those implemented in generated
code. The complete dataset and analysis results are available in our
repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Code</span>CoR: An LLM-Based Self-Reflective Multi-Agent Framework for <span class="highlight-title">Code</span>
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruwei Pan, Hongyu Zhang, Chao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code generation aims to produce code that fulfills requirements written in
natural languages automatically. Large language Models (LLMs) like ChatGPT have
demonstrated promising effectiveness in this area. Nonetheless, these LLMs
often fail to ensure the syntactic and semantic correctness of the generated
code. Recently, researchers proposed multi-agent frameworks that guide LLMs
with different prompts to analyze programming tasks, generate code, perform
testing in a sequential workflow. However, the performance of the workflow is
not robust as the code generation depends on the performance of each agent. To
address this challenge, we propose CodeCoR, a self-reflective multi-agent
framework that evaluates the effectiveness of each agent and their
collaborations. Specifically, for a given task description, four agents in
CodeCoR generate prompts, code, test cases, and repair advice, respectively.
Each agent generates more than one output and prunes away the low-quality ones.
The generated code is tested in the local environment: the code that fails to
pass the generated test cases is sent to the repair agent and the coding agent
re-generates the code based on repair advice. Finally, the code that passes the
most number of generated test cases is returned to users. Our experiments on
four widely used datasets, HumanEval, HumanEval-ET, MBPP, and MBPP-ET,
demonstrate that CodeCoR significantly outperforms existing baselines (e.g.,
CodeCoT and MapCoder), achieving an average Pass@1 score of 77.8%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Far are App Secrets from Being Stolen? A Case Study on Android 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lili Wei, Heqing Huang, Shing-Chi Cheung, Kevin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Android apps can hold secret strings of themselves such as cloud service
credentials or encryption keys. Leakage of such secret strings can induce
unprecedented consequences like monetary losses or leakage of user private
information. In practice, various security issues were reported because many
apps failed to protect their secrets. However, little is known about the types,
usages, exploitability, and consequences of app secret leakage issues. While a
large body of literature has been devoted to studying user private information
leakage, there is no systematic study characterizing app secret leakage issues.
How far are Android app secrets from being stolen?
  To bridge this gap, we conducted the first systematic study to characterize
app secret leakage issues in Android apps based on 575 potential app secrets
sampled from 14,665 popular Android apps on Google Play. We summarized the
common categories of leaked app secrets, assessed their security impacts and
disclosed app bad practices in storing app secrets. We devised a text mining
strategy using regular expressions and demonstrated that numerous app secrets
can be easily stolen, even from the highly popular Android apps on Google. In a
follow-up study, we harvested 3,711 distinct exploitable app secrets through
automatic analysis. Our findings highlight the prevalence of this problem and
call for greater attention to app secret protection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A pre-print version of the paper. It is accepted to Empirical
  Software Engineering (EMSE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empirical Analysis of Pull Requests for Google Summer of <span class="highlight-title">Code</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saheed Popoola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Internship and industry-affiliated capstone projects are popular ways to
expose students to real world experiences and bridge the gap between academic
training and industry requirements. However, these two approaches often require
active industry collaboration, and many students struggle to find industry
placements. Open-source contributions are a crucial alternative to gain real
world experience, earn publicly verifiable contribution with real-world impact,
and learn from experienced open-source contributors. The Google Summer of Code
(GSoC) is a global initiative that matches students or new contributors with
experienced mentors to work on open-source projects. The program aims to
introduce the students to open-source development, help them gain valuable
skills under the guidance of mentors, and hopefully encourage them to continue
contributing to open-source projects. The realization of the program objectives
will provide a continuous pool of talented new contributors necessary for
maintaining open-source projects. This study presents an empirical analysis of
pull requests created by interns during the GSoC program. We extracted and
analyzed 17,232 pull requests from 2,456 interns across 1,937 open-source
projects. The results show most tasks involve both code-intensive activities
like adding new features and fixing bugs, as well as non-code tasks like
updating documentation and restructuring the codebase. Feedback from reviewers
covers code functionality and programming logic, testing coverage, error
handling, code readability, and adherence to best practices. Finally, we
discuss the implications of these results for software engineering education.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study of Full Apps and Lite Apps for Android 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutian Tang, Xiaojiang Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  App developers aim to create apps that cater to the needs of different types
of users. This development approach, also known as the "one-size-fits-all"
strategy, involves combining various functionalities into one app. However,
this approach has drawbacks, such as lower conversion rates, slower download
speed, larger attack surfaces, and lower update rates. To address these issues,
developers have created "lite" versions to attract new users and enhance the
user experience. Despite this, there has been no study conducted to examine the
relationship between lite and full apps. To address this gap, we present a
comparative study of lite apps, exploring the similarities and differences
between lite and full apps from various perspectives. Our findings indicate
that most existing lite apps fail to fulfill their intended goals (e.g.,
smaller in size, faster to download, and using less data). Our study also
reveals the potential security risks associated with lite apps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COOL: Efficient and Reliable Chain-Oriented Objective Logic with Neural
  Networks Feedback Control for Program Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jipeng Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Program synthesis methods, whether formal or neural-based, lack fine-grained
control and flexible modularity, which limits their adaptation to complex
software development. These limitations stem from rigid Domain-Specific
Language (DSL) frameworks and neural network incorrect predictions. To this
end, we propose the Chain of Logic (CoL), which organizes the synthesis process
into an activity flow and provides heuristic control to guide the process.
Furthermore, by integrating neural networks with libraries and introducing a
Neural Network Feedback Control (NNFC) mechanism, our approach modularizes
synthesis and mitigates the impact of neural network mispredictions.
Experiments on relational and symbolic synthesis tasks show that CoL
significantly enhances the efficiency and reliability of DSL program synthesis
across multiple metrics. Specifically, CoL improves accuracy by 70% while
reducing tree operations by 91% and time by 95%. Additionally, NNFC further
boosts accuracy by 6%, with a 64% reduction in tree operations under
challenging conditions such as insufficient training data, increased
difficulty, and multidomain synthesis. These improvements confirm COOL as a
highly efficient and reliable program synthesis framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Failure Diagnosis in Microservice Systems: A Comprehensive <span class="highlight-title">Survey</span> and
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenglin Zhang, Sibo Xia, Wenzhao Fan, Binpeng Shi, Xiao Xiong, Zhenyu Zhong, Minghua Ma, Yongqian Sun, Dan Pei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Widely adopted for their scalability and flexibility, modern microservice
systems present unique failure diagnosis challenges due to their independent
deployment and dynamic interactions. This complexity can lead to cascading
failures that negatively impact operational efficiency and user experience.
Recognizing the critical role of fault diagnosis in improving the stability and
reliability of microservice systems, researchers have conducted extensive
studies and achieved a number of significant results. This survey provides an
exhaustive review of 98 scientific papers from 2003 to the present, including a
thorough examination and elucidation of the fundamental concepts, system
architecture, and problem statement. It also includes a qualitative analysis of
the dimensions, providing an in-depth discussion of current best practices and
future directions, aiming to further its development and application. In
addition, this survey compiles publicly available datasets, toolkits, and
evaluation metrics to facilitate the selection and validation of techniques for
practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nigerian Software Engineer or American Data Scientist? GitHub Profile
  Recruitment Bias in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Nakano, Kazumasa Shimari, Raula Gaikovina Kula, Christoph Treude, Marc Cheong, Kenichi Matsumoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have taken the world by storm, demonstrating
their ability not only to automate tedious tasks, but also to show some degree
of proficiency in completing software engineering tasks. A key concern with
LLMs is their "black-box" nature, which obscures their internal workings and
could lead to societal biases in their outputs. In the software engineering
context, in this early results paper, we empirically explore how well LLMs can
automate recruitment tasks for a geographically diverse software team. We use
OpenAI's ChatGPT to conduct an initial set of experiments using GitHub User
Profiles from four regions to recruit a six-person software development team,
analyzing a total of 3,657 profiles over a five-year period (2019-2023).
Results indicate that ChatGPT shows preference for some regions over others,
even when swapping the location strings of two profiles (counterfactuals).
Furthermore, ChatGPT was more likely to assign certain developer roles to users
from a specific country, revealing an implicit bias. Overall, this study
reveals insights into the inner workings of LLMs and has implications for
mitigating such societal biases in these models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OptiChat: Bridging Optimization Models and Practitioners with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Chen, Gonzalo Esteban Constante-Flores, Krishna Sri Ipsit Mantri, Sai Madhukiran Kompalli, Akshdeep Singh Ahluwalia, Can Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization models have been applied to solve a wide variety of
decision-making problems. These models are usually developed by optimization
experts but are used by practitioners without optimization expertise in various
application domains. As a result, practitioners often struggle to interact with
and draw useful conclusions from optimization models independently. To fill
this gap, we introduce OptiChat, a natural language dialogue system designed to
help practitioners interpret model formulation, diagnose infeasibility, analyze
sensitivity, retrieve information, evaluate modifications, and provide
counterfactual explanations. By augmenting large language models (LLMs) with
functional calls and code generation tailored for optimization models, we
enable seamless interaction and minimize the risk of hallucinations in
OptiChat. We develop a new dataset to evaluate OptiChat's performance in
explaining optimization models. Experiments demonstrate that OptiChat
effectively bridges the gap between optimization models and practitioners,
delivering autonomous, accurate, and instant responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empathetic Conversational Agents: Utilizing Neural and Physiological
  Signals for Enhanced Empathetic Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nastaran Saffaryazdi, Tamil Selvan Gunasekaran, Kate Laveys, Elizabeth Broadbent, Mark Billinghurst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational agents (CAs) are revolutionizing human-computer interaction by
evolving from text-based chatbots to empathetic digital humans (DHs) capable of
rich emotional expressions. This paper explores the integration of neural and
physiological signals into the perception module of CAs to enhance empathetic
interactions. By leveraging these cues, the study aims to detect emotions in
real-time and generate empathetic responses and expressions. We conducted a
user study where participants engaged in conversations with a DH about
emotional topics. The DH responded and displayed expressions by mirroring
detected emotions in real-time using neural and physiological cues. The results
indicate that participants experienced stronger emotions and greater engagement
during interactions with the Empathetic DH, demonstrating the effectiveness of
incorporating neural and physiological signals for real-time emotion
recognition. However, several challenges were identified, including recognition
accuracy, emotional transition speeds, individual personality effects, and
limitations in voice tone modulation. Addressing these challenges is crucial
for further refining Empathetic DHs and fostering meaningful connections
between humans and artificial entities. Overall, this research advances
human-agent interaction and highlights the potential of real-time neural and
physiological emotion recognition in creating empathetic DHs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Zero-Shot User Intent Recognition in Shared Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atharv Belsare, Zohre Karimi, Connor Mattson, Daniel S. Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental challenge of shared autonomy is to use high-DoF robots to
assist, rather than hinder, humans by first inferring user intent and then
empowering the user to achieve their intent. Although successful, prior methods
either rely heavily on a priori knowledge of all possible human intents or
require many demonstrations and interactions with the human to learn these
intents before being able to assist the user. We propose and study a zero-shot,
vision-only shared autonomy (VOSA) framework designed to allow robots to use
end-effector vision to estimate zero-shot human intents in conjunction with
blended control to help humans accomplish manipulation tasks with unknown and
dynamically changing object locations. To demonstrate the effectiveness of our
VOSA framework, we instantiate a simple version of VOSA on a Kinova Gen3
manipulator and evaluate our system by conducting a user study on three
tabletop manipulation tasks. The performance of VOSA matches that of an oracle
baseline model that receives privileged knowledge of possible human intents
while also requiring significantly less effort than unassisted teleoperation.
In more realistic settings, where the set of possible human intents is fully or
partially unknown, we demonstrate that VOSA requires less human effort and time
than baseline approaches while being preferred by a majority of the
participants. Our results demonstrate the efficacy and efficiency of using
off-the-shelf vision algorithms to enable flexible and beneficial shared
control of a robot manipulator. Code and videos available here:
https://sites.google.com/view/zeroshot-sharedautonomy/home.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, Accepted to IEEE/ACM International Conference on
  Human-Robot Interaction (HRI), 2025. Equal Contribution from the first three
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jigsaw: Authoring Immersive Storytelling Experiences with Augmented
  Reality and Internet of Things 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhang, Daekun Kim, Youjean Cho, Ava Robinson, Yu Jiang Tham, Rajan Vaish, Andrés Monroy-Hernández
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Augmented Reality (AR) presents new opportunities for immersive storytelling.
However, this immersiveness faces two main hurdles. First, AR's immersive
quality is often confined to visual elements, such as pixels on a screen.
Second, crafting immersive narratives is complex and generally beyond the reach
of amateurs due to the need for advanced technical skills. We introduce Jigsaw,
a system that empowers beginners to both experience and craft immersive
stories, blending virtual and physical elements. Jigsaw uniquely combines
mobile AR with readily available Internet-of-things (IoT) devices. We conducted
a qualitative study with 20 participants to assess Jigsaw's effectiveness in
both consuming and creating immersive narratives. The results were promising:
participants not only successfully created their own immersive stories but also
found the playback of three such stories deeply engaging. However, sensory
overload emerged as a significant challenge in these experiences. We discuss
design trade-offs and considerations for future endeavors in immersive
storytelling involving AR and IoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 2024 CHI Conference on Human Factors in
  Computing Systems (CHI '24). 14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cognitive Assessment and Training in Extended Reality: Multimodal
  Systems, Clinical Utility, and Current Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Palmira Victoria González-Erena, Sara Fernández-Guinea, Panagiotis Kourtesis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extended reality (XR) technologies-encompassing virtual reality (VR),
augmented reality (AR), and mixed reality (MR) are transforming cognitive
assessment and training by offering immersive, interactive environments that
simulate real-world tasks. XR enhances ecological validity while enabling
real-time, multimodal data collection through tools such as galvanic skin
response (GSR), electroencephalography (EEG), eye tracking (ET), hand tracking,
and body tracking. This allows for a more comprehensive understanding of
cognitive and emotional processes, as well as adaptive, personalized
interventions for users. Despite these advancements, current XR applications
often underutilize the full potential of multimodal integration, relying
primarily on visual and auditory inputs. Challenges such as cybersickness,
usability concerns, and accessibility barriers further limit the widespread
adoption of XR tools in cognitive science and clinical practice. This review
examines XR-based cognitive assessment and training, focusing on its advantages
over traditional methods, including ecological validity, engagement, and
adaptability. It also explores unresolved challenges such as system usability,
cost, and the need for multimodal feedback integration. The review concludes by
identifying opportunities for optimizing XR tools to improve cognitive
evaluation and rehabilitation outcomes, particularly for diverse populations,
including older adults and individuals with cognitive impairments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CG-MER: A Card Game-based Multimodal dataset for Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nessrine Farhat, Amine Bohi, Leila Ben Letaifa, Rim Slama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of affective computing has seen significant advancements in
exploring the relationship between emotions and emerging technologies. This
paper presents a novel and valuable contribution to this field with the
introduction of a comprehensive French multimodal dataset designed specifically
for emotion recognition. The dataset encompasses three primary modalities:
facial expressions, speech, and gestures, providing a holistic perspective on
emotions. Moreover, the dataset has the potential to incorporate additional
modalities, such as Natural Language Processing (NLP) to expand the scope of
emotion recognition research. The dataset was curated through engaging
participants in card game sessions, where they were prompted to express a range
of emotions while responding to diverse questions. The study included 10
sessions with 20 participants (9 females and 11 males). The dataset serves as a
valuable resource for furthering research in emotion recognition and provides
an avenue for exploring the intricate connections between human emotions and
digital technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures and 4 tables. Sixteenth International Conference
  on Machine Vision (ICMV 2023), Yerevan, Armenia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Phase Model of Misinformation Interventions <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hendrik Heuer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misinformation is a challenging problem. This paper provides the first
systematic interdisciplinary investigation of technical and non-technical
interventions against misinformation. It combines interviews and a survey to
understand which interventions are accepted across academic disciplines and
approved by misinformation experts. Four interventions are supported by more
than two in three misinformation experts: promoting media literacy, education
in schools and universities, finding information about claims, and finding
sources for claims. The most controversial intervention is deleting
misinformation. We discuss the potentials and risks of all interventions.
Education-based interventions are perceived as the most helpful by
misinformation experts. Interventions focused on providing evidence are also
widely perceived as helpful. We discuss them as scalable and always available
interventions that empower users to independently identify misinformation. We
also introduce the Phase Model of Misinformation Interventions that helps
practitioners make informed decisions about which interventions to focus on and
how to best combine interventions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CSCW 2025, April 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Symbiotic AI: <span class="highlight-title">Review</span>ing the AI Act for a Human-Centred,
  Principle-Based Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miriana Calvano, Antonio Curci, Giuseppe Desolda, Andrea Esposito, Rosa Lanzilotti, Antonio Piccinno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) spreads quickly as new technologies and services
take over modern society. The need to regulate AI design, development, and use
is strictly necessary to avoid unethical and potentially dangerous consequences
to humans. The European Union (EU) has released a new legal framework, the AI
Act, to regulate AI by undertaking a risk-based approach to safeguard humans
during interaction. At the same time, researchers offer a new perspective on AI
systems, commonly known as Human-Centred AI (HCAI), highlighting the need for a
human-centred approach to their design. In this context, Symbiotic AI (a
subtype of HCAI) promises to enhance human capabilities through a deeper and
continuous collaboration between human intelligence and AI. This article
presents the results of a Systematic Literature Review (SLR) that aims to
identify principles that characterise the design and development of Symbiotic
AI systems while considering humans as the core of the process. Through content
analysis, four principles emerged from the review that must be applied to
create Human-Centred AI systems that can establish a symbiotic relationship
with humans. In addition, current trends and challenges were defined to
indicate open questions that may guide future research for the development of
SAI systems that comply with the AI Act.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First version: 17 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Guide Dog: Egocentric Path Prediction on Smartphone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Jadhav, Jeffery Cao, Abhishree Shetty, Urvashi Priyam Kumar, Aditi Sharma, Ben Sukboontip, Jayant Sravan Tamarapalli, Jingyi Zhang, Anirudh Koul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces AI Guide Dog (AIGD), a lightweight egocentric
navigation assistance system for visually impaired individuals, designed for
real-time deployment on smartphones. AIGD addresses key challenges in blind
navigation by employing a vision-only, multi-label classification approach to
predict directional commands, ensuring safe traversal across diverse
environments. We propose a novel technique to enable goal-based outdoor
navigation by integrating GPS signals and high-level directions, while also
addressing uncertain multi-path predictions for destination-free indoor
navigation. Our generalized model is the first navigation assistance system to
handle both goal-oriented and exploratory navigation scenarios across indoor
and outdoor settings, establishing a new state-of-the-art in blind navigation.
We present methods, datasets, evaluations, and deployment insights to encourage
further innovations in assistive navigation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessment of Personalized Learning in Immersive and Intelligent Virtual
  Classroom on Student Engagement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Weng, Yiming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As trends in education evolve, personalized learning has transformed
individuals' engagement with knowledge and skill development. In the digital
age, state-of-the-art technologies have been increasingly integrated into
classrooms to support intelligent education and foster personalized learning
experiences. One promising approach is the use of eye-tracking technology to
evaluate student engagement in intelligent virtual classrooms. This paper
explores the assessment of personalized learning in the virtual classroom and
its impact on student engagement through the eye movement paradigm. The study
aims to provide insights into how personalized learning approaches can enhance
student participation, motivation, and academic performance in the online
learning environment. Through a comprehensive literature review, case study,
and data analysis, the paper examines the key elements of personalized
learning, the methods of assessment, and the resulting effects on student
engagement. The findings suggest that the eye movement paradigm has the
potential to assess student engagement and promote better educational outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unexploited Information Value in Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Guo, Yifan Wu, Jason Hartline, Jessica Hullman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans and AIs are often paired on decision tasks with the expectation of
achieving complementary performance -- where the combination of human and AI
outperforms either one alone. However, how to improve performance of a human-AI
team is often not clear without knowing more about what particular information
and strategies each agent employs. In this paper, we propose a model based in
statistical decision theory to analyze human-AI collaboration from the
perspective of what information could be used to improve a human or AI
decision. We demonstrate our model on a deepfake detection task to investigate
seven video-level features by their unexploited value of information. We
compare the human alone, AI alone and human-AI team and offer insights on how
the AI assistance impacts people's usage of the information and what
information that the AI exploits well might be useful for improving human
decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-Automation: How Labor-Offsetting Technologies Reconfigure Roles
  and Relationships in Frontline Retail Work <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pegah Moradi, Karen Levy, Cristobal Cheyre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-service machines are a form of pseudo-automation; rather than actually
automate tasks, they offset them to unpaid customers. Typically implemented for
customer convenience and to reduce labor costs, self-service is often
criticized for worsening customer service and increasing loss and theft for
retailers. Though millions of frontline service workers continue to interact
with these technologies on a day-to-day basis, little is known about how these
machines change the nature of frontline labor. Through interviews with current
and former cashiers who work with self-checkout technologies, we investigate
how technology that offsets labor from an employee to a customer can
reconfigure frontline work. We find three changes to cashiering tasks as a
result of self-checkout: (1) Working at self-checkout involved parallel demands
from multiple customers, (2) self-checkout work was more problem-oriented
(including monitoring and policing customers), and (3) traditional checkout
began to become more demanding as easier transactions were filtered to
self-checkout. As their interactions with customers became more focused on
problem solving and rule enforcement, cashiers were often positioned as
adversaries to customers at self-checkout. To cope with perceived
adversarialism, cashiers engaged in a form of relational patchwork, using
techniques like scapegoating the self-checkout machine and providing excessive
customer service in order to maintain positive customer interactions in the
face of potential conflict. Our findings highlight how even under
pseudo-automation, workers must engage in relational work to manage and mend
negative human-to-human interactions so that machines can be properly
implemented in context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pegah Moradi, Karen Levy, and Cristobal Cheyre. 2025.
  Pseudo-Automation: How Labor-Offsetting Technologies Reconfigure Roles and
  Relationships in Frontline Retail Work. Proc. ACM Hum.-Comput. Interact. 9,
  2, Article CSCW153 (April 2025), 21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaTalk: Efficient Holistic Gesture Synthesis with Selective State
  Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09471v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09471v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, Ronghui Li, Yachao Zhang, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gesture synthesis is a vital realm of human-computer interaction, with
wide-ranging applications across various fields like film, robotics, and
virtual reality. Recent advancements have utilized the diffusion model and
attention mechanisms to improve gesture synthesis. However, due to the high
computational complexity of these techniques, generating long and diverse
sequences with low latency remains a challenge. We explore the potential of
state space models (SSMs) to address the challenge, implementing a two-stage
modeling strategy with discrete motion priors to enhance the quality of
gestures. Leveraging the foundational Mamba block, we introduce MambaTalk,
enhancing gesture diversity and rhythm through multimodal integration.
Extensive experiments demonstrate that our method matches or exceeds the
performance of state-of-the-art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurlPS 2024, Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Virtual Reflections on a Dynamic 2D Eye Model Improve Spatial Reference
  Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matti Krüger, Yutaka Oshima, Yu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visible orientation of human eyes creates some transparency about
people's spatial attention and other mental states. This leads to a dual role
for the eyes as a means of sensing and communication. Accordingly, artificial
eye models are being explored as communication media in human-machine
interaction scenarios. One challenge in the use of eye models for communication
consists of resolving spatial reference ambiguities, especially for
screen-based models. Here, we introduce an approach for overcoming this
challenge through the introduction of reflection-like features that are
contingent on artificial eye movements. We conducted a user study with 30
participants in which participants had to use spatial references provided by
dynamic eye models to advance in a fast-paced group interaction task. Compared
to a non-reflective eye model and a pure reflection mode, their combination in
the new approach resulted in a higher identification accuracy and user
experience, suggesting a synergistic benefit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Experiencing the Future Mundane: Configuring Design Fiction as Breaching
  Experiment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Crabtree, Tom Lodge, Alan Chamberlain, Neelima Sailaja, Paul Coulton, Matthew Pilling, Ian Forrester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel methodological approach for surfacing the
acceptability and adoption challenges that confront future and emerging
technologies from the perspective of mundane action, in which they will
ultimately be embedded and used. This novel approach configures design fiction
as a breaching experiment to surface taken for granted background expectancies
that are fateful for acceptability and adoption. We explain the logic of this
new interdisciplinary method and present a concrete case to demonstrate its
viability: a design fiction called Experiencing the Future Mundane (EFM), which
depicts a future world in which watching TV is driven by smart adaptive media.
We explicate the design of the EFM, how it was configured to breach common
sense knowledge and surface taken for granted background expectancies
concerning how watching TV works and is expected to work, the acceptability and
adoption challenges that emerge from user engagement with the experience, and
how this novel approach may be adopted more broadly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spacewalker: Traversing Representation Spaces for Fast Interactive
  Exploration and Annotation of Unstructured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Heine, Fabian Hörst, Jana Fragemann, Gijs Luijten, Jan Egger, Fin Bahnsen, M. Saquib Sarfraz, Jens Kleesiek, Constantin Seibold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In industries such as healthcare, finance, and manufacturing, analysis of
unstructured textual data presents significant challenges for analysis and
decision making. Uncovering patterns within large-scale corpora and
understanding their semantic impact is critical, but depends on domain experts
or resource-intensive manual reviews. In response, we introduce Spacewalker in
this system demonstration paper, an interactive tool designed to analyze,
explore, and annotate data across multiple modalities. It allows users to
extract data representations, visualize them in low-dimensional spaces and
traverse large datasets either exploratory or by querying regions of interest.
We evaluated Spacewalker through extensive experiments and annotation studies,
assessing its efficacy in improving data integrity verification and annotation.
We show that Spacewalker reduces time and effort compared to traditional
methods. The code of this work is open-source and can be found at:
https://github.com/code-lukas/Spacewalker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A comparative study of sensory encoding models for human navigation in
  virtual reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tangyao Li, Qiyuan Zhan, Yitong Zhu, Bojing Hou, Yuyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In virtual reality applications, users often navigate through virtual
environments, but the issue of physiological responses, such as cybersickness,
fatigue, and cognitive workload, can disrupt or even halt these activities.
Despite its impact, the underlying mechanisms of how the sensory system encodes
information in VR remain unclear. In this study, we compare three sensory
encoding models, Bayesian Efficient Coding, Fitness Maximizing Coding, and the
Linear Nonlinear Poisson model, regarding their ability to simulate human
navigation behavior in VR. By incorporating the factor of physiological
responses into the models, we find that the Bayesian Efficient Coding model
generally outperforms the others. Furthermore, the Fitness Maximizing Code
framework provides more accurate estimates when the error penalty is small. Our
results suggest that the Bayesian Efficient Coding framework offers superior
predictions in most scenarios, providing a better understanding of human
navigation behavior in VR environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing Exploration and Cybersickness: Investigating Curiosity-Driven
  Behavior in Virtual Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tangyao Li, Yuyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During virtual navigation, users exhibit varied interaction and navigation
behaviors influenced by several factors. Existing theories and models have been
developed to explain and predict these diverse patterns. While users often
experience uncomfortable sensations, such as cybersickness, during virtual
reality (VR) use, they do not always make optimal decisions to mitigate these
effects. Although methods like reinforcement learning have been used to model
decision-making processes, they typically rely on random selection to simulate
actions, failing to capture the complexities of real navigation behavior. In
this study, we propose curiosity as a key factor driving irrational
decision-making, suggesting that users continuously balance exploration and
cybersickness according to the free energy principle during virtual navigation.
Our findings show that VR users generally adopt conservative strategies when
navigating, with most participants displaying negative curiosity across trials.
However, curiosity levels tend to rise when the virtual environment changes,
illustrating the dynamic interplay between exploration and discomfort. This
study provides a quantitative approach to decoding curiosity-driven behavior
during virtual navigation, offering insights into how users balance exploration
and the avoidance of cybersickness. Future research will further refine this
model by incorporating additional psychological and environmental factors to
improve the accuracy of navigation pattern predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GOMA: Proactive Embodied Cooperative Communication via Goal-Oriented
  Mental Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lance Ying, Kunal Jha, Shivam Aarya, Joshua B. Tenenbaum, Antonio Torralba, Tianmin Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verbal communication plays a crucial role in human cooperation, particularly
when the partners only have incomplete information about the task, environment,
and each other's mental state. In this paper, we propose a novel cooperative
communication framework, Goal-Oriented Mental Alignment (GOMA). GOMA formulates
verbal communication as a planning problem that minimizes the misalignment
between the parts of agents' mental states that are relevant to the goals. This
approach enables an embodied assistant to reason about when and how to
proactively initialize communication with humans verbally using natural
language to help achieve better cooperation. We evaluate our approach against
strong baselines in two challenging environments, Overcooked (a multiplayer
game) and VirtualHome (a household simulator). Our experimental results
demonstrate that large language models struggle with generating meaningful
communication that is grounded in the social and physical context. In contrast,
our approach can successfully generate concise verbal communication for the
embodied assistant to effectively boost the performance of the cooperation as
well as human users' perception of the assistant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Foundation Models for Wearable Movement Data in Mental Health
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15240v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15240v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Franklin Y. Ruan, Aiwei Zhang, Jenny Y. Oh, SouYoung Jin, Nicholas C. Jacobson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained foundation models and transformer architectures have driven the
success of large language models (LLMs) and other modern AI breakthroughs.
However, similar advancements in health data modeling remain limited due to the
need for innovative adaptations. Wearable movement data offers a valuable
avenue for exploration, as it's a core feature in nearly all commercial
smartwatches, well established in clinical and mental health research, and the
sequential nature of the data shares similarities to language. We introduce the
Pretrained Actigraphy Transformer (PAT), the first open source foundation model
designed for time-series wearable movement data. Leveraging transformer-based
architectures and novel techniques, such as patch embeddings, and pretraining
on data from 29,307 participants in a national U.S. sample, PAT achieves
state-of-the-art performance in several mental health prediction tasks. PAT is
also lightweight and easily interpretable, making it a robust tool for mental
health research.
  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Biologically-Inspired Technologies: Integrating Brain-Computer Interface
  and Neuromorphic Computing for Human Digital Twins 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Shang, Jiadong Yu, Dinh Thai Hoang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of immersive communication into a human-centric ecosystem has
intensified the demand for sophisticated Human Digital Twins (HDTs) driven by
multifaceted human data. However, the effective construction of HDTs faces
significant challenges due to the heterogeneity of data collection devices, the
high energy demands associated with processing intricate data, and concerns
over the privacy of sensitive information. This work introduces a novel
biologically-inspired (bio-inspired) HDT framework that leverages
Brain-Computer Interface (BCI) sensor technology to capture brain signals as
the data source for constructing HDT. By collecting and analyzing these
signals, the framework not only minimizes device heterogeneity and enhances
data collection efficiency, but also provides richer and more nuanced
physiological and psychological data for constructing personalized HDTs. To
this end, we further propose a bio-inspired neuromorphic computing learning
model based on the Spiking Neural Network (SNN). This model utilizes discrete
neural spikes to emulate the way of human brain processes information, thereby
enhancing the system's ability to process data effectively while reducing
energy consumption. Additionally, we integrate a Federated Learning (FL)
strategy within the model to strengthen data privacy. We then conduct a case
study to demonstrate the performance of our proposed twofold bio-inspired
scheme. Finally, we present several challenges and promising directions for
future research of HDTs driven by bio-inspired technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smartphone-based Eye Tracking System using Edge Intelligence and Model
  Optimisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishan Gunawardena, Gough Yumu Lui, Jeewani Anupama Ginige, Bahman Javadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A significant limitation of current smartphone-based eye-tracking algorithms
is their low accuracy when applied to video-type visual stimuli, as they are
typically trained on static images. Also, the increasing demand for real-time
interactive applications like games, VR, and AR on smartphones requires
overcoming the limitations posed by resource constraints such as limited
computational power, battery life, and network bandwidth. Therefore, we
developed two new smartphone eye-tracking techniques for video-type visuals by
combining Convolutional Neural Networks (CNN) with two different Recurrent
Neural Networks (RNN), namely Long Short Term Memory (LSTM) and Gated Recurrent
Unit (GRU). Our CNN+LSTM and CNN+GRU models achieved an average Root Mean
Square Error of 0.955 cm and 1.091 cm, respectively. To address the
computational constraints of smartphones, we developed an edge intelligence
architecture to enhance the performance of smartphone-based eye tracking. We
applied various optimisation methods like quantisation and pruning to deep
learning models for better energy, CPU, and memory usage on edge devices,
focusing on real-time processing. Using model quantisation, the model inference
time in the CNN+LSTM and CNN+GRU models was reduced by 21.72% and 19.50%,
respectively, on edge devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I have included the three papers as reference, which are closely
  related. We have expanded the future work section to provide a more thorough
  discussion of the concepts of "varying lighting conditions" and "dynamic user
  environments." We have added a note below Table 4 to clarify the
  abbreviations' meaning. Elaborated the role of the Domain Expert within the
  presentation layer in Section 4.1</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-13T00:00:00Z">2025-01-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smells-sus: Sustainability Smells in IaC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seif Ashraf, Mohammad Hamdaqa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practitioners use Infrastructure as Code (IaC) scripts to efficiently
configure IT infrastructures through machine-readable definition files.
However, during the development of these scripts, some code patterns or
deployment choices may lead to sustainability issues like inefficient resource
utilization or redundant provisioning for example. We call this type of
patterns sustainability smells. These inefficiencies pose significant
environmental and financial challenges, given the growing scale of cloud
computing. This research focuses on Terraform, a widely adopted IaC tool. Our
study involves defining seven sustainability smells and validating them through
a survey with 19 IaC practitioners. We utilized a dataset of 28,327 Terraform
scripts from 395 open-source repositories. We performed a detailed qualitative
analysis of a randomly sampled 1,860 Terraform scripts from the original
dataset to identify code patterns that correspond to the sustainability smells
and used the other 26,467 Terraform scripts to study the prevalence of the
defined sustainability smells. Our results indicate varying prevalence rates of
these smells across the dataset. The most prevalent smell is Monolithic
Infrastructure, which appears in 9.67\% of the scripts. Additionally, our
findings highlight the complexity of conducting root cause analysis for
sustainability issues, as these smells often arise from a confluence of script
structures, configuration choices, and deployment contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulink Mutation Testing using <span class="highlight-title">Code</span><span class="highlight-title">BERT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfan Zhang, Delaram Ghobari, Mehrdad Sabetzadeh, Shiva Nejati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present BERTiMuS, an approach that uses CodeBERT to generate mutants for
Simulink models. BERTiMuS converts Simulink models into textual
representations, masks tokens from the derived text, and uses CodeBERT to
predict the masked tokens. Simulink mutants are obtained by replacing the
masked tokens with predictions from CodeBERT. We evaluate BERTiMuS using
Simulink models from an industrial benchmark, and compare it with FIM -- a
state-of-the-art mutation tool for Simulink. We show that, relying exclusively
on CodeBERT, BERTiMuS can generate the block-based Simulink mutation patterns
documented in the literature. Further, our results indicate that: (a) BERTiMuS
is complementary to FIM, and (b) when one considers a requirements-aware notion
of mutation testing, BERTiMuS outperforms FIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at the 6th ACM/IEEE International
  Conference on Automation of Software Test (AST 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Agent-based Program Repair at Google 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pat Rondon, Renyao Wei, José Cambronero, Jürgen Cito, Aaron Sun, Siddhant Sanyam, Michele Tufano, Satish Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agent-based program repair offers to automatically resolve complex bugs
end-to-end by combining the planning, tool use, and code generation abilities
of modern LLMs. Recent work has explored the use of agent-based repair
approaches on the popular open-source SWE-Bench, a collection of bugs from
highly-rated GitHub Python projects. In addition, various agentic approaches
such as SWE-Agent have been proposed to solve bugs in this benchmark. This
paper explores the viability of using an agentic approach to address bugs in an
enterprise context. To investigate this, we curate an evaluation set of 178
bugs drawn from Google's issue tracking system. This dataset spans both
human-reported (78) and machine-reported bugs (100).
  To establish a repair performance baseline on this benchmark, we implement
Passerine, an agent similar in spirit to SWE-Agent that can work within
Google's development environment. We show that with 20 trajectory samples and
Gemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e.,
plausible) for 73% of machine-reported and 25.6% of human-reported bugs in our
evaluation set. After manual examination, we found that 43% of machine-reported
bugs and 17.9% of human-reported bugs have at least one patch that is
semantically equivalent to the ground-truth patch.
  These results establish a baseline on an industrially relevant benchmark,
which as we show, contains bugs drawn from a different distribution -- in terms
of language diversity, size, and spread of changes, etc. -- compared to those
in the popular SWE-Bench dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LitmusKt: Concurrency Stress Testing for Kotlin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Lochmelis, Evgenii Moiseenko, Yaroslav Golubev, Anton Podkopaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LitmusKt - the first tool for litmus testing concurrent programs
in Kotlin. The tool's novelty also lies in the fact that Kotlin is a
multiplatform language, i.e., it compiles into multiple platforms, which means
that the concurrency has to be tested on several of them. Our tool allows
writing litmus tests in a single custom DSL, and these tests are then run in
Kotlin/Native and Kotlin/JVM, two main platforms for concurrent programming in
Kotlin. Using LitmusKt, we discovered novel bugs in the Kotlin compiler, which
we then fixed and they are no longer present. Moreover, LitmusKt was integrated
into the CI pipeline for Kotlin. We believe that our tool is valuable for
further studying concurrency in Kotlin and other multiplatform languages, as
well as for further developing the Kotlin memory model.
  LitmusKt is openly available on GitHub:
https://github.com/Jetbrains-Research/litmuskt. The demonstration video is
available on YouTube: https://youtu.be/gXI0aYJDnRw.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests
  Through Precise Contextual Information Injection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Yin, Chao Ni, Xinrui Li, Liushan Chen, Guojun Ma, Xiaohu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though many learning-based approaches have been proposed for unit test
generation and achieved remarkable performance, they still have limitations in
relying on task-specific datasets. Recently, Large Language Models (LLMs)
guided by prompt engineering have gained attention for their ability to handle
a broad range of tasks, including unit test generation. Despite their success,
LLMs may exhibit hallucinations when generating unit tests for focal methods or
functions due to their lack of awareness regarding the project's global
context. These hallucinations may manifest as calls to non-existent methods, as
well as incorrect parameters or return values, such as mismatched parameter
types or numbers. While many studies have explored the role of context, they
often extract fixed patterns of context for different models and focal methods,
which may not be suitable for all generation processes (e.g., excessive
irrelevant context could lead to redundancy, preventing the model from focusing
on essential information). To overcome this limitation, we propose RATester,
which enhances the LLM's ability to generate more repository-aware unit tests
through global contextual information injection. To equip LLMs with global
knowledge similar to that of human testers, we integrate the language server
gopls, which provides essential features (e.g., definition lookup) to assist
the LLM. When RATester encounters an unfamiliar identifier (e.g., an unfamiliar
struct name), it first leverages gopls to fetch relevant definitions and
documentation comments, and then uses this global knowledge to guide the LLM.
By utilizing gopls, RATester enriches the LLM's knowledge of the project's
global context, thereby reducing hallucinations during unit test generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Affirmative Hackathon for Software Developers with Disabilities: An
  Industry Initiative <span class="chip">ASE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thayssa Rocha, Nicole Davila, Rafaella Vaccari, Nicoly Menezes, Marcelle Mota, Edward Monteiro, Cleidson de Souza, Gustavo Pinto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People with disabilities (PWD) often encounter several barriers to becoming
employed. A growing body of evidence in software development highlights the
benefits of diversity and inclusion in the field. However, recruiting, hiring,
and fostering a supportive environment for PWD remains challenging. These
challenges are exacerbated by the lack of skilled professionals with experience
in inclusive hiring and management, which prevents companies from effectively
increasing PWD representation on software development teams. Inspired by the
strategy adopted in some technology companies that attract talent through
hackathons and training courses, this paper reports the experience of Zup
Innovation, a Brazilian software company, in hosting a fully remote affirmative
hackathon with 50 participants to attract PWD developers. This event resulted
in 10 new hires and 146 people added to the company's talent pool. Through
surveys with participants, we gathered attendees' perceptions and experiences,
aiming to improve future hackathons and similar initiatives by providing
insights on accessibility and collaboration. Our findings offer lessons for
other companies seeking to address similar challenges and promote greater
inclusion in tech teams.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted for CHASE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating <span class="highlight-title">Pre-Train</span>ed Models for Multi-Language Vulnerability Patching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zanis Ali Khan, Aayush Garg, Yuejun Guo, Qiang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software vulnerabilities pose critical security risks, demanding prompt and
effective mitigation strategies. While advancements in Automated Program Repair
(APR) have primarily targeted general software bugs, the domain of
vulnerability patching, which is a security-critical subset of APR, remains
underexplored. This paper investigates the potential of pre-trained language
models, CodeBERT and CodeT5, for automated vulnerability patching across
diverse datasets and five programming languages. We evaluate these models on
their accuracy, computational efficiency, and how the length of vulnerable code
patches impacts performance. Our findings reveal promising accuracy levels,
particularly for CodeT5 on datasets with complex vulnerability patterns, while
CodeBERT demonstrates strengths in handling fragmented or context-limited
datasets. CodeT5 further showcases superior efficiency, making it well-suited
for large-scale applications. However, both models face challenges in
maintaining performance as patch length increases, highlighting the complexity
of addressing extended in program repair specifically aimed at fixing
vulnerabilities. This study benchmarks model performance, highlights key
limitations, and offers insights to improve automated vulnerability patching
for practical security applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Security-by-design: Securing a compromised system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Awais Rashid, Sana Belguith, Matthew Bradbury, Sadie Creese, Ivan Flechais, Neeraj Suri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital infrastructures are seeing convergence and connectivity at
unprecedented scale. This is true for both current critical national
infrastructures and emerging future systems that are highly cyber-physical in
nature with complex intersections between humans and technologies, e.g., smart
cities, intelligent transportation, high-value manufacturing and Industry 4.0.
Diverse legacy and non-legacy software systems underpinned by heterogeneous
hardware compose on-the-fly to deliver services to millions of users with
varying requirements and unpredictable actions. This complexity is compounded
by intricate and complicated supply-chains with many digital assets and
services outsourced to third parties. The reality is that, at any particular
point in time, there will be untrusted, partially-trusted or compromised
elements across the infrastructure. Given this reality, and the societal scale
of digital infrastructures, delivering secure and resilient operations is a
major challenge. We argue that this requires us to move beyond the paradigm of
security-by-design and embrace the challenge of securing-a-compromised-system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Article for the Rossfest Symposium in memory of Ross Anderson,
  Cambridge, UK, 25 March 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Containers as the Quantum Leap in Software Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iftikhar Ahmad, Teemu Autto, Teerath Das, Joonas Hämäläinen, Pasi Jalonen, Viljami Järvinen, Harri Kallio, Tomi Kankainen, Taija Kolehmainen, Pertti Kontio, Pyry Kotilainen, Matti Kurittu, Tommi Mikkonen, Rahul Mohanani, Niko Mäkitalo, Jari Partanen, Roope Pajasmaa, Jarkko Pellikka, Manu Setälä, Jari Siukonen, Anssi Sorvisto, Maha Sroor, Teppo Suominen, Salla Timonen, Muhammad Waseem, Yuriy Yevstihnyeyev, Verneri Äberg, Leif Åstrand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of the project QLEAP (2022-24), funded by Business Finland and
participating organizations, was to study using containers as elements of
architecture design. Such systems include containerized AI systems, using
containers in a hybrid setup (public/hybrid/private clouds), and related
security concerns. The consortium consists of four companies that represent
different concerns over using containers (Bittium, M-Files, Solita/ADE
Insights, Vaadin) and one research organization (University of Jyv\"askyl\"a).
In addition, it has received support from two Veturi companies - Nokia and
Tietoevry - who have also participated in steering the project. Moreover, the
SW4E ecosystem has participated in the project. This document gathers the key
lessons learned from the project.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Empirical Research Methods in Software Engineering: An
  Editorial Introduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Mendez, Paris Avgeriou, Marcos Kalinowski, Nauman bin Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical Software Engineering has received much attention in recent years
and became a de-facto standard for scientific practice in Software Engineering.
However, while extensive guidelines are nowadays available for designing,
conducting, reporting, and reviewing empirical studies, similar attention has
not yet been paid to teaching empirical software engineering. Closing this gap
is the scope of this edited book. In the following editorial introduction, we,
the editors, set the foundation by laying out the larger context of the
discipline for a positioning of the remainder of this book.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint to a chapter for the edited book "Handbook on Teaching
  Empirical Software Engineering", Springer, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling <span class="highlight-title">Code</span> Clone Patterns in Open Source VR Software: An Empirical
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huashan Chen, Zisheng Huang, Yifan Xu, Wenjie Huang, Jinfu Chen, Haotang Li, Kebin Peng, Feng Liu, Sen He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code cloning is frequently observed in software development, often leading to
a variety of maintenance and security issues. While substantial research has
been conducted on code cloning in traditional software, to the best of my
knowledge, there is a lack of studies on cloning in VR software that consider
its unique nature, particularly the presence of numerous serialized files in
conjunction with the source code. In this paper, we conduct the first
large-scale quantitative empirical analysis of software clones in 345
open-source VR projects, using the NiCad detector for source code clone
detection and large language models (LLMs) for identifying serialized file
clones. Our study leads to a number of insights into cloning phenomena in VR
software, guided by seven carefully formulated research questions. These
findings, along with their implications, are anticipated to provide useful
guidance for both researchers and software developers within the VR field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The importance of visual modelling languages in generative software
  engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17976v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17976v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Rossi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal GPTs represent a watershed in the interplay between Software
Engineering and Generative Artificial Intelligence. GPT-4 accepts image and
text inputs, rather than simply natural language. We investigate relevant use
cases stemming from these enhanced capabilities of GPT-4. To the best of our
knowledge, no other work has investigated similar use cases involving Software
Engineering tasks carried out via multimodal GPTs prompted with a mix of
diagrams and natural language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, working paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Galapagos: Automated N-Version Programming with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Ron, Diogo Gaspar, Javier Cabrera-Arteaga, Benoit Baudry, Martin Monperrus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  N-Version Programming is a well-known methodology for developing
fault-tolerant systems. It achieves fault detection and correction at runtime
by adding diverse redundancy into programs, minimizing fault mode overlap
between redundant program variants. In this work, we propose the automated
generation of program variants using large language models. We design, develop
and evaluate Gal\'apagos: a tool for generating program variants using LLMs,
validating their correctness and equivalence, and using them to assemble
N-Version binaries. We evaluate Gal\'apagos by creating N-Version components of
real-world C code. Our original results show that Gal\'apagos can produce
program variants that are proven to be functionally equivalent, even when the
variants are written in a different programming language. Our systematic
diversity measurement indicates that functionally equivalent variants produced
by Gal\'apagos, are statically different after compilation, and present
diverging internal behavior at runtime. We demonstrate that the variants
produced by Gal\'apagos can protect C code against real miscompilation bugs
which affect the Clang compiler. Overall, our paper shows that producing
N-Version software can be drastically automated by advanced usage of practical
formal verification and generative language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArkAnalyzer: The Static Analysis Framework for OpenHarmony 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Chen, Daihang Chen, Yizhuo Yang, Lingyun Xu, Liang Gao, Mingyi Zhou, Chunming Hu, Li Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ArkTS is a new programming language dedicated to developing apps for the
emerging OpenHarmony mobile operating system. Like other programming languages
constantly suffering from performance-related code smells or vulnerabilities,
the ArkTS programming language will likely encounter the same problems. The
solution given by our research community is to invent static analyzers, which
are often implemented on top of a common static analysis framework, to detect
and subsequently repair those issues automatically. Unfortunately, such an
essential framework is not available for the OpenHarmony community yet.
Existing program analysis methods have several problems when handling the ArkTS
code. To bridge the gap, we design and implement a framework named ArkAnalyzer
and make it publicly available as an open-source project. Our ArkAnalyzer
addresses the aforementioned problems and has already integrated a number of
fundamental static analysis functions that are ready to be reused by developers
to implement OpenHarmony
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing
  LLMs' Vulnerability Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16185v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16185v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqiang Sun, Daoyuan Wu, Yue Xue, Han Liu, Wei Ma, Lyuye Zhang, Yang Liu, Yingjiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant potential in
various tasks, including those requiring human-level intelligence, such as
vulnerability detection. However, recent efforts to use LLMs for vulnerability
detection remain preliminary, as they lack a deep understanding of whether a
subject LLM's vulnerability reasoning capability stems from the model itself or
from external aids such as knowledge retrieval and tooling support.
  In this paper, we aim to decouple LLMs' vulnerability reasoning from other
capabilities, such as vulnerability knowledge adoption, context information
retrieval, and advanced prompt schemes. We introduce LLM4Vuln, a unified
evaluation framework that separates and assesses LLMs' vulnerability reasoning
capabilities and examines improvements when combined with other enhancements.
  We conduct controlled experiments using 147 ground-truth vulnerabilities and
147 non-vulnerable cases in Solidity, Java and C/C++, testing them in a total
of 3,528 scenarios across four LLMs (GPT-3.5, GPT-4, Phi-3, and Llama 3). Our
findings reveal the varying impacts of knowledge enhancement, context
supplementation, and prompt schemes. We also identify 14 zero-day
vulnerabilities in four pilot bug bounty programs, resulting in $3,576 in
bounties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a technical report by Nanyang Technological University.
  Updated to support Solidity, Java and C/C++</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward a Better Understanding of Probabilistic Delta Debugging <span class="chip">ICSE25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengxiao Zhang, Zhenyang Xu, Yongqiang Tian, Xinru Cheng, Chengnian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a list L of elements and a property that L exhibits, ddmin is a
well-known test input minimization algorithm designed to automatically
eliminate irrelevant elements from L. This algorithm is extensively adopted in
test input minimization and software debloating. Recently, ProbDD, an advanced
variant of ddmin, has been proposed and achieved state-of-the-art performance.
Employing Bayesian optimization, ProbDD predicts the likelihood of each element
in L being essential, and statistically decides which elements and how many
should be removed each time. Despite its impressive results, the theoretical
probabilistic model of ProbDD is complex, and the specific factors driving its
superior performance have not been investigated. In this paper, we conduct the
first in-depth theoretical analysis of ProbDD, clarifying trends in probability
and subset size changes while simplifying the probability model. Complementing
this analysis, we perform empirical experiments, including success rate
analysis, ablation studies, and analysis on trade-offs and limitations, to
better understand and demystify this state-of-the-art algorithm. Our success
rate analysis shows how ProbDD addresses bottlenecks of ddmin by skipping
inefficient queries that attempt to delete complements of subsets and
previously tried subsets. The ablation study reveals that randomness in ProbDD
has no significant impact on efficiency. Based on these findings, we propose
CDD, a simplified version of ProbDD, reducing complexity in both theory and
implementation. Besides, the performance of CDD validates our key findings.
Comprehensive evaluations across 76 benchmarks in test input minimization and
software debloating show that CDD can achieve the same performance as ProbDD
despite its simplification. These insights provide valuable guidance for future
research and applications of test input minimization algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICSE25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Automated <span class="highlight-title">Code</span> Transformation and Pragma
  Insertion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03058v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03058v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stéphane Pouget, Louis-Noël Pouchet, Jason Cong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-level synthesis, source-to-source compilers, and various Design Space
Exploration techniques for pragma insertion have significantly improved the
Quality of Results of generated designs. These tools offer benefits such as
reduced development time and enhanced performance. However, achieving
high-quality results often requires additional manual code transformations and
tiling selections, which are typically performed separately or as
pre-processing steps. Although DSE techniques enable code transformation
upfront, the vastness of the search space often limits the exploration of all
possible code transformations, making it challenging to determine which
transformations are necessary. Additionally, ensuring correctness remains
challenging, especially for complex transformations and optimizations.
  To tackle this obstacle, we first propose a comprehensive framework
leveraging HLS compilers. Our system streamlines code transformation, pragma
insertion, and tiles size selection for on-chip data caching through a unified
optimization problem, aiming to enhance parallelization, particularly
beneficial for computation-bound kernels. Them employing a novel Non-Linear
Programming (NLP) approach, we simultaneously ascertain transformations,
pragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation
demonstrates that our framework adeptly identifies the appropriate
transformations, including scenarios where no transformation is necessary, and
inserts pragmas to achieve a favorable Quality of Results.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">23</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable Vertical Ground Reaction Force Estimation with Smart Insole
  During Walking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Femi Olugbon, Nozhan Ghoreishi, Ming-Chun Huang, Wenyao Xu, Diliang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vertical ground reaction force (vGRF) and its characteristic weight
acceptance and push-off peaks measured during walking are important for gait
and biomechanical analysis. Current wearable vGRF estimation methods suffer
from drifting errors or low generalization performances, limiting their
practical application. This paper proposes a novel method for reliably
estimating vGRF and its characteristic peaks using data collected from the
smart insole, including inertial measurement unit data and the newly introduced
center of the pressed sensor data. These data were fused with machine learning
algorithms including artificial neural networks, random forest regression, and
bi-directional long-short-term memory. The proposed method outperformed the
state-of-the-art methods with the root mean squared error, normalized root mean
squared error, and correlation coefficient of 0.024 body weight (BW), 1.79% BW,
and 0.997 in intra-participant testing, and 0.044 BW, 3.22% BW, and 0.991 in
inter-participant testing, respectively. The difference between the reference
and estimated weight acceptance and push-off peak values are 0.022 BW and 0.017
BW with a delay of 1.4% and 1.8% of the gait cycle for the intra-participant
testing and 0.044 BW and 0.025 BW with a delay of 1.5% and 2.3% of the gait
cycle for the inter-participant testing. The results indicate that the proposed
vGRF estimation method has the potential to achieve accurate vGRF measurement
during walking in free living environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Practice, Perception, and Challenge of Blind or Low
  Vision Students Learning through Accessible Technologies in Non-Inclusive
  'Blind Colleges' 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuqi Tommy Zhu, Ziyue Qiu, Ye Wei, Jianhao Wang, Yang Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In developing and underdeveloped regions, many 'Blind Colleges' exclusively
enroll individuals with Blindness or Vision Impairment (BLV) for higher
education. While advancements in accessible technologies have facilitated BLV
student integration into 'Integrated Colleges,' their implementation in 'Blind
Colleges' remains uneven due to complex economic, social, and policy
challenges. This study investigates the practices, perceptions, and challenges
of BLV students using accessible technologies in a Chinese 'Blind College'
through a two-part empirical approach. Our findings demonstrate that tactile
and digital technologies enhance access to education but face significant
integration barriers. We emphasize the critical role of early education in
addressing capability gaps, BLV students' aspirations for more inclusive
educational environments, and the systemic obstacles within existing
frameworks. We advocate for leveraging accessible technologies to transition
'Blind Colleges' into 'Integrated Colleges,' offering actionable insights for
policymakers, designers, and educators. Finally, we outline future research
directions on accessible technology innovation and its implications for BLV
education in resource-constrained settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing Human-Hand Segmentation on In-Distribution and
  Out-of-Distribution Data in Human-Robot Interactions Using a Deep Ensemble
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Jalayer, Yuxin Chen, Masoud Jalayer, Carlotta Orsenigo, Masayoshi Tomizuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable detection and segmentation of human hands are critical for enhancing
safety and facilitating advanced interactions in human-robot collaboration.
Current research predominantly evaluates hand segmentation under
in-distribution (ID) data, which reflects the training data of deep learning
(DL) models. However, this approach fails to address out-of-distribution (OOD)
scenarios that often arise in real-world human-robot interactions. In this
study, we present a novel approach by evaluating the performance of pre-trained
DL models under both ID data and more challenging OOD scenarios. To mimic
realistic industrial scenarios, we designed a diverse dataset featuring simple
and cluttered backgrounds with industrial tools, varying numbers of hands (0 to
4), and hands with and without gloves. For OOD scenarios, we incorporated
unique and rare conditions such as finger-crossing gestures and motion blur
from fast-moving hands, addressing both epistemic and aleatoric uncertainties.
To ensure multiple point of views (PoVs), we utilized both egocentric cameras,
mounted on the operator's head, and static cameras to capture RGB images of
human-robot interactions. This approach allowed us to account for multiple
camera perspectives while also evaluating the performance of models trained on
existing egocentric datasets as well as static-camera datasets. For
segmentation, we used a deep ensemble model composed of UNet and RefineNet as
base learners. Performance evaluation was conducted using segmentation metrics
and uncertainty quantification via predictive entropy. Results revealed that
models trained on industrial datasets outperformed those trained on
non-industrial datasets, highlighting the importance of context-specific
training. Although all models struggled with OOD scenarios, those trained on
industrial datasets demonstrated significantly better generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Investigation of Experiences Engaging the Margins in Data-Centric
  Innovation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriella Thompson, Ebtesam Al Haque, Paulette Blanc, Meme Styles, Denae Ford, Angela D. R. Smith, Brittany Johnson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-centric technologies provide exciting opportunities, but recent research
has shown how lack of representation in datasets, often as a result of systemic
inequities and socioeconomic disparities, can produce inequitable outcomes that
can exclude or harm certain demographics. In this paper, we discuss preliminary
insights from an ongoing effort aimed at better understanding barriers to
equitable data-centric innovation. We report findings from a survey of 261
technologists and researchers who use data in their work regarding their
experiences seeking adequate, representative datasets. Our findings suggest
that age and identity play a significant role in the seeking and selection of
representative datasets, warranting further investigation into these aspects of
data-centric research and development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Near Data" and "Far Data" for Urban Sustainability: How Do Community
  Advocates Envision Data Intermediaries? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Qiao, Siyi Wu, Christoph Becker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the densifying data ecosystem of today's cities, data intermediaries are
crucial stakeholders in facilitating data access and use. Community advocates
live in these sites of social injustices and opportunities for change. Highly
experienced in working with data to enact change, they offer distinctive
insights on data practices and tools. This paper examines the unique
perspectives that community advocates offer on data intermediaries. Based on
interviews with 17 advocates working with 23 grassroots and nonprofit
organizations, we propose the quality of "near" and "far" to be seriously
considered in data intermediaries' works and articulate advocates' vision of
connecting "near data" and "far data." To pursue this vision, we identified
three pathways for data intermediaries: align data exploration with ways of
storytelling, communicate context and uncertainties, and decenter artifacts for
relationship building. These pathways help data intermediaries to put data
feminism into practice, surface design opportunities and tensions, and raise
key questions for supporting the pursuit of the Right to the City.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ML Mule: Mobile-Driven Context-Aware Collaborative Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxiang Yu, Javier Berrocal, Christine Julien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has been integrated into nearly every aspect of daily
life, powering applications from object detection with computer vision to large
language models for writing emails and compact models in smart homes. These
machine learning models cater to individual users but are often detached from
them, as they are typically stored and processed in centralized data centers.
This centralized approach raises privacy concerns, incurs high infrastructure
costs, and struggles with personalization. Federated and fully decentralized
learning methods have been proposed to address these issues, but they still
depend on centralized servers or face slow convergence due to communication
constraints. To overcome these challenges, we propose ML Mule, a approach that
utilizes individual mobile devices as 'Mules' to train and transport model
snapshots as they move through physical spaces, sharing these models with the
physical 'Spaces' they inhabit. This method implicitly forms affinity groups
among devices associated with users who share particular spaces, enabling
collaborative model evolution, and protecting users' privacy. Our approach
addresses several major shortcomings of traditional, federated, and fully
decentralized learning systems. The proposed framework represents a new class
of machine learning methods that are more robust, distributed, and
personalized, bringing the field closer to realizing the original vision of
intelligent, adaptive, and genuinely context-aware smart environments. The
results show that ML Mule converges faster and achieves higher model accuracy
compared to other existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the distribution of connectivity weights in resting-state EEG
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Gong, Xiaolong Huang, Jie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The resting-state brain networks (RSNs) reflects the functional connectivity
patterns between brain modules, providing essential foundations for decoding
intrinsic neural information within the brain. It serves as one of the primary
tools for describing the spatial dynamics of the brain using various
neuroimaging techniques, such as electroencephalography (EEG) and
magnetoencephalography (MEG). However, the distribution rules or potential
modes of functional connectivity weights in the resting state remain unclear.
In this context, we first start from simulation, using forward solving model to
generate scalp EEG with four channel densities (19, 32, 64, 128). Subsequently,
we construct scalp brain networks using five coupling measures, aiming to
explore whether different channel density or coupling measures affect the
distribution pattern of functional connectivity weights. Next, we quantify the
distribution pattern by calculating the skewness, kurtosis, and Shannon entropy
of the functional connectivity network weights. Finally, the results of the
simulation were validated in a normative database. We observed that: 1) The
functional connection weights exhibit a right-skewed distribution, and are not
influenced by channel density or coupling measures; 2) The functional
connection weights exhibit a relatively uniform distribution, with the
potential for volume conduction to affect the degree of uniformity in the
distribution; 3) Networks constructed using coupling measures influenced by
volume conduction exhibit significant correlations between the average
connection weight and measures of skewness, kurtosis, and Shannon entropy. This
study contributes to a deeper understanding of RSNs, providing valuable
insights for research in the field of neuroscience, and holds promise for being
associated with brain cognition and disease diagnosis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChartEditor: A Human-AI Paired Tool for Authoring Pictorial Charts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Yan, Tiancheng Liu, Weikai Yang, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pictorial charts are favored for their memorability and visual appeal,
offering a more engaging alternative to basic charts. However, their creation
can be complex and time-consuming due to the lack of native support in popular
visualization tools like Tableau. While AI-generated content (AIGC) tools have
lowered the barrier to creating pictorial charts, they often lack precise
design control. To address this issue, we introduce ChartEditor, a human-AI
paired tool that transforms basic charts into pictorial versions based on user
intent. ChartEditor decomposes chart images into visual components and
organizes them within a hierarchical tree. Based on this tree, users can
express their intent in natural language, which is then translated into
modifications to the hierarchy. In addition, users can directly interact with
and modify specific chart components via an intuitive interface to achieve
fine-grained design control. A user study demonstrates the effectiveness and
usability of ChartEditor in simplifying the creation of pictorial charts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Interaction with Augmented Reality through Mid-Air Haptic
  Feedback: Architecture Design and User Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Vaquero-Melchor, Ana M. Bernardos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of haptics within Augmented Reality may help to deliver an
enriched experience, while facilitating the performance of specific actions
(e.g. repositioning or resizin ) that are still dependent on the user's skills.
This paper gathers the description of a flexible architecture designed to
deploy haptically-enabled AR applications. The haptic feedback may be generated
through a variety of devices (e.g., wearable, graspable, or mid-air ones), and
the architecture facilitates handling the specificity of each. For this reason,
it is discussed how to generate a haptic representation of a 3D digital object
depending on the application and the target device. Additionally, it is
included an analysis of practical, relevant issues that arise when setting up a
system to work with specific devices like Head-Mounted Displays (e.g.,
HoloLens) and mid-air haptic devices (e.g., Ultrahaptics UHK), such as the
alignment between the real world and the virtual one. The architecture
applicability is demonstrated through the implementation of two applications:
Form Inspector and Simon Game, built for HoloLens and iOS mobile phones for
visualization and for UHK for mid-air haptics delivery. These applications have
been used by nine users to explore the efficiency, meaningfulness, and
usefulness of mid-air haptics for form perception, object resizing, and push
interaction tasks. Results show that, although mobile interaction is preferred
when this option is available, haptics turn out to be more meaningful in
identifying shapes when compared to what users initially expect and in
contributing to the execution of resizing tasks. Moreover, this preliminary
user study reveals that users may be expecting a tailored interface metaphor,
not necessarily inspired in natural interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-face emotion detection for effective Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Ala Yahyaoui, Mouaad Oujabour, Leila Ben Letaifa, Amine Bohi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of dialogue interfaces in mobile devices has become
ubiquitous, providing a wide array of services. As technology progresses,
humanoid robots designed with human-like features to interact effectively with
people are gaining prominence, and the use of advanced human-robot dialogue
interfaces is continually expanding. In this context, emotion recognition plays
a crucial role in enhancing human-robot interaction by enabling robots to
understand human intentions. This research proposes a facial emotion detection
interface integrated into a mobile humanoid robot, capable of displaying
real-time emotions from multiple individuals on a user interface. To this end,
various deep neural network models for facial expression recognition were
developed and evaluated under consistent computer-based conditions, yielding
promising results. Afterwards, a trade-off between accuracy and memory
footprint was carefully considered to effectively implement this application on
a mobile humanoid robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures and 1 table. Accepted at the 17th International
  Conference on Agents and Artificial Intelligence (ICAART 2025), Porto,
  Portugal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Security-by-design: Securing a compromised system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Awais Rashid, Sana Belguith, Matthew Bradbury, Sadie Creese, Ivan Flechais, Neeraj Suri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital infrastructures are seeing convergence and connectivity at
unprecedented scale. This is true for both current critical national
infrastructures and emerging future systems that are highly cyber-physical in
nature with complex intersections between humans and technologies, e.g., smart
cities, intelligent transportation, high-value manufacturing and Industry 4.0.
Diverse legacy and non-legacy software systems underpinned by heterogeneous
hardware compose on-the-fly to deliver services to millions of users with
varying requirements and unpredictable actions. This complexity is compounded
by intricate and complicated supply-chains with many digital assets and
services outsourced to third parties. The reality is that, at any particular
point in time, there will be untrusted, partially-trusted or compromised
elements across the infrastructure. Given this reality, and the societal scale
of digital infrastructures, delivering secure and resilient operations is a
major challenge. We argue that this requires us to move beyond the paradigm of
security-by-design and embrace the challenge of securing-a-compromised-system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Article for the Rossfest Symposium in memory of Ross Anderson,
  Cambridge, UK, 25 March 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crowdsourced human-based computational approach for tagging peripheral
  blood smear sample images from Sickle Cell Disease patients using non-expert
  users 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José María Buades Rubio, Gabriel Moyà-Alcover, Antoni Jaume-i-Capó, Nataša Petrović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a human-based computation approach for the analysis
of peripheral blood smear (PBS) images images in patients with Sickle Cell
Disease (SCD). We used the Mechanical Turk microtask market to crowdsource the
labeling of PBS images. We then use the expert-tagged erythrocytesIDB dataset
to assess the accuracy and reliability of our proposal. Our results showed that
when a robust consensus is achieved among the Mechanical Turk workers,
probability of error is very low, based on comparison with expert analysis.
This suggests that our proposed approach can be used to annotate datasets of
PBS images, which can then be used to train automated methods for the diagnosis
of SCD. In future work, we plan to explore the potential integration of our
findings with outcomes obtained through automated methodologies. This could
lead to the development of more accurate and reliable methods for the diagnosis
of SCD
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Voices: A Co-Hashtag Analysis of TikTok Discourse on the 2023
  Israel-Palestine Crisis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rozin Hasin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  TikTok has gradually become one of the most pervasive social media platforms
in our daily lives. In this research article, I explore how users on TikTok
discussed the crisis in Palestine that worsened in 2023. Using network
analysis, I situate keywords representing the conflict and categorize them
thematically based on a coding schema derived from politically and
ideologically differentiable stances. I conclude that that activism and
propaganda are contending amongst themselves in the thriving space afforded by
TikTok today.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Robotic Approach Techniques for the Insertion of a Straight
  Instrument into a Vitreoretinal Surgery Trocar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ross Henry, Martin Huber, Anestis Mablekos-Alexiou, Carlo Seneci, Mohamed Abdelaziz, Hans Natalius, Lyndon da Cruz, Christos Bergeles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in vitreoretinal robotic surgery enable precise techniques for gene
therapies. This study evaluates three robotic approaches using the 7-DoF
robotic arm for docking a micro-precise tool to a trocar: fully co-manipulated,
hybrid co-manipulated/teleoperated, and hybrid with camera assistance. The
fully co-manipulated approach was the fastest but had a 42% success rate.
Hybrid methods showed higher success rates (91.6% and 100%) and completed tasks
within 2 minutes. NASA Task Load Index (TLX) assessments indicated lower
physical demand and effort for hybrid approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 Pages, 2 Figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROSAnnotator: A Web Application for ROSBag Data Analysis in Human-Robot
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Zhang, Haoqi Li, Ramtin Tabatabaei, Wafa Johal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-robot interaction (HRI) is an interdisciplinary field that utilises
both quantitative and qualitative methods. While ROSBags, a file format within
the Robot Operating System (ROS), offer an efficient means of collecting
temporally synched multimodal data in empirical studies with real robots, there
is a lack of tools specifically designed to integrate qualitative coding and
analysis functions with ROSBags. To address this gap, we developed
ROSAnnotator, a web-based application that incorporates a multimodal Large
Language Model (LLM) to support both manual and automated annotation of ROSBag
data. ROSAnnotator currently facilitates video, audio, and transcription
annotations and provides an open interface for custom ROS messages and tools.
By using ROSAnnotator, researchers can streamline the qualitative analysis
process, create a more cohesive analysis pipeline, and quickly access
statistical summaries of annotations, thereby enhancing the overall efficiency
of HRI data analysis. https://github.com/CHRI-Lab/ROSAnnotator
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to HRI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Enrichment Work and AI Labor in Latin America and the Caribbean 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianna Williams, Maya De Los Santos, Alexandra To, Saiph Savage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global AI surge demands crowdworkers from diverse languages and cultures.
They are pivotal in labeling data for enabling global AI systems. Despite
global significance, research has primarily focused on understanding the
perspectives and experiences of US and India crowdworkers, leaving a notable
gap. To bridge this, we conducted a survey with 100 crowdworkers across 16
Latin American and Caribbean countries. We discovered that these workers
exhibited pride and respect for their digital labor, with strong support and
admiration from their families. Notably, crowd work was also seen as a stepping
stone to financial and professional independence. Surprisingly, despite wanting
more connection, these workers also felt isolated from peers and doubtful of
others' labor quality. They resisted collaboration and gender-based tools,
valuing gender-neutrality. Our work advances HCI understanding of Latin
American and Caribbean crowdwork, offering insights for digital resistance
tools for the region.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages of content with 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scideator: Human-LLM Scientific Idea Generation Grounded in
  Research-Paper Facet Recombination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14634v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14634v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S. Weld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scientific ideation process often involves blending salient aspects of
existing papers to create new ideas. To see if large language models (LLMs) can
assist this process, we contribute Scideator, a novel mixed-initiative tool for
scientific ideation. Starting from a user-provided set of papers, Scideator
extracts key facets (purposes, mechanisms, and evaluations) from these and
relevant papers, allowing users to explore the idea space by interactively
recombining facets to synthesize inventive ideas. Scideator also helps users to
gauge idea novelty by searching the literature for potential overlaps and
showing automated novelty assessments and explanations. To support these tasks,
Scideator introduces four LLM-powered retrieval-augmented generation (RAG)
modules: Analogous Paper Facet Finder, Faceted Idea Generator, Idea Novelty
Checker, and Idea Novelty Iterator. In a within-subjects user study, 19
computer-science researchers identified significantly more interesting ideas
using Scideator compared to a strong baseline combining a scientific search
engine with LLM interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added supplementary material</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cocoa: Co-Planning and Co-Execution with AI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K. J. Kevin Feng, Kevin Pu, Matt Latzke, Tal August, Pao Siangliulue, Jonathan Bragg, Daniel S. Weld, Amy X. Zhang, Joseph Chee Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Cocoa, a system that implements a novel interaction design pattern
-- interactive plans -- for users to collaborate with an AI agent on complex,
multi-step tasks in a document editor. Cocoa harmonizes human and AI efforts
and enables flexible delegation of agency through two actions: Co-planning
(where users collaboratively compose a plan of action with the agent) and
Co-execution (where users collaboratively execute plan steps with the agent).
Using scientific research as a sample domain, we motivate the design of Cocoa
through a formative study with 9 researchers while also drawing inspiration
from the design of computational notebooks. We evaluate Cocoa through a user
study with 16 researchers and find that when compared to a strong chat
baseline, Cocoa improved agent steerability without sacrificing ease of use. A
deeper investigation of the general utility of both systems uncovered insights
into usage contexts where interactive plans may be more appropriate than chat,
and vice versa. Our work surfaces numerous practical implications and paves new
paths for interactive interfaces that foster more effective collaboration
between humans and agentic AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Telegram as a Battlefield: Kremlin-related Communications during the
  Russia-Ukraine Conflict 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01884v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01884v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apaar Bawa, Ugur Kursuncu, Dilshod Achilov, Valerie L. Shalin, Nitin Agarwal, Esra Akbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Telegram emerged as a crucial platform for both parties during the conflict
between Russia and Ukraine. Per its minimal policies for content moderation,
Pro-Kremlin narratives and potential misinformation were spread on Telegram,
while anti-Kremlin narratives with related content were also propagated, such
as war footage, troop movements, maps of bomb shelters, and air raid warnings.
This paper presents a dataset of posts from both pro-Kremlin and anti-Kremlin
Telegram channels, collected over a period spanning a year before and a year
after the Russian invasion. The dataset comprises 404 pro-Kremlin channels with
4,109,645 posts and 114 anti-Kremlin channels with 1,117,768 posts. We provide
details on the data collection process, processing methods, and dataset
characterization. Lastly, we discuss the potential research opportunities this
dataset may enable researchers across various disciplines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08926v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08926v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Virmarie Maquiling, Sean Anthony Byrne, Diederick C. Niehorster, Marco Carminati, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the transformative potential of SAM 2, a vision foundation model,
in advancing gaze estimation and eye tracking technologies. By significantly
reducing annotation time, lowering technical barriers through its ease of
deployment, and enhancing segmentation accuracy, SAM 2 addresses critical
challenges faced by researchers and practitioners. Utilizing its zero-shot
segmentation capabilities with minimal user input-a single click per video-we
tested SAM 2 on over 14 million eye images from diverse datasets, including
virtual reality setups and the world's largest unified dataset recorded using
wearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches
the performance of domain-specific models trained solely on eye images,
achieving competitive mean Intersection over Union (mIoU) scores of up to 93%
without fine-tuning. Additionally, we provide our code and segmentation masks
for these widely used datasets to promote further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Virmarie Maquiling and Sean Anthony Byrne contributed equally to this
  paper, 8 pages, 3 figures, ETRA 2025, pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Walk along: An Experiment on Controlling the Mobile Robot 'Spot' with
  Voice and Gestures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11218v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11218v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renchi Zhang, Jesse van der Linden, Dimitra Dodou, Harleigh Seyffert, Yke Bauke Eisma, Joost C. F. de Winter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots are becoming more capable and can autonomously perform tasks such as
navigating between locations. However, human oversight remains crucial. This
study compared two touchless methods for directing mobile robots: voice control
and gesture control, to investigate the efficiency of the methods and the
preference of users. We tested these methods in two conditions: one in which
participants remained stationary and one in which they walked freely alongside
the robot. We hypothesized that walking alongside the robot would result in
higher intuitiveness ratings and improved task performance, based on the idea
that walking promotes spatial alignment and reduces the effort required for
mental rotation. In a 2x2 within-subject design, 218 participants guided the
quadruped robot Spot along a circuitous route with multiple 90-degree turns
using rotate left, rotate right, and walk forward commands. After each trial,
participants rated the intuitiveness of the command mapping, while
post-experiment interviews were used to gather the participants' preferences.
Results showed that voice control combined with walking with Spot was the most
favored and intuitive, whereas gesture control while standing caused confusion
for left/right commands. Nevertheless, 29% of participants preferred gesture
control, citing increased task engagement and visual congruence as reasons. An
odometry-based analysis revealed that participants often followed behind Spot,
particularly in the gesture control condition, when they were allowed to walk.
In conclusion, voice control with walking produced the best outcomes. Improving
physical ergonomics and adjusting gesture types could make gesture control more
effective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robot Error Awareness Through Human Reactions: Implementation,
  Evaluation, and Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maia Stiber, Russell Taylor, Chien-Ming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective error detection is crucial to prevent task disruption and maintain
user trust. Traditional methods often rely on task-specific models or user
reporting, which can be inflexible or slow. Recent research suggests social
signals, naturally exhibited by users in response to robot errors, can enable
more flexible, timely error detection. However, most studies rely on post hoc
analysis, leaving their real-time effectiveness uncertain and lacking
user-centric evaluation. In this work, we developed a proactive error detection
system that combines user behavioral signals (facial action units and speech),
user feedback, and error context for automatic error detection. In a study (N =
28), we compared our proactive system to a status quo reactive approach.
Results show our system 1) reliably and flexibly detects error, 2) detects
errors faster than the reactive approach, and 3) is perceived more favorably by
users than the reactive one. We discuss recommendations for enabling robot
error awareness in future HRI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Adaptive Benchmark for Modeling User Exploration of Large Datasets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.15748v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.15748v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joanna Purich, Anthony Wise, Leilani Battle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a new DBMS performance benchmark that can simulate
user exploration with any specified dashboard design made of standard
visualization and interaction components. The distinguishing feature of our
SImulation-BAsed (or SIMBA) benchmark is its ability to model user analysis
goals as a set of SQL queries to be generated through a valid sequence of user
interactions, as well as measure the completion of analysis goals by testing
for equivalence between the user's previous queries and their goal queries. In
this way, the SIMBA benchmark can simulate how an analyst opportunistically
searches for interesting insights at the beginning of an exploration session
and eventually hones in on specific goals towards the end. To demonstrate the
versatility of the SIMBA benchmark, we use it to test the performance of four
DBMSs with six different dashboard specifications and compare our results with
IDEBench. Our results show how goal-driven simulation can reveal gaps in DBMS
performance missed by existing benchmarking methods and across a range of data
exploration scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-12T00:00:00Z">2025-01-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TensorConvolutionPlus: A python package for distribution system
  flexibility area estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Demetris Chrysostomou, Jose Luis Rueda Torres, Jochen Lorenz Cremer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Power system operators need new, efficient operational tools to use the
flexibility of distributed resources and deal with the challenges of highly
uncertain and variable power systems. Transmission system operators can
consider the available flexibility in distribution systems (DSs) without
breaching the DS constraints through flexibility areas. However, there is an
absence of open-source packages for flexibility area estimation. This paper
introduces TensorConvolutionPlus, a user-friendly Python-based package for
flexibility area estimation. The main features of TensorConvolutionPlus include
estimating flexibility areas using the TensorConvolution+ algorithm, the power
flow-based algorithm, an exhaustive PF-based algorithm, and an optimal power
flow-based algorithm. Additional features include adapting flexibility area
estimations from different operating conditions and including flexibility
service providers offering discrete setpoints of flexibility. The
TensorConvolutionPlus package facilitates a broader adaptation of flexibility
estimation algorithms by system operators and power system researchers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How is Google using AI for internal <span class="highlight-title">code</span> migrations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stoyan Nikolov, Daniele Codecasa, Anna Sjovall, Maxim Tabachnyk, Satish Chandra, Siddharth Taneja, Celal Ziftci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a tremendous interest in using generative AI,
and particularly large language models (LLMs) in software engineering; indeed
there are now several commercially available tools, and many large companies
also have created proprietary ML-based tools for their own software engineers.
While the use of ML for common tasks such as code completion is available in
commodity tools, there is a growing interest in application of LLMs for more
bespoke purposes. One such purpose is code migration.
  This article is an experience report on using LLMs for code migrations at
Google. It is not a research study, in the sense that we do not carry out
comparisons against other approaches or evaluate research questions/hypotheses.
Rather, we share our experiences in applying LLM-based code migration in an
enterprise context across a range of migration cases, in the hope that other
industry practitioners will find our insights useful. Many of these learnings
apply to any application of ML in software engineering. We see evidence that
the use of LLMs can reduce the time needed for migrations significantly, and
can reduce barriers to get started and complete migration programs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing the Evolution and Maintenance of Quantum Computing
  Repositories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishna Upadhyay, Vinaik Chhetri, A. B. Siddique, Umar Farooq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing is an emerging field with significant potential, yet
software development and maintenance challenges limit its accessibility and
maturity. This work investigates the current state, evolution, and maintenance
practices in the quantum computing community by conducting a large-scale mining
analysis of over 21,000 quantum software repositories on GitHub, containing
more than 1.2 million commits contributed by over 10,000 unique developers.
Specifically, the focus of this paper is to: (i) assess the community's status
and growth by examining the popularity of quantum computing, trends in
programming languages and framework usage, growth of contributors, and insights
from repository documentation; and (ii) analyze maintenance practices through
commit patterns, issue classification, and maintenance levels. Our findings
indicate rapid growth in the quantum computing community, with a 200% increase
in the number of repositories and a 150% rise in contributors since 2017. Our
analysis of commits shows a strong focus on perfective updates, while the
relatively low number of corrective commits highlights potential gaps in bug
resolution. Furthermore, one-third of the quantum computing issues highlight
the need for specialized tools in addition to general software infrastructure.
In summary, this work provides a foundation for targeted improvements in
quantum software to support sustained growth and technical advancement. Based
on our analysis of development activity, community structure, and maintenance
practices, this study offers actionable recommendations to enhance quantum
programming tools, documentation, and resources. We are also open-sourcing our
dataset to support further analysis by the community and to guide future
research and tool development for quantum computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures, 6 tables,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An efficient approach to represent enterprise web application structure
  using <span class="highlight-title">Large Language Model</span> in the service of Intelligent Quality Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaber Al Hassan Ayon, Gulam Husain, Roshankumar Bisoi, Waliur Rahman, Dr Tom Osborn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to represent enterprise web application
structures using Large Language Models (LLMs) to enable intelligent quality
engineering at scale. We introduce a hierarchical representation methodology
that optimizes the few-shot learning capabilities of LLMs while preserving the
complex relationships and interactions within web applications. The approach
encompasses five key phases: comprehensive DOM analysis, multi-page synthesis,
test suite generation, execution, and result analysis. Our methodology
addresses existing challenges around usage of Generative AI techniques in
automated software testing by developing a structured format that enables LLMs
to understand web application architecture through in-context learning. We
evaluated our approach using two distinct web applications: an e-commerce
platform (Swag Labs) and a healthcare application (MediBox) which is deployed
within Atalgo engineering environment. The results demonstrate success rates of
90\% and 70\%, respectively, in achieving automated testing, with high
relevance scores for test cases across multiple evaluation criteria. The
findings suggest that our representation approach significantly enhances LLMs'
ability to generate contextually relevant test cases and provide better quality
assurance overall, while reducing the time and effort required for testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 figure and 4 tables, relevant for Gen AI and enterprise
  AI use cases</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Low Can We Go? Minimizing Interaction Samples for Configurable
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Krupke, Ahmad Moradi, Michael Perk, Phillip Keldenich, Gabriel Gehrke, Sebastian Krieter, Thomas Thüm, Sándor P. Fekete
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern software systems are typically configurable, a fundamental
prerequisite for wide applicability and reusability. This flexibility poses an
extraordinary challenge for quality assurance, as the enormous number of
possible configurations makes it impractical to test each of them separately.
This is where t-wise interaction sampling can be used to systematically cover
the configuration space and detect unknown feature interactions. Over the last
two decades, numerous algorithms for computing small interaction samples have
been studied, providing improvements for a range of heuristic results;
nevertheless, it has remained unclear how much these results can still be
improved.
  We present a significant breakthrough: a fundamental framework, based on the
mathematical principle of duality, for combining near-optimal solutions with
provable lower bounds on the required sample size. This implies that we no
longer need to work on heuristics with marginal or no improvement, but can
certify the solution quality by establishing a limit on the remaining gap; in
many cases, we can even prove optimality of achieved solutions. This
theoretical contribution also provides extensive practical improvements: Our
algorithm SampLNS was tested on 47 small and medium-sized configurable systems
from the existing literature. SampLNS can reliably find samples of smaller size
than previous methods in 85% of the cases; moreover, we can achieve and prove
optimality of solutions for 63% of all instances. This makes it possible to
avoid cumbersome efforts of minimizing samples by researchers as well as
practitioners, and substantially save testing resources for most configurable
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hold On! Is My Feedback Useful? Evaluating the Usefulness of <span class="highlight-title">Code</span> <span class="highlight-title">Review</span>
  Comments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharif Ahmed, Nasir U. Eisty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context: In collaborative software development, the peer code review process
proves beneficial only when the reviewers provide useful comments. Objective:
This paper investigates the usefulness of Code Review Comments (CR comments)
through textual feature-based and featureless approaches. Method: We select
three available datasets from both open-source and commercial projects.
Additionally, we introduce new features from software and non-software domains.
Moreover, we experiment with the presence of jargon, voice, and codes in CR
comments and classify the usefulness of CR comments through featurization,
bag-of-words, and transfer learning techniques. Results: Our models outperform
the baseline by achieving state-of-the-art performance. Furthermore, the result
demonstrates that the commercial gigantic LLM, GPT-4o, or non-commercial naive
featureless approach, Bag-of-Word with TF-IDF, is more effective for predicting
the usefulness of CR comments. Conclusion: The significant improvement in
predicting usefulness solely from CR comments escalates research on this task.
Our analyses portray the similarities and differences of domains, projects,
datasets, models, and features for predicting the usefulness of CR comments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Empirical Software Engineering (EMSE)
  Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbol Resolution MatRs: Make it Fast and Observable with Stable Linking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farid Zakaria, Andrew Quinn, Thomas R. W. Scogland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic linking is the standard mechanism for using external dependencies
since it enables code reuse, streamlines software updates, and reduces
disk/network use. Dynamic linking waits until runtime to calculate an
application's relocation mapping, i.e., the mapping between each externally
referenced symbol in the application to the dependency that provides the
symbol. Unfortunately, it comes with two downsides. First, dynamic linking
limits the performance of current systems since it can take seconds to
calculate a relocation mapping for a large program. Second, dynamic linking
limits the dependency management of applications since it prevents a developer
from accurately observing a relocation mapping except at runtime.
  This paper makes the key insight that the benefits conventionally attributed
to dynamic linking: code reuse, streamlined software updates, and reduced
disk/network use are actually benefits of shared libraries. Thus, we present
stable linking, a new mechanism for using dependencies that uses shared
libraries to retain their benefits but eliminates the downsides of dynamic
linking. Stable linking separates a system's state into management times; when
the system can be modified, and epochs when it cannot. Stable linking
calculates each application's relocation mapping at the beginning of each
epoch, allows developers to inspect the relocation mapping during the epoch,
and reuses the mapping for subsequent executions in the epoch. We design and
build MatR, the first stable linker. We use MatR in three workloads and show
that it improves upon dynamic linking performance by a factor of 2.19 on
average. Additionally, we use the system in three vignettes, or case-studies,
that illustrate the system's improvements to dependency management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling
  Autonomous Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinfang Chen, Manish Shetty, Gagan Somashekar, Minghua Ma, Yogesh Simmhan, Jonathan Mace, Chetan Bansal, Rujia Wang, Saravan Rajmohan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI for IT Operations (AIOps) aims to automate complex operational tasks, such
as fault localization and root cause analysis, to reduce human workload and
minimize customer impact. While traditional DevOps tools and AIOps algorithms
often focus on addressing isolated operational tasks, recent advances in Large
Language Models (LLMs) and AI agents are revolutionizing AIOps by enabling
end-to-end and multitask automation. This paper envisions a future where AI
agents autonomously manage operational tasks throughout the entire incident
lifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps.
Realizing this vision requires a comprehensive framework to guide the design,
development, and evaluation of these agents. To this end, we present AIOPSLAB,
a framework that not only deploys microservice cloud environments, injects
faults, generates workloads, and exports telemetry data but also orchestrates
these components and provides interfaces for interacting with and evaluating
agents. We discuss the key requirements for such a holistic framework and
demonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps
agents. Through evaluations of state-of-the-art LLM agents within the benchmark
created by AIOPSLAB, we provide insights into their capabilities and
limitations in handling complex operational tasks in cloud environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>ing in the Wild: An Empirical Study of <span class="highlight-title">Prompt</span> Evolution in
  Software Repositories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahan Tafreshipour, Aaron Imani, Eric Huang, Eduardo Almeida, Thomas Zimmermann, Iftekhar Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of Large Language Models (LLMs) is reshaping software
development as developers integrate these LLMs into their applications. In such
applications, prompts serve as the primary means of interacting with LLMs.
Despite the widespread use of LLM-integrated applications, there is limited
understanding of how developers manage and evolve prompts. This study presents
the first empirical analysis of prompt evolution in LLM-integrated software
development. We analyzed 1,262 prompt changes across 243 GitHub repositories to
investigate the patterns and frequencies of prompt changes, their relationship
with code changes, documentation practices, and their impact on system
behavior. Our findings show that developers primarily evolve prompts through
additions and modifications, with most changes occurring during feature
development. We identified key challenges in prompt engineering: only 21.9% of
prompt changes are documented in commit messages, changes can introduce logical
inconsistencies, and misalignment often occurs between prompt changes and LLM
responses. These insights emphasize the need for specialized testing
frameworks, automated validation tools, and improved documentation practices to
enhance the reliability of LLM-integrated applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MSR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ METFORD -- Mutation tEsTing Framework fOR anDroid <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Auri M. R. Vincenzi, Pedro H. Kuroishi, João C. M. Bispo, Ana R. C. da Veiga, David R. C. da Mata, Francisco B. Azevedo, Ana C. R. Paiva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mutation testing may be used to guide test case generation and as a technique
to assess the quality of test suites. Despite being used frequently, mutation
testing is not so commonly applied in the mobile world. One critical challenge
in mutation testing is dealing with its computational cost. Generating mutants,
running test cases over each mutant, and analyzing the results may require
significant time and resources. This research aims to contribute to reducing
Android mutation testing costs. It implements mutation testing operators
(traditional and Android-specific) according to mutant schemata (implementing
multiple mutants into a single code file). It also describes an Android
mutation testing framework developed to execute test cases and determine
mutation scores. Additional mutation operators can be implemented in JavaScript
and easily integrated into the framework. The overall approach is validated
through case studies showing that mutant schemata have advantages over the
traditional mutation strategy (one file per mutant). The results show mutant
schemata overcome traditional mutation in all evaluated aspects with no
additional cost: it takes 8.50% less time for mutant generation, requires
99.78% less disk space, and runs, on average, 6.45% faster than traditional
mutation. Moreover, considering sustainability metrics, mutant schemata have
8,18% less carbon footprint than traditional strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept for publication in the Journal of System and Software - JSS.
  This work is partially supported by Brazilian Funding Agencies FAPESP (Grant
  n. 2019/23160-0 and 2023/00001-9), CAPES, and CNPq</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SusDevOps: Promoting Sustainability to a First Principle in Software
  Delivery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14843v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14843v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Istvan David
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sustainability is becoming a key property of modern software systems. While
there is a substantial and growing body of knowledge on engineering sustainable
software, end-to-end frameworks that situate sustainability-related activities
within the software delivery lifecycle are missing. In this article, we propose
the SusDevOps framework that promotes sustainability to a first principle
within a DevOps context. We demonstrate the lifecycle phases and techniques of
SusDevOps through the case of a software development startup company.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Combining Automation and Expertise: A Semi-automated Approach to
  Correcting Eye Tracking Data in Reading Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naser Al Madi, Brett Torra, Yixin Li, Najam Tariq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reading tasks drift can move fixations from one word to another or even
another line, invalidating the eye tracking recording. Manual correction is
time-consuming and subjective, while automated correction is fast yet limited
in accuracy. In this paper we present Fix8 (Fixate), an open-source GUI tool
that offers a novel semi-automated correction approach for eye tracking data in
reading tasks. The proposed approach allows the user to collaborate with an
algorithm to produce accurate corrections faster without sacrificing accuracy.
Through a usability study (N=14) we assess the time benefits of the proposed
technique, and measure the correction accuracy in comparison to manual
correction. In addition, we assess subjective workload through NASA Task Load
Index, and user opinions through Likert-scale questions. Our results show that
on average the proposed technique was 44% faster than manual correction without
any sacrifice in accuracy. In addition, users reported a preference for the
proposed technique, lower workload, and higher perceived performance compared
to manual correction. Fix8 is a valuable tool that offers useful features for
generating synthetic eye tracking data, visualization, filters, data
converters, and eye movement analysis in addition to the main contribution in
data correction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate
  Patient Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyao Ma, Rui Zhu, Zihao Wang, Jingwei Xiong, Qingyu Chen, Haixu Tang, L. Jean Camp, Lucila Ohno-Machado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing scenarios, particularly in simulating domain-specific experts
using tailored prompts. This ability enables LLMs to adopt the persona of
individuals with specific backgrounds, offering a cost-effective and efficient
alternative to traditional, resource-intensive user studies. By mimicking human
behavior, LLMs can anticipate responses based on concrete demographic or
professional profiles. In this paper, we evaluate the effectiveness of LLMs in
simulating individuals with diverse backgrounds and analyze the consistency of
these simulated behaviors compared to real-world outcomes. In particular, we
explore the potential of LLMs to interpret and respond to discharge summaries
provided to patients leaving the Intensive Care Unit (ICU). We evaluate and
compare with human responses the comprehensibility of discharge summaries among
individuals with varying educational backgrounds, using this analysis to assess
the strengths and limitations of LLM-driven simulations. Notably, when LLMs are
primed with educational background information, they deliver accurate and
actionable medical guidance 88% of the time. However, when other information is
provided, performance significantly drops, falling below random chance levels.
This preliminary study shows the potential benefits and pitfalls of
automatically generating patient-specific health information from diverse
populations. While LLMs show promise in simulating health personas, our results
highlight critical gaps that must be addressed before they can be reliably used
in clinical settings. Our findings suggest that a straightforward
query-response model could outperform a more tailored approach in delivering
health information. This is a crucial first step in understanding how LLMs can
be optimized for personalized health communication while maintaining accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Games! What are they good for? The Struggle of Serious Game Adoption for
  Rehabilitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Micaela Fonseca, Nuno Fachada, Micael Sousa, Jorge Oliveira, Pedro Rodrigues, Sara Sousa, Claudia Quaresma, Phil Lopes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of serious games for health has grown significantly, demonstrating
effectiveness in various clinical contexts such as stroke, spinal cord injury,
and degenerative neurological diseases. Despite their potential benefits,
therapists face barriers to adopting serious games in rehabilitation, including
limited training and game literacy, concerns about cost and equipment
availability, and a lack of evidence-based research on game effectiveness.
Serious games for rehabilitation often involve repetitive exercises, which can
be tedious and reduce motivation for continued rehabilitation, treating clients
as passive recipients of clinical outcomes rather than players. This study
identifies gaps and provides essential insights for advancing serious games in
rehabilitation, aiming to enhance their engagement for clients and
effectiveness as a therapeutic tool. Addressing these challenges requires a
paradigm shift towards developing and co-creating serious games for
rehabilitation with therapists, researchers, and stakeholders. Furthermore,
future research is crucial to advance the development of serious games,
ensuring they adhere to evidence-based principles and engage both clients and
therapists. This endeavor will identify gaps in the field, inspire new
directions, and support the creation of practical guidelines for serious games
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Virtual Reality-Based Telerehabilitation for Upper Limb Recovery
  Post-Stroke: A Systematic <span class="highlight-title">Review</span> of Design Principles, Monitoring, Safety,
  and Engagement Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Rodrigues, Claudia Quaresma, Maria Costa, Filipe Luz, Maria Micaela Fonseca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke rehabilitation continues to face challenges in accessibility and
patient engagement, where traditional approaches often fall short. Virtual
reality (VR)-based telerehabilitation offers a promising avenue, by enabling
home-based recovery through immersive environments and gamification. This
systematic review evaluates current VR solutions for upper-limb post-stroke
recovery, focusing on design principles, safety measures, patient-therapist
communication, and strategies to promote motivation and adherence. Following
PRISMA 2020 guidelines, a comprehensive search was conducted across PubMed,
IEEE Xplore, and ScienceDirect. The review reveals a scarcity of studies
meeting the inclusion criteria, possibly reflecting the challenges inherent in
the current paradigm of VR telerehabilitation systems. Although these systems
have potential to enhance accessibility and patient autonomy, they often lack
standardized safety protocols and reliable real-time monitoring. Human-centered
design principles are evident in some solutions, but inconsistent patient
involvement during the development process limits their usability and clinical
relevance. Furthermore, communication between patients and therapists remains
constrained by technological barriers, although advancements in real-time
feedback and adaptive systems offer promising solutions. This review
underscores the potential of VR telerehabilitation to address critical needs in
upper-limb stroke recovery while highlighting the importance of addressing
existing limitations to ensure broader clinical implementation and improved
patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Foundational Generative Model for Breast Ultrasound Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojun Yu, Youcheng Li, Nan Zhang, Zihan Niu, Xuantong Gong, Yanwen Luo, Haotian Ye, Siyu He, Quanlin Wu, Wangyan Qin, Mengyuan Zhou, Jie Han, Jia Tao, Ziwei Zhao, Di Dai, Di He, Dong Wang, Binghui Tang, Ling Huo, James Zou, Qingli Zhu, Yong Wang, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational models have emerged as powerful tools for addressing various
tasks in clinical settings. However, their potential development to breast
ultrasound analysis remains untapped. In this paper, we present BUSGen, the
first foundational generative model specifically designed for breast ultrasound
image analysis. Pretrained on over 3.5 million breast ultrasound images, BUSGen
has acquired extensive knowledge of breast structures, pathological features,
and clinical variations. With few-shot adaptation, BUSGen can generate
repositories of realistic and informative task-specific data, facilitating the
development of models for a wide range of downstream tasks. Extensive
experiments highlight BUSGen's exceptional adaptability, significantly
exceeding real-data-trained foundational models in breast cancer screening,
diagnosis, and prognosis. In breast cancer early diagnosis, our approach
outperformed all board-certified radiologists (n=9), achieving an average
sensitivity improvement of 16.5% (P-value<0.0001). Additionally, we
characterized the scaling effect of using generated data which was as effective
as the collected real-world data for training diagnostic models. Moreover,
extensive experiments demonstrated that our approach improved the
generalization ability of downstream models. Importantly, BUSGen protected
patient privacy by enabling fully de-identified data sharing, making progress
forward in secure medical data utilization. An online demo of BUSGen is
available at https://aibus.bio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Peking University; Stanford University; Peking University Cancer
  Hospital & Institute; Peking Union Medical College Hospital; Cancer Hospital,
  Chinese Academy of Medical Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward a Universal Concept of Artificial Personality: Implementing
  Robotic Personality in a Kinova Arm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alice Nardelli, Lorenzo Landolfi, Dario Pasquali, Antonio Sgorbissa, Francesco Rea, Carmine Recchiuto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fundamental role of personality in shaping interactions is increasingly
being exploited in robotics. A carefully designed robotic personality has been
shown to improve several key aspects of Human-Robot Interaction (HRI). However,
the fragmentation and rigidity of existing approaches reveal even greater
challenges when applied to non-humanoid robots. On one hand, the state of the
art is very dispersed; on the other hand, Industry 4.0 is moving towards a
future where humans and industrial robots are going to coexist. In this
context, the proper design of a robotic personality can lead to more successful
interactions. This research takes a first step in that direction by integrating
a comprehensive cognitive architecture built upon the definition of robotic
personality - validated on humanoid robots - into a robotic Kinova Jaco2 arm.
The robot personality is defined through the cognitive architecture as a vector
in the three-dimensional space encompassing Conscientiousness, Extroversion,
and Agreeableness, affecting how actions are executed, the action selection
process, and the internal reaction to environmental stimuli. Our main objective
is to determine whether users perceive distinct personalities in the robot,
regardless of its shape, and to understand the role language plays in shaping
these perceptions. To achieve this, we conducted a user study comprising 144
sessions of a collaborative game between a Kinova Jaco2 arm and participants,
where the robot's behavior was influenced by its assigned personality.
Furthermore, we compared two conditions: in the first, the robot communicated
solely through gestures and action choices, while in the second, it also
utilized verbal interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OptiCarVis: Improving Automated Vehicle Functionality Visualizations
  Using Bayesian Optimization to Enhance User Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Jansen, Mark Colley, Svenja Krauß, Daniel Hirschle, Enrico Rukzio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated vehicle (AV) acceptance relies on their understanding via feedback.
While visualizations aim to enhance user understanding of AV's detection,
prediction, and planning functionalities, establishing an optimal design is
challenging. Traditional "one-size-fits-all" designs might be unsuitable,
stemming from resource-intensive empirical evaluations. This paper introduces
OptiCarVis, a set of Human-in-the-Loop (HITL) approaches using Multi-Objective
Bayesian Optimization (MOBO) to optimize AV feedback visualizations. We compare
conditions using eight expert and user-customized designs for a Warm-Start HITL
MOBO. An online study (N=117) demonstrates OptiCarVis's efficacy in
significantly improving trust, acceptance, perceived safety, and predictability
without increasing cognitive load. OptiCarVis facilitates a comprehensive
design space exploration, enhancing in-vehicle interfaces for optimal passenger
experiences and broader applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Cardiac Monitoring using In-ear Ballistocardiogram on COTS
  Wireless Earbuds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongjian Fu, Ke Sun, Ruyao Wang, Xinyi Li, Ju Ren, Yaoxue Zhang, Xinyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human ear offers a unique opportunity for cardiac monitoring due to its
physiological and practical advantages. However, existing earable solutions
require additional hardware and complex processing, posing challenges for
commercial True Wireless Stereo (TWS) earbuds which are limited by their form
factor and resources. In this paper, we propose TWSCardio, a novel system that
repurposes the IMU sensors in TWS earbuds for cardiac monitoring. Our key
finding is that these sensors can capture in-ear ballistocardiogram (BCG)
signals. TWSCardio reuses the unstable Bluetooth channel to stream the IMU data
to a smartphone for BCG processing. It incorporates a signal enhancement
framework to address issues related to missing data and low sampling rate,
while mitigating motion artifacts by fusing multi-axis information.
Furthermore, it employs a region-focused signal reconstruction method to
translate the multi-axis in-ear BCG signals into fine-grained seismocardiogram
(SCG) signals. We have implemented TWSCardio as an efficient real-time app. Our
experiments on 100 subjects verify that TWSCardio can accurately reconstruct
cardiac signals while showing resilience to motion artifacts, missing data, and
low sampling rates. Our case studies further demonstrate that TWSCardio can
support diverse cardiac monitoring applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Sensitivity Vision-Based Tactile Sensing Enhanced by
  Microstructures and Lightweight CNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20758v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20758v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mayue Shi, Yongqi Zhang, Xiaotong Guo, Eric M. Yeatman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile sensing is critical in advanced interactive systems by emulating the
human sense of touch to detect stimuli. Vision-based tactile sensors (VBTSs)
are promising for their ability to provide rich information, robustness,
adaptability, low cost, and multimodal capabilities. However, current
technologies still have limitations in sensitivity, spatial resolution, and the
high computational demands of deep learning-based image processing. This paper
presents a comprehensive approach combining a novel sensor structure with
micromachined structures and an efficient image processing method, and
demonstrates that carefully engineered microstructures within the sensor
hardware can significantly enhance sensitivity while reducing computational
load. Unlike traditional designs with tracking markers, our sensor incorporates
an interface surface with micromachined trenches, as an example of
microstructures, which modulate light transmission and amplify the variation in
response to applied force. By capturing variations in brightness, wire width,
and cross pattern locations with a camera, the sensor accurately infers the
contact location, the magnitude of displacement and applied force with a
lightweight convolutional neural network (CNN). Theoretical and experimental
results demonstrated that the microstructures significantly enhance sensitivity
by amplifying the visual effects of shape distortion. The sensor system
effectively detected forces below 10 mN, and achieved a millimetre-level
single-point spatial resolution. Using a model with only one convolutional
layer, a mean absolute error (MAE) below 0.05 mm have been achieved. Its soft
sensor body ensures compatibility with soft robots and wearable electronics,
while its immunity to electrical crosstalk and interference guarantees
reliability in complex human-machine environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 13 figures, 2 tables; rearranged figures; corrected typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EEG-based AI-BCI Wheelchair Advancement: A Brain-Computer Interfacing
  Wheelchair System Using Deep Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biplov Paneru, Bishwash Paneru, Bipul Thapa, Khem Narayan Poudyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study offers a revolutionary strategy to developing wheelchairs based on
the Brain-Computer Interface (BCI) that incorporates Artificial Intelligence
(AI) using a The device uses electroencephalogram (EEG) data to mimic
wheelchair navigation. Five different models were trained on a pre-filtered
dataset that was divided into fixed-length windows using a sliding window
technique. Each window contained statistical measurements, FFT coefficients for
different frequency bands, and a label identifying the activity carried out
during that window that was taken from an open-source Kaggle repository. The
XGBoost model outperformed the other models, CatBoost, GRU, SVC, and XGBoost,
with an accuracy of 60%. The CatBoost model with a major difference between
training and testing accuracy shows overfitting, and similarly, the
best-performing model, with SVC, was implemented in a tkinter GUI. The
wheelchair movement could be simulated in various directions, and a Raspberry
Pi-powered wheelchair system for brain-computer interface is proposed here.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "What's Happening"- A Human-centered Multimodal Interpreter Explaining
  the Actions of Autonomous Vehicles <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuewen Luo, Fan Ding, Ruiqi Chen, Rishikesh Panda, Junnyong Loo, Shuyun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public distrust of self-driving cars is growing. Studies emphasize the need
for interpreting the behavior of these vehicles to passengers to promote trust
in autonomous systems. Interpreters can enhance trust by improving transparency
and reducing perceived risk. However, current solutions often lack a
human-centric approach to integrating multimodal interpretations. This paper
introduces a novel Human-centered Multimodal Interpreter (HMI) system that
leverages human preferences to provide visual, textual, and auditory feedback.
The system combines a visual interface with Bird's Eye View (BEV), map, and
text display, along with voice interaction using a fine-tuned large language
model (LLM). Our user study, involving diverse participants, demonstrated that
the HMI system significantly boosts passenger trust in AVs, increasing average
trust levels by over 8%, with trust in ordinary environments rising by up to
30%. These results underscore the potential of the HMI system to improve the
acceptance and reliability of autonomous vehicles by providing clear,
real-time, and context-sensitive explanations of vehicle actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at WACV Workshop HAVI
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disconnected from Reality: Do the core concepts of the metaverse exclude
  disabled individuals? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08222v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08222v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Quinlan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commercially-driven metaverse development has been driven by philosophical
and science fiction concepts. Through translating these concepts into products,
the developers may have inadvertently excluded individuals with disabilities
from this new expanded reality. This ideologically-driven development is
presented in this paper through a brief background of what we see as the most
influential of these concepts, and explain how these might affect disabled
individuals wishing to engage with said products. It is our hope that these
ideas prompt conversation on future inclusivity access from the concept stage
of future metaverse development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TIAM23 Workshop at ACM CHI 23, Hamburg, Germany</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-11T00:00:00Z">2025-01-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not real or too soft? On the challenges of publishing interdisciplinary
  software engineering research <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonja M. Hyrynsalmi, Grischa Liebel, Ronnie de Souza Santos, Sebastian Baltes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discipline of software engineering (SE) combines social and technological
dimensions. It is an interdisciplinary research field. However,
interdisciplinary research submitted to software engineering venues may not
receive the same level of recognition as more traditional or technical topics
such as software testing. For this paper, we conducted an online survey of 73
SE researchers and used a mixed-method data analysis approach to investigate
their challenges and recommendations when publishing interdisciplinary research
in SE. We found that the challenges of publishing interdisciplinary research in
SE can be divided into topic-related and reviewing-related challenges.
Furthermore, while our initial focus was on publishing interdisciplinary
research, the impact of current reviewing practices on marginalized groups
emerged from our data, as we found that marginalized groups are more likely to
receive negative feedback. In addition, we found that experienced researchers
are less likely to change their research direction due to feedback they
receive. To address the identified challenges, our participants emphasize the
importance of highlighting the impact and value of interdisciplinary work for
SE, collaborating with experienced researchers, and establishing clearer
submission guidelines and new interdisciplinary SE publication venues. Our
findings contribute to the understanding of the current state of the SE
research community and how we could better support interdisciplinary research
in our field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 4 tables, 47th International Conference on
  Software Engineering: Software Engineering in Society (ICSE-SEIS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Requirements Classification with SMOTE-Tomek Preprocessing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barak Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study emphasizes the domain of requirements engineering by applying the
SMOTE-Tomek preprocessing technique, combined with stratified K-fold
cross-validation, to address class imbalance in the PROMISE dataset. This
dataset comprises 969 categorized requirements, classified into functional and
non-functional types. The proposed approach enhances the representation of
minority classes while maintaining the integrity of validation folds, leading
to a notable improvement in classification accuracy. Logistic regression
achieved 76.16\%, significantly surpassing the baseline of 58.31\%. These
results highlight the applicability and efficiency of machine learning models
as scalable and interpretable solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing The Open Network: Definition and Automated Detection of Smart
  Contract Defects <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Song, Teng Li, Jiachi Chen, Ting Chen, Beibei Li, Zhangyan Lin, Yi Lu, Pan Li, Xihan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Open Network (TON), designed to support Telegram's extensive user base of
hundreds of millions, has garnered considerable attention since its launch in
2022. FunC is the most popular programming language for writing smart contracts
on TON. It is distinguished by a unique syntax compared to other smart contract
languages. Despite growing interest, research on the practical defects of TON
smart contracts is still in its early stages. In this paper, we summarize eight
smart contract defects identified from TON's official blogs and audit reports,
each with detailed definitions and code examples. Furthermore, we propose a
static analysis framework called TONScanner to facilitate the detection of
these defects. Specifically, TONScanner reuses FunC compiler's frontend code to
transform the FunC source code into FunC intermediate representation (IR) in
the form of a directed acyclic graph (DAG). Based on this IR, TONScanner
constructs a control flow graph (CFG), then transforms it into a static single
assignment (SSA) form to simplify further analysis. TONScanner also integrates
Data Dependency, Call Graph, Taint Analysis, and Cell Construct, which are
specifically tailored for TON blockchain's unique data structures. These
components finally facilitate the identification of the eight defects. We
evaluate the effectiveness of TONScanner by applying it to 1,640 smart
contracts and find a total of 14,995 defects. Through random sampling and
manual labeling, we find that TONScanner achieves an overall precision of
97.49%. The results reveal that current TON contracts contain numerous defects,
indicating that developers are prone to making errors. TONScanner has proven
its ability to accurately identify these defects, thereby aiding in their
correction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted for presentation at the 47th IEEE/ACM
  International Conference on Software Engineering (ICSE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Testing in the Wild: A Case Study with Qiskit Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neilson Carlos Leite Ramalho, Erico Augusto da Silva, Higor Amario de Souza, Marcos Lordello Chaim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although classical computing has excelled in a wide range of applications,
there remain problems that push the limits of its capabilities, especially in
fields like cryptography, optimization, and materials science. Quantum
computing introduces a new computational paradigm, based on principles of
superposition and entanglement to explore solutions beyond the capabilities of
classical computation. With the increasing interest in the field, there are
challenges and opportunities for academics and practitioners in terms of
software engineering practices, particularly in testing quantum programs. This
paper presents an empirical study of testing patterns in quantum algorithms. We
analyzed all the tests handling quantum aspects of the implementations in the
Qiskit Algorithms library and identified seven distinct patterns that make use
of (1) fixed seeds for algorithms based on random elements; (2) deterministic
oracles; (3) precise and approximate assertions; (4) Data-Driven Testing (DDT);
(5) functional testing; (6) testing for intermediate parts of the algorithms
being tested; and (7) equivalence checking for quantum circuits. Our results
show a prevalence of classical testing techniques to test the quantum-related
elements of the library, while recent advances from the research community have
yet to achieve wide adoption among practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation in the ERA track at the
  2025 IEEE International Conference on Software Analysis, Evolution and
  Reengineering (SANER 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering Non-native Speakers' Experiences in Global Software
  Development Teams -- A Bourdieusian Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Wang, Yang Yue, Wei Wang, Gaowei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Globally distributed software development has been a mainstream paradigm in
developing modern software systems. We have witnessed a fast-growing population
of software developers from areas where English is not a native language in the
last several decades. Given that English is still the de facto working language
in most global software engineering teams, we need to gain more knowledge about
the experiences of developers who are non-native English speakers. We conducted
an empirical study to fill this research gap. In this study, we interviewed 27
Chinese developers in commercial software development and open source global
software development teams and applied Bourdieu's capital-field-habitus
framework in an abductive data analysis process. Our study reveals four types
of capital (language, social, symbolic, and economic) involved in their
experiences and examines the interrelations among them. We found that
non-native speakers' insufficient language capital played an essential role in
prohibiting them from accessing and accumulating other capital, thus
reproducing the sustained and systematic disadvantaged positions of non-native
English speakers in GSD teams. We further discussed the theoretical and
practical implications of the study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing digital experiences with content delivery networks:
  Architectures, performance strategies, and future trends 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Tyagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research investigates how CDNs (Content Delivery Networks) can improve
the digital experience, as consumers increasingly expect fast, efficient, and
effortless access to online resources. CDNs play a crucial role in reducing
latency, enhancing scalability, and optimizing delivery mechanisms, which is
evident across various platforms and regions. The study focuses on key CDN
concerns, such as foundational and modern CDN architectures, edge computing,
hybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing
topics, including caching, load balancing, and the novel features of HTTP/3 and
QUIC.
  Current trends, such as integrating CDNs with 5G networks, serverless
architectures, and AI-driven traffic management, are examined to demonstrate
how CDN technology is likely to evolve. The study also addresses challenges
related to security, cost, and global regulations. Practical examples from the
e-commerce, streaming, and gaming industries highlight how enhanced CDNs are
transforming these sectors.
  The conclusions emphasize the need to evolve CDN strategies to meet growing
user expectations and adapt to the rapidly changing digital landscape.
Additionally, the research identifies future research opportunities,
particularly in exploring the impact of QC, the enhancement of AI services, and
the sustainability of CDN solutions. Overall, the study situates architectural
design, performance strategies, and emerging trends to address gaps and create
a more efficient and secure approach for improving digital experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards User-Focused Cross-Domain Testing: Disentangling Accessibility,
  Usability, and Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matheus de Morais Leça, Ronnie de Souza Santos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness testing is increasingly recognized as fundamental in software
engineering, especially in the domain of data-driven systems powered by
artificial intelligence. However, its practical integration into software
development may pose challenges, given its overlapping boundaries with
usability and accessibility testing. In this tertiary study, we explore these
complexities using insights from 12 systematic reviews published in the past
decade, shedding light on the nuanced interactions among fairness, usability,
and accessibility testing and how they intersect within contemporary software
development practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling <span class="highlight-title">Code</span> Clones in Quantum Programming: An Empirical Study with
  Qiskit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenta Manoku, Jianjun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code clones, referring to identical or similar code fragments, have long
posed challenges in classical programming, impacting software quality,
maintainability, and scalability. However, their presence and characteristics
in quantum programming remain unexplored. This paper presents an empirical
study of code clones in quantum programs, specifically focusing on software
developed using the Qiskit framework. We examine the existence, distribution,
density, and size of code clones in quantum software, revealing a high density
of Type-2 and Type-3 clones involving minor modifications. Our findings suggest
that these clones are more frequent in quantum software, likely due to the
complexity of quantum algorithms and their integration with classical logic.
This highlights the need for advanced clone detection and refactoring tools
specifically designed for the quantum domain to improve software
maintainability and scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Devising, Evaluating and <span class="highlight-title">Fine-tuning</span> Indoor Tracking
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alpha Diallo, Benoit Garbinato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, we have observed a growing interest in Indoor Tracking
Systems (ITS) for providing location-based services indoors. This is due to the
limitations of Global Navigation and Satellite Systems, which do not operate in
non-line-of-sight environments. Depending on their architecture, ITS can rely
on expensive infrastructure, accumulate errors, or be challenging to evaluate
in real-life environments. Building an ITS is a complex process that involves
devising, evaluating and fine-tuning tracking algorithms. This process is not
yet standard, as researchers use different types of equipment, deployment
environments, and evaluation metrics. Therefore, it is challenging for
researchers to build novel tracking algorithms and for the research community
to reproduce the experiments.
  To address these challenges, we propose MobiXIM, a framework that provides a
set of tools for devising, evaluating and fine-tuning tracking algorithms in a
structured manner. For devising tracking algorithms, MobiXIM introduces a novel
plugin architecture, allowing researchers to collaborate and extend existing
algorithms. We assess our framework by building an ITS encompassing the key
elements of wireless, inertial, and collaborative ITS. The proposed ITS
achieves a positioning accuracy of 4 m, which is an improvement of up to 33%
compared to a baseline Pedestrian Dead Reckoning algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Marketplace: A Benchmark for Data Management in Microservices <span class="chip">SIGMOD'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12605v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12605v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Laigner, Zhexiang Zhang, Yijian Liu, Leonardo Freitas Gomes, Yongluan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microservice architectures have become a popular approach for designing
scalable distributed applications. Despite their extensive use in industrial
settings for over a decade, there is limited understanding of the data
management challenges that arise in these applications. Consequently, it has
been difficult to advance data system technologies that effectively support
microservice applications. To fill this gap, we present Online Marketplace, a
microservice benchmark that highlights core data management challenges that
existing benchmarks fail to address. These challenges include transaction
processing, query processing, event processing, constraint enforcement, and
data replication. We have defined criteria for various data management issues
to enable proper comparison across data systems and platforms.
  Through case studies with state-of-the-art data platforms, we discuss the
issues encountered while implementing and meeting Online Marketplace's
criteria. By capturing the overhead of meeting the key data management
requirements that are overlooked by existing benchmarks, we gain actionable
insights into the experimental platforms. This highlights the significance of
Online Marketplace in advancing future data systems to meet the needs of
microservice practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version accepted at SIGMOD'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACTesting: Automated Cross-modal Testing Method of Text-to-Image
  Software 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12933v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12933v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Gu, Chunrong Fang, Quanjun Zhang, Zhenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, creative generative artificial intelligence software has emerged as
a pivotal assistant, enabling users to generate content and seek inspiration
rapidly. Text-to-Image (T2I) software, one of the most widely used, synthesizes
images with text input by engaging in a cross-modal process. However, despite
substantial advancements in the T2I engine, T2I software still encounters
errors when generating complex or non-realistic scenes, including omitting
focal entities, low image realism, and mismatched text-image information. The
cross-modal nature of T2I software complicates error detection for traditional
testing methods, and the absence of test oracles further exacerbates the
complexity of the testing process. To fill this gap, we propose ACTesting, an
Automated Cross-modal Testing Method of Text-to-Image Software, the first
testing method explicitly designed for T2I software. ACTesting utilizes the
metamorphic testing principle to address the oracle problem and identifies
cross-modal semantic consistency as its fundamental Metamorphic relation (MR)
by employing the Entity-relationship (ER) triples. We design three kinds of
mutation operators under the guidance of MR and the adaptability density
constraint to construct the new input text. After generating the images based
on the text, ACTesting verifies whether MR is satisfied by detecting the ER
triples across two modalities to detect the errors of T2I software. In our
experiments across five popular T2I software, ACTesting effectively generates
error-revealing tests, resulting in a decrease in text-image consistency by up
to 20% when compared to the baseline. Additionally, an ablation study
demonstrates the efficacy of the proposed mutation operators. The experimental
results validate that ACTesting can reliably identify errors within T2I
software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling Overlooked Performance Variance in Serverless Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinfeng Wen, Zhenpeng Chen, Federica Sarro, Shangguang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Serverless computing is an emerging cloud computing paradigm for developing
applications at the function level, known as serverless functions. Due to the
highly dynamic execution environment, multiple identical runs of the same
serverless function can yield different performance, specifically in terms of
end-to-end response latency. However, surprisingly, our analysis of serverless
computing-related papers published in top-tier conferences highlights that the
research community lacks awareness of the performance variance problem, with
only 38.38% of these papers employing multiple runs for quantifying it. To
further investigate, we analyze the performance of 72 serverless functions
collected from these papers. Our findings reveal that the performance of these
serverless functions can differ by up to 338.76% (44.28% on average) across
different runs. Moreover, 61.11% of these functions produce unreliable
performance results, with a low number of repetitions commonly employed in the
serverless computing literature. Our study highlights a lack of awareness in
the serverless computing community regarding the well-known performance
variance problem in software engineering. The empirical results illustrate the
substantial magnitude of this variance, emphasizing that ignoring the variance
can affect research reproducibility and result reliability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication in Empirical Software
  Engineering!</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Few-Shot <span class="highlight-title">Prompt</span>ing of <span class="highlight-title">GPT</span>-4 LLMs with <span class="highlight-title">BERT</span> Classifiers for
  Open-Response Assessment in Tutor Equity Training <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjit Kakarla, Conrad Borchers, Danielle Thomas, Shambhavi Bhushan, Kenneth R. Koedinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing learners in ill-defined domains, such as scenario-based human
tutoring training, is an area of limited research. Equity training requires a
nuanced understanding of context, but do contemporary large language models
(LLMs) have a knowledge base that can navigate these nuances? Legacy
transformer models like BERT, in contrast, have less real-world knowledge but
can be more easily fine-tuned than commercial LLMs. Here, we study whether
fine-tuning BERT on human annotations outperforms state-of-the-art LLMs (GPT-4o
and GPT-4-Turbo) with few-shot prompting and instruction. We evaluate
performance on four prediction tasks involving generating and explaining
open-ended responses in advocacy-focused training lessons in a higher education
student population learning to become middle school tutors. Leveraging a
dataset of 243 human-annotated open responses from tutor training lessons, we
find that BERT demonstrates superior performance using an offline fine-tuning
approach, which is more resource-efficient than commercial GPT models. We
conclude that contemporary GPT models may not adequately capture nuanced
response patterns, especially in complex tasks requiring explanation. This work
advances the understanding of AI-driven learner evaluation under the lens of
fine-tuning versus few-shot prompting on the nuanced task of equity training,
contributing to more effective training solutions and assisting practitioners
in choosing adequate assessment methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Page Workshop Paper, AAAI2025 Workshop on Innovation and
  Responsibility in AI-Supported Education (iRAISE) - Open-response Grading,
  Feedback, Equity Training, LLMs, BERT, GPT-4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Drawing Partner: Co-Creative Drawing Agent and Research Platform to
  Model Co-Creation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Davis, Janet Rafner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the AI Drawing Partner, which is a co-creative drawing
agent that also serves as a research platform to model co-creation. The AI
Drawing Partner is an early example of a quantified co-creative AI system that
automatically models the co-creation that happens on the system. The method the
system uses to capture this data is based on a new cognitive science framework
called co-creative sense-making (CCSM). The CCSM is based on the cognitive
theory of enaction, which describes how meaning emerges through interaction
with the environment and other people in that environment in a process of
sense-making. The CCSM quantifies elements of interaction dynamics to identify
sense-making patterns and interaction trends. This paper describes a new
technique for modeling the interaction and collaboration dynamics of
co-creative AI systems with the co-creative sense-making (CCSM) framework. A
case study is conducted of ten co-creative drawing sessions between a human
user and the co-creative agent. The analysis includes showing the artworks
produced, the quantified data from the AI Drawing Partner, the curves
describing interaction dynamics, and a visualization of interaction trend
sequences. The primary contribution of this paper is presenting the AI Drawing
Partner, which is a unique co-creative AI system and research platform that
collaborates with the user in addition to quantifying, modeling, and
visualizing the co-creative process using the CCSM framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EmoXpt: Analyzing Emotional Variances in Human Comments and
  LLM-Generated Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shireesh Reddy Pyreddy, Tarannum Shaila Zaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of generative AI has generated diverse opinions, with
individuals expressing both support and criticism of its applications. This
study investigates the emotional dynamics surrounding generative AI by
analyzing human tweets referencing terms such as ChatGPT, OpenAI, Copilot, and
LLMs. To further understand the emotional intelligence of ChatGPT, we examine
its responses to selected tweets, highlighting differences in sentiment between
human comments and LLM-generated responses. We introduce EmoXpt, a sentiment
analysis framework designed to assess both human perspectives on generative AI
and the sentiment embedded in ChatGPT's responses. Unlike prior studies that
focus exclusively on human sentiment, EmoXpt uniquely evaluates the emotional
expression of ChatGPT. Experimental results demonstrate that LLM-generated
responses are notably more efficient, cohesive, and consistently positive than
human responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 10 figures, 5 tables. This paper has been accepted and
  presented at the 2025 IEEE 15th Annual Computing and Communication Workshop
  and Conference (CCWC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaffolding Creativity: Integrating Generative AI Tools and Real-world
  Experiences in Business Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicole C. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This case study explores the integration of Generative AI tools and
real-world experiences in business education. Through a study of an innovative
undergraduate course, we investigate how AI-assisted learning, combined with
experiential components, impacts students' creative processes and learning
outcomes. Our findings reveal that this integrated approach accelerates
knowledge acquisition, enables students to overcome traditional creative
barriers, and facilitates a dynamic interplay between AI-generated insights and
real-world observations. The study also highlights challenges, including the
need for instructors with high AI literacy and the rapid evolution of AI tools
creating a moving target for curriculum design. These insights contribute to
the growing body of literature on AI in education and provide actionable
recommendations for educators preparing students for the complexities of modern
business environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NVS-SQA: Exploring <span class="highlight-title">Self-Supervised</span> Quality Representation Learning for
  Neurally Synthesized Scenes without References 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, Weidong Cai, Tongliang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting,
effectively creates photorealistic scenes from sparse viewpoints, typically
evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However,
these full-reference methods, which compare synthesized views to reference
views, may not fully capture the perceptual quality of neurally synthesized
scenes (NSS), particularly due to the limited availability of dense reference
views. Furthermore, the challenges in acquiring human perceptual labels hinder
the creation of extensive labeled datasets, risking model overfitting and
reduced generalizability. To address these issues, we propose NVS-SQA, a NSS
quality assessment method to learn no-reference quality representations through
self-supervision without reliance on human labels. Traditional self-supervised
learning predominantly relies on the "same instance, similar representation"
assumption and extensive datasets. However, given that these conditions do not
apply in NSS quality assessment, we employ heuristic cues and quality scores as
learning objectives, along with a specialized contrastive pair preparation
process to improve the effectiveness and efficiency of learning. The results
show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e.,
on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second
best) and even exceeds 16 full-reference methods across all evaluation metrics
(i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Influencing Humans to Conform to Preference Models for RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephane Hatgis-Kessell, W. Bradley Knox, Serena Booth, Scott Niekum, Peter Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing a reinforcement learning from human feedback (RLHF) algorithm to
approximate a human's unobservable reward function requires assuming,
implicitly or explicitly, a model of human preferences. A preference model that
poorly describes how humans generate preferences risks learning a poor
approximation of the human's reward function. In this paper, we conduct three
human studies to asses whether one can influence the expression of real human
preferences to more closely conform to a desired preference model. Importantly,
our approach does not seek to alter the human's unobserved reward function.
Rather, we change how humans use this reward function to generate preferences,
such that they better match whatever preference model is assumed by a
particular RLHF algorithm. We introduce three interventions: showing humans the
quantities that underlie a preference model, which is normally unobservable
information derived from the reward function; training people to follow a
specific preference model; and modifying the preference elicitation question.
All intervention types show significant effects, providing practical tools to
improve preference data quality and the resultant alignment of the learned
reward functions. Overall we establish a novel research direction in model
alignment: designing interfaces and training interventions to increase human
conformance with the modeling assumptions of the algorithm that will learn from
their input.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Medical Low-Back Pain Physical Rehabilitation Dataset for Human Body
  Movement Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00521v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00521v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sao Mai Nguyen, Maxime Devanne, Olivier Remy-Neris, Mathieu Lempereur, André Thepaut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While automatic monitoring and coaching of exercises are showing encouraging
results in non-medical applications, they still have limitations such as errors
and limited use contexts. To allow the development and assessment of physical
rehabilitation by an intelligent tutoring system, we identify in this article
four challenges to address and propose a medical dataset of clinical patients
carrying out low back-pain rehabilitation exercises. The dataset includes 3D
Kinect skeleton positions and orientations, RGB videos, 2D skeleton data, and
medical annotations to assess the correctness, and error classification and
localisation of body part and timespan. Along this dataset, we perform a
complete research path, from data collection to processing, and finally a small
benchmark. We evaluated on the dataset two baseline movement recognition
algorithms, pertaining to two different approaches: the probabilistic approach
with a Gaussian Mixture Model (GMM), and the deep learning approach with a
Long-Short Term Memory (LSTM).
  This dataset is valuable because it includes rehabilitation relevant motions
in a clinical setting with patients in their rehabilitation program, using a
cost-effective, portable, and convenient sensor, and because it shows the
potential for improvement on these challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WearableMil: An End-to-End Framework for Military Activity Recognition
  and Performance Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barak Gahtan, Shany Funk, Einat Kodesh, Itay Ketko, Tsvi Kuflik, Alex M. Bronstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Musculoskeletal injuries during military training significantly impact
readiness, making prevention through activity monitoring crucial. While Human
Activity Recognition (HAR) using wearable devices offers promising solutions,
it faces challenges in processing continuous data streams and recognizing
diverse activities without predefined sessions. This paper introduces an
end-to-end framework for preprocessing, analyzing, and recognizing activities
from wearable data in military training contexts. Using data from 135 soldiers
wearing \textit{Garmin--55} smartwatches over six months with over 15 million
minutes. We develop a hierarchical deep learning approach that achieves 93.8%
accuracy in temporal splits and 83.8% in cross-user evaluation. Our framework
addresses missing data through physiologically-informed methods, reducing
unknown sleep states from 40.38% to 3.66%. We demonstrate that while longer
time windows (45-60 minutes) improve basic state classification, they present
trade-offs in detecting fine-grained activities. Additionally, we introduce an
intuitive visualization system that enables real-time comparison of individual
performance against group metrics across multiple physiological indicators.
This approach to activity recognition and performance monitoring provides
military trainers with actionable insights for optimizing training programs and
preventing injuries.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-10T00:00:00Z">2025-01-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Probabilistic Framework for Analyzing and Improving
  LLM-Enabled Software 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Manuel Baldonado, Flavia Bonomo-Braberman, Víctor Adrián Braberman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the reliability and verifiability of large language model
(LLM)-enabled systems remains a significant challenge in software engineering.
We propose a probabilistic framework for systematically analyzing and improving
these systems by modeling and refining distributions over clusters of
semantically equivalent outputs. This framework facilitates the evaluation and
iterative improvement of Transference Models -- key software components that
utilize LLMs to transform inputs into outputs for downstream tasks. To
illustrate its utility, we apply the framework to the autoformalization
problem, where natural language documentation is transformed into formal
program specifications. Our case illustrates how probabilistic analysis enables
the identification of weaknesses and guides focused alignment improvements,
resulting in more reliable and interpretable outputs. This principled approach
offers a foundation for addressing critical challenges in the development of
robust LLM-enabled systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning-Driven Adaptation Chains: A Robust Framework for
  Multi-Cloud Workflow Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nafiseh Soveizi, Dimka Karastoyanova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud computing has emerged as a crucial solution for managing data- and
compute-intensive workflows, offering scalability to address dynamic demands.
However, security concerns persist, especially for workflows involving
sensitive data and tasks. One of the main gaps in the literature is the lack of
robust and flexible measures for reacting to these security violations. To
address this, we propose an innovative approach leveraging Reinforcement
Learning (RL) to formulate adaptation chains, responding effectively to
security violations within cloud-based workflows. These chains consist of
sequences of adaptation actions tailored to attack characteristics, workflow
dependencies, and user-defined requirements. Unlike conventional single-task
adaptations, adaptation chains provide a comprehensive mitigation strategy by
taking into account both control and data dependencies between tasks, thereby
accommodating conflicting objectives effectively. Moreover, our RL-based
approach uses insights from past responses to mitigate uncertainties associated
with adaptation costs. We evaluate the method using our jBPM and Cloudsim Plus
based implementation and compare the impact of selected adaptation chains on
workflows with the single adaptation approach. Results demonstrate that the
adaptation chain approach outperforms in terms of total adaptation cost,
offering resilience and adaptability against security threats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dafny as Verification-Aware Intermediate Language for <span class="highlight-title">Code</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Chen Li, Stefan Zetzsche, Siva Somayyajula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using large language models (LLMs) to generate source code from natural
language prompts is a popular and promising idea with a wide range of
applications. One of its limitations is that the generated code can be faulty
at times, often in a subtle way, despite being presented to the user as
correct. In this paper, we explore ways in which formal methods can assist with
increasing the quality of code generated by an LLM. Instead of emitting code in
a target language directly, we propose that the user guides the LLM to first
generate an opaque intermediate representation, in the verification-aware
language Dafny, that can be automatically validated for correctness against
agreed on specifications. The correct Dafny program is then compiled to the
target language and returned to the user. All user-system interactions
throughout the procedure occur via natural language; Dafny code is never
exposed. We describe our current prototype and report on its performance on the
HumanEval Python code generation benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> engineering and its implications on the energy consumption of
  <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Rubei, Aicha Moussaid, Claudio di Sipio, Davide di Ruscio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reducing the environmental impact of AI-based software systems has become
critical. The intensive use of large language models (LLMs) in software
engineering poses severe challenges regarding computational resources, data
centers, and carbon emissions. In this paper, we investigate how prompt
engineering techniques (PETs) can impact the carbon emission of the Llama 3
model for the code generation task. We experimented with the CodeXGLUE
benchmark to evaluate both energy consumption and the accuracy of the generated
code using an isolated testing environment. Our initial results show that the
energy consumption of LLMs can be reduced by using specific tags that
distinguish different prompt parts. Even though a more in-depth evaluation is
needed to confirm our findings, this work suggests that prompt engineering can
reduce LLMs' energy consumption during the inference phase without compromising
performance, paving the way for further investigations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLKAPS: Machine Learning and Adaptive Sampling for HPC Kernel
  Auto-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mathys Jam, Eric Petit, Pablo de Oliveira Castro, David Defour, Greg Henry, William Jalby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many High-Performance Computing (HPC) libraries rely on decision trees to
select the best kernel hyperparameters at runtime,depending on the input and
environment. However, finding optimized configurations for each input and
environment is challengingand requires significant manual effort and
computational resources. This paper presents MLKAPS, a tool that automates this
task usingmachine learning and adaptive sampling techniques. MLKAPS generates
decision trees that tune HPC kernels' design parameters toachieve efficient
performance for any user input. MLKAPS scales to large input and design spaces,
outperforming similar state-of-the-artauto-tuning tools in tuning time and mean
speedup. We demonstrate the benefits of MLKAPS on the highly optimized Intel
MKLdgetrf LU kernel and show that MLKAPS finds blindspots in the manual tuning
of HPC experts. It improves over 85% of the inputswith a geomean speedup of
x1.30. On the Intel MKL dgeqrf QR kernel, MLKAPS improves performance on 85% of
the inputs with ageomean speedup of x1.18.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test Case Generation for Simulink Models: An Experience from the E-Bike
  Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Marzella, Andrea Bombarda, Marcello Minervini, Nunzio Marco Bisceglia, Angelo Gargantini, Claudio Menghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cyber-physical systems development often requires engineers to search for
defects in their Simulink models. Search-based software testing (SBST) is a
standard technology that supports this activity. To increase practical
adaption, industries need empirical evidence of the effectiveness and
efficiency of (existing) SBST techniques on benchmarks from different domains
and of varying complexity. To address this industrial need, this paper presents
our experience assessing the effectiveness and efficiency of SBST in generating
failure-revealing test cases for cyber-physical systems requirements. Our study
subject is within the electric bike (e-Bike) domain and concerns the software
controller of an e-Bike motor, particularly its functional, regulatory, and
safety requirements. We assessed the effectiveness and efficiency of HECATE, an
SBST framework for Simulink models, to analyze two software controllers. HECATE
successfully identified failure-revealing test cases for 83% (30 out of 36) of
our experiments. It required, on average, 1 h 17 min 26 s (min = 11 min 56 s,
max = 8 h 16 min 22 s, std = 1 h 50 min 34 s) to compute the failure-revealing
test cases. The developer of the e-Bike model confirmed the failures identified
by HECATE. We present the lessons learned and discuss the relevance of our
results for industrial applications, the state of practice improvement, and the
results' generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 pages for references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I Can't Share <span class="highlight-title">Code</span>, but I need Translation -- An Empirical Study on <span class="highlight-title">Code</span>
  Translation through Federated LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jahnavi Kumar, Venkata Lakshmana Sasaank Janapati, Mokshith Reddy Tanguturi, Sridhar Chimalakonda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the rapid evolution of technologies and project requirements,
organizations need to upgrade the code base in their software projects to a new
version of the programming language or even translating to an entirely new one.
However, code translation is resource-intensive and requires expertise in both
the source and target languages. While researchers have made progress in
automating translations between legacy and modern languages, recent work has
increasingly turned to pre-trained Large Language Models (LLMs) to translate
efficiently.
  Given the proprietary nature of code, organizations prefer fine-tuning LLMs
locally rather than relying on external APIs. This is one of the first
empirical studies that proposes a Federated LLM-based approach for code
translation. The proposed approach enables clients to jointly train a code
translator without sharing sensitive data. This study demonstrates that
participants can collaboratively develop a FedLLM for efficient code
translation (particularly C\# to Java and vice-versa) with superior results
(more than 40\% improvement in CodeLLaMA's CodeBLEU score) compared to
individual client models. Our findings indicate that FedLLM offers a
collaborative approach to code translation and could serve as a promising
direction for future research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debugging Without Error Messages: How LLM <span class="highlight-title">Prompt</span>ing Strategy Affects
  Programming Error Explanation Effectiveness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Audrey Salmon, Katie Hammer, Eddie Antonio Santos, Brett A. Becker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Making errors is part of the programming process -- even for the most
seasoned professionals. Novices in particular are bound to make many errors
while learning. It is well known that traditional (compiler/interpreter)
programming error messages have been less than helpful for many novices and can
have effects such as being frustrating, containing confusing jargon, and being
downright misleading. Recent work has found that large language models (LLMs)
can generate excellent error explanations, but that the effectiveness of these
error messages heavily depends on whether the LLM has been provided with
context -- typically the original source code where the problem occurred.
Knowing that programming error messages can be misleading and/or contain that
serves little-to-no use (particularly for novices) we explore the reverse: what
happens when GPT-3.5 is prompted for error explanations on just the erroneous
source code itself -- original compiler/interpreter produced error message
excluded. We utilized various strategies to make more effective error
explanations, including one-shot prompting and fine-tuning. We report the
baseline results of how effective the error explanations are at providing
feedback, as well as how various prompting strategies might improve the
explanations' effectiveness. Our results can help educators by understanding
how LLMs respond to such prompts that novices are bound to make, and hopefully
lead to more effective use of Generative AI in the classroom.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Test Case Repair Using Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06765v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06765v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmadreza Saboor Yaraghi, Darren Holden, Nafiseh Kahani, Lionel Briand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring the quality of software systems through testing is essential, yet
maintaining test cases poses significant challenges and costs. The need for
frequent updates to align with the evolving system under test often entails
high complexity and cost for maintaining these test cases. Further, unrepaired
broken test cases can degrade test suite quality and disrupt the software
development process, wasting developers' time. To address this challenge, we
present TaRGet (Test Repair GEneraTor), a novel approach leveraging pre-trained
code language models for automated test case repair. TaRGet treats test repair
as a language translation task, employing a two-step process to fine-tune a
language model based on essential context data characterizing the test
breakage. To evaluate our approach, we introduce TaRBench, a comprehensive
benchmark we developed covering 45,373 broken test repairs across 59
open-source projects. Our results demonstrate TaRGet's effectiveness, achieving
a 66.1% exact match accuracy. Furthermore, our study examines the effectiveness
of TaRGet across different test repair scenarios. We provide a practical guide
to predict situations where the generated test repairs might be less reliable.
We also explore whether project-specific data is always necessary for
fine-tuning and if our approach can be effective on new projects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closing the Gap: A User Study on the Real-world Usefulness of AI-powered
  Vulnerability Detection & Repair in the IDE <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Steenhoek, Kalpathy Sivaraman, Renata Saldivar Gonzalez, Yevhen Mohylevskyy, Roshanak Zilouchian Moghaddam, Wei Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first empirical study of a vulnerability detection
and fix tool with professional software developers on real projects that they
own. We implemented DeepVulGuard, an IDE-integrated tool based on
state-of-the-art detection and fix models, and show that it has promising
performance on benchmarks of historic vulnerability data. DeepVulGuard scans
code for vulnerabilities (including identifying the vulnerability type and
vulnerable region of code), suggests fixes, provides natural-language
explanations for alerts and fixes, leveraging chat interfaces. We recruited 17
professional software developers at Microsoft, observed their usage of the tool
on their code, and conducted interviews to assess the tool's usefulness, speed,
trust, relevance, and workflow integration. We also gathered detailed
qualitative feedback on users' perceptions and their desired features. Study
participants scanned a total of 24 projects, 6.9k files, and over 1.7 million
lines of source code, and generated 170 alerts and 50 fix suggestions. We find
that although state-of-the-art AI-powered detection and fix tools show promise,
they are not yet practical for real-world use due to a high rate of false
positives and non-applicable fixes. User feedback reveals several actionable
pain points, ranging from incomplete context to lack of customization for the
user's codebase. Additionally, we explore how AI features, including confidence
scores, explanations, and chat interaction, can apply to vulnerability
detection and fixing. Based on these insights, we offer practical
recommendations for evaluating and deploying AI detection and fix models. Our
code and data are available at https://doi.org/10.6084/m9.figshare.26367139.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICSE 2025 research track. Camera-ready version with
  updated acknowledgments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Invisible Labor in Open Source Software Ecosystems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Meluso, Amanda Casari, Katie McLaughlin, Milo Z. Trujillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Invisible labor is work that is either not fully visible or not appropriately
compensated. In open source software (OSS) ecosystems, essential tasks that do
not involve code (like content moderation) often become invisible to the
detriment of individuals and organizations. However, invisible labor is
sufficiently difficult to measure that we do not know how much of OSS
activities are invisible. Our study addresses this challenge, demonstrating
that roughly half of OSS work is invisible. We do this by developing a
cognitive anchoring survey technique that measures OSS developer
self-assessments of labor visibility and attribution. Survey respondents
(n=142) reported that their work is more likely to be invisible (2 in 3 tasks)
than visible, and that half (50.1%) is uncompensated. Priming participants with
the idea of visibility caused participants to think their work was more
visible, and that visibility was less important, than those primed with
invisibility. We also found evidence that tensions between attribution
motivations probably increase how common invisible labor is. This suggests that
advertising OSS activities as "open" may lead contributors to overestimate how
visible their labor actually is. Our findings suggest benefits to working with
varied stakeholders to make select, collectively valued activities visible, and
increasing compensation in valued forms (like attribution, opportunities, or
pay) when possible. This could improve fairness in software development while
providing greater transparency into work designs that help organizations and
communities achieve their goals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 7 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On <span class="highlight-title">Large Language Model</span>s in Mission-Critical IT Governance: Are We Ready
  Yet? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Esposito, Francesco Palagiano, Valentina Lenarduzzi, Davide Taibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context. The security of critical infrastructure has been a pressing concern
since the advent of computers and has become even more critical in today's era
of cyber warfare. Protecting mission-critical systems (MCSs), essential for
national security, requires swift and robust governance, yet recent events
reveal the increasing difficulty of meeting these challenges. Aim. Building on
prior research showcasing the potential of Generative AI (GAI), such as Large
Language Models, in enhancing risk analysis, we aim to explore practitioners'
views on integrating GAI into the governance of IT MCSs. Our goal is to provide
actionable insights and recommendations for stakeholders, including
researchers, practitioners, and policymakers. Method. We designed a survey to
collect practical experiences, concerns, and expectations of practitioners who
develop and implement security solutions in the context of MCSs. Conclusions
and Future Works. Our findings highlight that the safe use of LLMs in MCS
governance requires interdisciplinary collaboration. Researchers should focus
on designing regulation-oriented models and focus on accountability;
practitioners emphasize data protection and transparency, while policymakers
must establish a unified AI framework with global benchmarks to ensure ethical
and secure LLMs-based MCS governance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-In-the-Loop Software Development Agents <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wannita Takerngsaksiri, Jirat Pasuksmit, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Ruixiong Zhang, Fan Jiang, Jing Li, Evan Cook, Kun Chen, Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs)-based multi-agent paradigms for
software engineering are introduced to automatically resolve software
development tasks (e.g., from a given issue to source code). However, existing
work is evaluated based on historical benchmark datasets, rarely considers
human feedback at each stage of the automated software development process, and
has not been deployed in practice. In this paper, we introduce a
Human-in-the-loop LLM-based Agents framework (HULA) for software development
that allows software engineers to refine and guide LLMs when generating coding
plans and source code for a given task. We design, implement, and deploy the
HULA framework into Atlassian JIRA for internal uses. Through a multi-stage
evaluation of the HULA framework, Atlassian software engineers perceive that
HULA can minimize the overall development time and effort, especially in
initiating a coding plan and writing code for straightforward tasks. On the
other hand, challenges around code quality remain a concern in some cases. We
draw lessons learned and discuss opportunities for future work, which will pave
the way for the advancement of LLM-based agents in software development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, ICSE SEIP 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Automate This? Exploring the Connection between Time Use, Well-being
  and Robot Automation Across Social Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruchira Ray, Leona Pang, Sanjana Srivastava, Li Fei-Fei, Samantha Shorey, Roberto Martín-Martín
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the motivations underlying the human inclination to automate
tasks is vital to developing truly helpful robots integrated into daily life.
Accordingly, we ask: are individuals more inclined to automate chores based on
the time they consume or the feelings experienced while performing them? This
study explores these preferences and whether they vary across different social
groups (i.e., gender category and income level). Leveraging data from the
BEHAVIOR-1K dataset, the American Time-Use Survey, and the American Time-Use
Survey Well-Being Module, we investigate the relationship between the desire
for automation, time spent on daily activities, and their associated feelings -
Happiness, Meaningfulness, Sadness, Painfulness, Stressfulness, or Tiredness.
Our key findings show that, despite common assumptions, time spent does not
strongly relate to the desire for automation for the general population. For
the feelings analyzed, only happiness and pain are key indicators. Significant
differences by gender and economic level also emerged: Women prefer to automate
stressful activities, whereas men prefer to automate those that make them
unhappy; mid-income individuals prioritize automating less enjoyable and
meaningful activities, while low and high-income show no significant
correlations. We hope our research helps motivate technologies to develop
robots that match the priorities of potential users, moving domestic robotics
toward more socially relevant solutions. We open-source all the data, including
an online tool that enables the community to replicate our analysis and explore
additional trends at https://hri1260.github.io/why-automate-this.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding How Paper Writers Use AI-Generated Captions in Figure
  Caption Writing <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Yin,  Ng, Ting-Yao Hsu, Jiyoo Min, Sungchul Kim, Ryan A. Rossi, Tong Yu, Hyunggu Jung, Ting-Hao 'Kenneth' Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Figures and their captions play a key role in scientific publications.
However, despite their importance, many captions in published papers are poorly
crafted, largely due to a lack of attention by paper authors. While prior AI
research has explored caption generation, it has mainly focused on
reader-centered use cases, where users evaluate generated captions rather than
actively integrating them into their writing. This paper addresses this gap by
investigating how paper authors incorporate AI-generated captions into their
writing process through a user study involving 18 participants. Each
participant rewrote captions for two figures from their own recently published
work, using captions generated by state-of-the-art AI models as a resource. By
analyzing video recordings of the writing process through interaction analysis,
we observed that participants often began by copying and refining AI-generated
captions. Paper writers favored longer, detail-rich captions that integrated
textual and visual elements but found current AI models less effective for
complex figures. These findings highlight the nuanced and diverse nature of
figure caption composition, revealing design opportunities for AI systems to
better support the challenges of academic writing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper will appear at AAAI 2025 Workshop (2nd AI4Research
  Workshop: Towards a Knowledge-grounded Scientific Research Lifecycle)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScooterLab: A Programmable and Participatory Sensing Research Testbed
  using Micromobility Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ubaidullah Khan, Raveen Wijewickrama, Buddhi Ashan M. K., A. H. M. Nazmus Sakib, Khoi Trinh, Christina Duthie, Nima Najafian, Ahmer Patel, R. N. Molina, Anindya Maiti, Sushil K. Prasad, Greg P. Griffin, Murtuza Jadliwala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Micromobility vehicles, such as e-scooters, are increasingly popular in urban
communities but present significant challenges in terms of road safety, user
privacy, infrastructure planning, and civil engineering. Addressing these
critical issues requires a large-scale and easily accessible research
infrastructure to collect diverse mobility and contextual data from
micromobility users in realistic settings. To this end, we present ScooterLab,
a community research testbed comprising a fleet of customizable battery-powered
micromobility vehicles retrofitted with advanced sensing, communication, and
control capabilities. ScooterLab enables interdisciplinary research at the
intersection of computing, mobility, and urban planning by providing
researchers with tools to design and deploy customized sensing experiments and
access curated datasets. The testbed will enable advances in machine learning,
privacy, and urban transportation research while promoting sustainable
mobility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The interplay of user preference and precision in different gaze-based
  interaction methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06073v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06073v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Björn Rene Severitt, Yannick Sauer, Alexander Neugebauer, Rajat Agarwala, Nora Castner, Siegfried Wahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we investigated gaze-based interaction methods within a
virtual reality game with a visual search task with 52 participants. We
compared four different interaction techniques: Selection by dwell time or
confirmation of selection by head orientation, nodding or smooth pursuit eye
movements. We evaluated both subjective and objective performance metrics,
including NASA-TLX for subjective task load as well as time to find the correct
targets and points achieved for objective analysis. The results showed
significant differences between the interaction methods in terms of NASA TLX
dimensions, time to find the right targets, and overall performance scores,
suggesting differential effectiveness of gaze-based approaches in improving
intuitive system communication. Interestingly, the results revealed
gender-specific differences, suggesting interesting implications for the design
of gaze-based interaction paradigms that are optimized for different user needs
and preferences. These findings could help to develop more customized and
effective gaze interaction systems that can improve accessibility and user
satisfaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MinMo: A Multimodal <span class="highlight-title">Large Language Model</span> for Seamless Voice Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Chen, Yafeng Chen, Yanni Chen, Mengzhe Chen, Yingda Chen, Chong Deng, Zhihao Du, Ruize Gao, Changfeng Gao, Zhifu Gao, Yabin Li, Xiang Lv, Jiaqing Liu, Haoneng Luo, Bin Ma, Chongjia Ni, Xian Shi, Jialong Tang, Hui Wang, Hao Wang, Wen Wang, Yuxuan Wang, Yunlan Xu, Fan Yu, Zhijie Yan, Yexin Yang, Baosong Yang, Xian Yang, Guanrou Yang, Tianyu Zhao, Qinglin Zhang, Shiliang Zhang, Nan Zhao, Pei Zhang, Chong Zhang, Jinren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) and multimodal
speech-text models have laid the groundwork for seamless voice interactions,
enabling real-time, natural, and human-like conversations. Previous models for
voice interactions are categorized as native and aligned. Native models
integrate speech and text processing in one framework but struggle with issues
like differing sequence lengths and insufficient pre-training. Aligned models
maintain text LLM capabilities but are often limited by small datasets and a
narrow focus on speech tasks. In this work, we introduce MinMo, a Multimodal
Large Language Model with approximately 8B parameters for seamless voice
interaction. We address the main limitations of prior aligned multimodal
models. We train MinMo through multiple stages of speech-to-text alignment,
text-to-speech alignment, speech-to-speech alignment, and duplex interaction
alignment, on 1.4 million hours of diverse speech data and a broad range of
speech tasks. After the multi-stage training, MinMo achieves state-of-the-art
performance across various benchmarks for voice comprehension and generation
while maintaining the capabilities of text LLMs, and also facilitates
full-duplex conversation, that is, simultaneous two-way communication between
the user and the system. Moreover, we propose a novel and simple voice decoder
that outperforms prior models in voice generation. The enhanced
instruction-following capabilities of MinMo supports controlling speech
generation based on user instructions, with various nuances including emotions,
dialects, and speaking rates, and mimicking specific voices. For MinMo, the
speech-to-text latency is approximately 100ms, full-duplex latency is
approximately 600ms in theory and 800ms in practice. The MinMo project web page
is https://funaudiollm.github.io/minmo, and the code and models will be
released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. Authors are listed in alphabetical order by family
  name</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring LLMs for Automated Pre-Testing of Cross-Cultural <span class="highlight-title">Survey</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divya Mani Adhikari, Vikram Kamath Cannanure, Alexander Hartland, Ingmar Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing culturally relevant questionnaires for ICTD research is
challenging, particularly when adapting surveys for populations to non-western
contexts. Prior work adapted questionnaires through expert reviews and pilot
studies, which are resource-intensive and time-consuming. To address these
challenges, we propose using large language models (LLMs) to automate the
questionnaire pretesting process in cross-cultural settings. Our study used
LLMs to adapt a U.S.-focused climate opinion survey for a South African
audience. We then tested the adapted questionnaire with 116 South African
participants via Prolific, asking them to provide feedback on both versions.
Participants perceived the LLM-adapted questions as slightly more favorable
than the traditional version. Our note opens discussions on the potential role
of LLMs in adapting surveys and facilitating cross-cultural questionnaire
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICTD 2024 (Notes)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying Think-Aloud in ICTD: A Case Study of a Chatbot Use by Teachers
  in Rural Côte d'Ivoire 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikram Kamath Cannanure, Sharon Wolf, Kaja Jasińska, Timothy X Brown, Amy Ogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Think-alouds are a common HCI usability method where participants verbalize
their thoughts while using interfaces. However, their utility in cross-cultural
settings, particularly in the Global South, is unclear, where cultural
differences impact user interactions. This paper investigates the usability
challenges teachers in rural C\^ote d'Ivoire faced when using a chatbot
designed to support an educational program. We conducted think-aloud sessions
with 20 teachers two weeks after a chatbot deployment, analyzing their
navigation, errors, and time spent on tasks. We discuss our approach and
findings that helped us identify usability issues and challenging features for
improving the chatbot designs. Our note summarizes our reflections on using
think-aloud and contributes to discussions on its culturally sensitive
adaptation in the Global South.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICTD 24, Notes track. International Conference on Information &
  Communication Technologies and Development 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Impact of Human Feedback via Influence Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taywon Min, Haeone Lee, Hanho Ryu, Yongchan Kwon, Kimin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn
suitable reward models from human feedback to align large language models
(LLMs) with human intentions. However, human feedback can often be noisy,
inconsistent, or biased, especially when evaluating complex responses. Such
feedback can lead to misaligned reward signals, potentially causing unintended
side effects during the RLHF process. To address these challenges, we explore
the use of influence functions to measure the impact of human feedback on the
performance of reward models. We propose a compute-efficient approximation
method that enables the application of influence functions to LLM-based reward
models and large-scale preference datasets. In our experiments, we demonstrate
two key applications of influence functions: (1) detecting common forms of
labeler bias in human feedback datasets and (2) guiding labelers to refine
their strategies to align more closely with expert feedback. By quantifying the
impact of human feedback on reward models, we believe that influence functions
can enhance feedback interpretability and contribute to scalable oversight in
RLHF, helping labelers provide more accurate and consistent feedback. Source
code is available at https://github.com/mintaywon/IF_RLHF
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Source code: https://github.com/mintaywon/IF_RLHF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Enable Effective Cooperation Between Humans and NLP Models: A
  <span class="highlight-title">Survey</span> of Principles, Formalizations, and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Huang, Yang Deng, Wenqiang Lei, Jiancheng Lv, Tat-Seng Chua, Jimmy Xiangji Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of large language models (LLMs), intelligent models have
evolved from mere tools to autonomous agents with their own goals and
strategies for cooperating with humans. This evolution has birthed a novel
paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable
progress in numerous NLP tasks in recent years. In this paper, we take the
first step to present a thorough review of human-model cooperation, exploring
its principles, formalizations, and open challenges. In particular, we
introduce a new taxonomy that provides a unified perspective to summarize
existing approaches. Also, we discuss potential frontier areas and their
corresponding challenges. We regard our work as an entry point, paving the way
for more breakthrough research in this regard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debugging Without Error Messages: How LLM <span class="highlight-title">Prompt</span>ing Strategy Affects
  Programming Error Explanation Effectiveness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Audrey Salmon, Katie Hammer, Eddie Antonio Santos, Brett A. Becker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Making errors is part of the programming process -- even for the most
seasoned professionals. Novices in particular are bound to make many errors
while learning. It is well known that traditional (compiler/interpreter)
programming error messages have been less than helpful for many novices and can
have effects such as being frustrating, containing confusing jargon, and being
downright misleading. Recent work has found that large language models (LLMs)
can generate excellent error explanations, but that the effectiveness of these
error messages heavily depends on whether the LLM has been provided with
context -- typically the original source code where the problem occurred.
Knowing that programming error messages can be misleading and/or contain that
serves little-to-no use (particularly for novices) we explore the reverse: what
happens when GPT-3.5 is prompted for error explanations on just the erroneous
source code itself -- original compiler/interpreter produced error message
excluded. We utilized various strategies to make more effective error
explanations, including one-shot prompting and fine-tuning. We report the
baseline results of how effective the error explanations are at providing
feedback, as well as how various prompting strategies might improve the
explanations' effectiveness. Our results can help educators by understanding
how LLMs respond to such prompts that novices are bound to make, and hopefully
lead to more effective use of Generative AI in the classroom.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visualization Tool: Exploring COVID-19 Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Hyun Jeon, Jong Kwan Lee, Prabal Dhaubhadel, Aaron Kuhlman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to effectively visualize data is crucial in the contemporary
world where information is often voluminous and complex. Visualizations, such
as charts, graphs, and maps, provide an intuitive and easily understandable
means to interpret, analyze, and communicate patterns, trends, and insights
hidden within large datasets. These graphical representations can help
researchers, policymakers, and the public to better comprehend and respond to a
multitude of issues. In this study, we explore a visualization tool to
interpret and understand various data of COVID-19 pandemic. While others have
shown COVID-19 visualization methods/tools, our tool provides a mean to analyze
COVID-19 data in a more comprehensive way. We have used the public data from NY
Times and CDC, and various COVID-19 data (e.g., core places, patterns, foot
traffic) from Safegraph. Figure 1 shows the basic view of our visualization
view. In addition to providing visualizations of these data, our visualization
also considered the Surprising Map. The Surprising Map is a type of choropleth
map that can avoid misleading of producing visual prominence to known base
rates or to artifacts of sample size and normalization in visualizing the
density of events in spatial data. It is based on Bayesian surprise-it creates
a space of equi-plausible models and uses Bayesian updating to re-estimate
their plausibility based on individual events.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ISIITA 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing Sleep and Study: Cultural Contexts in Family Informatics for
  Taiwanese Parents and Children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Hong, Ru-Yun Tseng, Ying-Yu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the intersection of academic pressure and sleep within
Taiwanese families, revealing how cultural norms and expectations shape sleep
practices. Through interviews and two-week diaries from eleven families, we
found that academic demands significantly influence children's sleep patterns,
leading to reduced sleep duration and varied sleep schedules. Our research
highlights the importance of integrating care and attuning into the design of
sleep-tracking technologies, advocating for a family informatics approach that
considers both health needs and social expectations. By exploring these
dynamics, we contribute to a broader understanding of family contexts in
diverse cultural settings and offer insights for more inclusive technology
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 2 figures, ACM GROUP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExoFabric: A Re-moldable Textile System for Creating Customizable Soft
  Goods and Wearable Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rosalie Lin, Aditi Maheshwari, Jung Wook Park, Andreea Danielescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fabric has been a fundamental part of human life for thousands of years,
providing comfort, protection, and aesthetic expression. While modern
advancements have enhanced fabric's functionality, it remains static and
unchangeable, failing to adapt to our evolving body shapes and preferences.
This lack of adaptability can lead to unsustainable practices, as consumers
often buy more items to meet their changing needs. In this paper, we propose
ExoFabric, a re-moldable fabric system for customized soft goods applications.
We created ExoFabric by embedding thermoplastic threads into fabric through
computerized embroidery to allow for tunability between rigid plastic and
conformable fabric. We defined a library of design primitives to enable
geometric formability, stiffness, and stretchability by identifying suitable
fabrics, threads, embroidery parameters, and machine limitations. To facilitate
practical applications, we demonstrated practical methods for linking
parameters to application requirements, showcasing form-fitting wearables,
structural support, and shape-changeable furniture for repeatable or one-time
customization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concerns and Values in Human-Robot Interactions: A Focus on Social
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Antonio Abbo, Tony Belpaeme, Micol Spitale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots, as AI with physical instantiation, inhabit our social and physical
world, where their actions have both social and physical consequences, posing
challenges for researchers when designing social robots. This study starts with
a scoping review to identify discussions and potential concerns arising from
interactions with robotic systems. Two focus groups of technology ethics
experts then validated a comprehensive list of key topics and values in
human-robot interaction (HRI) literature. These insights were integrated into
the HRI Value Compass web tool, to help HRI researchers identify ethical values
in robot design. The tool was evaluated in a pilot study. This work benefits
the HRI community by highlighting key concerns in human-robot interactions and
providing an instrument to help researchers design robots that align with human
values, ensuring future robotic systems adhere to these values in social
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 10 figures, 5 appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning About Algorithm Auditing in Five Steps: Scaffolding How High
  School Youth Can Systematically and Critically Evaluate Machine Learning
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06989v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06989v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Morales-Navarro, Yasmin B. Kafai, Lauren Vogelstein, Evelyn Yu, Danaë Metaxa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While there is widespread interest in supporting young people to critically
evaluate machine learning-powered systems, there is little research on how we
can support them in inquiring about how these systems work and what their
limitations and implications may be. Outside of K-12 education, an effective
strategy in evaluating black-boxed systems is algorithm auditing-a method for
understanding algorithmic systems' opaque inner workings and external impacts
from the outside in. In this paper, we review how expert researchers conduct
algorithm audits and how end users engage in auditing practices to propose five
steps that, when incorporated into learning activities, can support young
people in auditing algorithms. We present a case study of a team of teenagers
engaging with each step during an out-of-school workshop in which they audited
peer-designed generative AI TikTok filters. We discuss the kind of scaffolds we
provided to support youth in algorithm auditing and directions and challenges
for integrating algorithm auditing into classroom activities. This paper
contributes: (a) a conceptualization of five steps to scaffold algorithm
auditing learning activities, and (b) examples of how youth engaged with each
step during our pilot study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robots in Family Routines: Development of and Initial Insights from the
  Family-Robot Routines Inventory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael F. Xu, Bengisu Cagiltay, Joseph Michaelis, Sarah Sebo, Bilge Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advances in areas such as the personalization of robots, sustaining
adoption of robots for long-term use in families remains a challenge. Recent
studies have identified integrating robots into families' routines and rituals
as a promising approach to support long-term adoption. However, few studies
explored the integration of robots into family routines and there is a gap in
systematic measures to capture family preferences for robot integration.
Building upon existing routine inventories, we developed Family-Robot Routines
Inventory (FRRI), with 24 family routines and 24 child routine items, to
capture parents' attitudes toward and expectations from the integration of
robotic technology into their family routines. Using this inventory, we
collected data from 150 parents through an online survey. Our analysis
indicates that parents had varying perceptions for the utility of integrating
robots into their routines. For example, parents found robot integration to be
more helpful in children's individual routines, than to the collective routines
of their families. We discuss the design implications of these preliminary
findings, and how they may serve as a first step toward understanding the
diverse challenges and demands of designing and integrating household robots
for families.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Use of Robots for Diary Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael F. Xu, Bilge Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As interest in studying in-the-wild human-robot interaction grows, there is a
need for methods to collect data over time and in naturalistic or potentially
private environments. HRI researchers have increasingly used the diary method
for these studies, asking study participants to self-administer a structured
data collection instrument, i.e., a diary, over a period of time. Although the
diary method offers a unique window into settings that researchers may not have
access to, they also lack the interactivity and probing that interview-based
methods offer. In this paper, we explore a novel data collection method in
which a robot plays the role of an interactive diary. We developed the Diary
Robot system and performed in-home deployments for a week to evaluate the
feasibility and effectiveness of this approach. Using traditional text-based
and audio-based diaries as benchmarks, we found that robots are able to
effectively elicit the intended information. We reflect on our findings, and
describe scenarios where the utilization of robots in diary studies as a data
collection instrument may be especially applicable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 20th ACM/IEEE International Conference on Human
  Robot Interaction (HRI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Problem Solving in Mixed Reality: A Study on Visual Graph
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitar Garkov, Tommaso Piselli, Emilio Di Giacomo, Karsten Klein, Giuseppe Liotta, Fabrizio Montecchiani, Falk Schreiber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Problem solving is a composite cognitive process, invoking a number of
systems and subsystems, such as perception and memory. Individuals may form
collectives to solve a given problem together, in collaboration, especially
when complexity is thought to be high. To determine if and when collaborative
problem solving is desired, we must quantify collaboration first. For this, we
investigate the practical virtue of collaborative problem solving. Using visual
graph analysis, we perform a study with 72 participants in two countries and
three languages. We compare ad hoc pairs to individuals and nominal pairs,
solving two different tasks on graphs in visuospatial mixed reality. The
average collaborating pair does not outdo its nominal counterpart, but it does
have a significant trade-off against the individual: an ad hoc pair uses 1.46
more time to achieve 4.6 higher accuracy. We also use the concept of task
instance complexity to quantify differences in complexity. As task instance
complexity increases, these differences largely scale, though with two notable
exceptions. With this study we show the importance of using nominal groups as
benchmark in collaborative virtual environments research. We conclude that a
mixed reality environment does not automatically imply superior collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VLM-driven Behavior Tree for Context-aware Task Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Wake, Atsushi Kanehira, Jun Takamatsu, Kazuhiro Sasabuchi, Katsushi Ikeuchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of Large Language Models (LLMs) for generating Behavior Trees (BTs)
has recently gained attention in the robotics community, yet remains in its
early stages of development. In this paper, we propose a novel framework that
leverages Vision-Language Models (VLMs) to interactively generate and edit BTs
that address visual conditions, enabling context-aware robot operations in
visually complex environments. A key feature of our approach lies in the
conditional control through self-prompted visual conditions. Specifically, the
VLM generates BTs with visual condition nodes, where conditions are expressed
as free-form text. Another VLM process integrates the text into its prompt and
evaluates the conditions against real-world images during robot execution. We
validated our framework in a real-world cafe scenario, demonstrating both its
feasibility and limitations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures, 5 tables. Last updated on January 9th, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-In-the-Loop Software Development Agents <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wannita Takerngsaksiri, Jirat Pasuksmit, Patanamon Thongtanunam, Chakkrit Tantithamthavorn, Ruixiong Zhang, Fan Jiang, Jing Li, Evan Cook, Kun Chen, Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs)-based multi-agent paradigms for
software engineering are introduced to automatically resolve software
development tasks (e.g., from a given issue to source code). However, existing
work is evaluated based on historical benchmark datasets, rarely considers
human feedback at each stage of the automated software development process, and
has not been deployed in practice. In this paper, we introduce a
Human-in-the-loop LLM-based Agents framework (HULA) for software development
that allows software engineers to refine and guide LLMs when generating coding
plans and source code for a given task. We design, implement, and deploy the
HULA framework into Atlassian JIRA for internal uses. Through a multi-stage
evaluation of the HULA framework, Atlassian software engineers perceive that
HULA can minimize the overall development time and effort, especially in
initiating a coding plan and writing code for straightforward tasks. On the
other hand, challenges around code quality remain a concern in some cases. We
draw lessons learned and discuss opportunities for future work, which will pave
the way for the advancement of LLM-based agents in software development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, ICSE SEIP 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-09T00:00:00Z">2025-01-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing <span class="highlight-title">Large Language Model</span> for Virtual Reality Exploration Testing:
  A Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Qi, Haotang Li, Hao Qin, Kebin Peng, Sen He, Xue Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the Virtual Reality (VR) industry expands, the need for automated GUI
testing is growing rapidly. Large Language Models (LLMs), capable of retaining
information long-term and analyzing both visual and textual data, are emerging
as a potential key to deciphering the complexities of VR's evolving user
interfaces. In this paper, we conduct a case study to investigate the
capability of using LLMs, particularly GPT-4o, for field of view (FOV) analysis
in VR exploration testing. Specifically, we validate that LLMs can identify
test entities in FOVs and that prompt engineering can effectively enhance the
accuracy of test entity identification from 41.67% to 71.30%. Our study also
shows that LLMs can accurately describe identified entities' features with at
least a 90% correction rate. We further find out that the core features that
effectively represent an entity are color, placement, and shape. Furthermore,
the combination of the three features can especially be used to improve the
accuracy of determining identical entities in multiple FOVs with the highest
F1-score of 0.70. Additionally, our study demonstrates that LLMs are capable of
scene recognition and spatial understanding in VR with precisely designed
structured prompts. Finally, we find that LLMs fail to label the identified
test entities, and we discuss potential solutions as future research
directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Search-based Testing of Simulink Models with Requirements Tables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Formica, Chris George, Shayda Rahmatyan, Vera Pantelic, Mark Lawford, Angelo Gargantini, Claudio Menghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search-based software testing (SBST) of Simulink models helps find scenarios
that demonstrate that the system can reach a state that violates one of its
requirements. However, many SBST techniques for Simulink models rely on
requirements being expressed in logical languages, limiting their adoption in
industry. To help with the adoption, SBST methods and tools for Simulink models
need to be integrated with tools used by engineers to specify requirements.
This work presents the first black-box testing approach for Simulink models
that supports Requirements Table (RT), a tool from Simulink Requirements
Toolbox used by practitioners to express software requirements.
  We evaluated our solution by considering 60 model-RT combinations each made
by a model and an RT. Our SBST framework returned a failure-revealing test case
for 70% of the model-RT combinations. Remarkably, it identified a
failure-revealing test case for three model-RT combinations for a cruise
controller of an industrial simulator that other previously used tools were not
able to find. The efficiency of our SBST solution is acceptable for practical
applications and comparable with existing SBST tools that are not based on RT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair<span class="highlight-title">Code</span>: Evaluating Social Bias of LLMs in <span class="highlight-title">Code</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongkang Du, Jen-tse Huang, Jieyu Zhao, Lu Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated significant capability in code
generation, drawing increasing attention to the evaluation of the quality and
safety of their outputs. However, research on bias in code generation remains
limited. Existing studies typically assess bias by applying malicious prompts
or reapply tasks and dataset for discriminative models. Given that LLMs are
often aligned with human values and that prior datasets are not fully optimized
for code-related tasks, there is a pressing need for benchmarks specifically
designed for evaluating code models. In this study, we introduce FairCode, a
novel benchmark for evaluating bias in code generation. FairCode comprises two
tasks: function implementation and test case generation, each evaluating social
bias through diverse scenarios. Additionally, we propose a new metric,
FairScore, to assess model performance on this benchmark. We conduct
experiments on widely used LLMs and provide a comprehensive analysis of the
results. The findings reveal that all tested LLMs exhibit bias. The code is
available at https://github.com/YongkDu/FairCode.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automating the Detection of <span class="highlight-title">Code</span> Vulnerabilities by Analyzing GitHub
  Issues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Cipollone, Changjie Wang, Mariano Scazzariello, Simone Ferlin, Maliheh Izadi, Dejan Kostic, Marco Chiesa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's digital landscape, the importance of timely and accurate
vulnerability detection has significantly increased. This paper presents a
novel approach that leverages transformer-based models and machine learning
techniques to automate the identification of software vulnerabilities by
analyzing GitHub issues. We introduce a new dataset specifically designed for
classifying GitHub issues relevant to vulnerability detection. We then examine
various classification techniques to determine their effectiveness. The results
demonstrate the potential of this approach for real-world application in early
vulnerability detection, which could substantially reduce the window of
exploitation for software vulnerabilities. This research makes a key
contribution to the field by providing a scalable and computationally efficient
framework for automated detection, enabling the prevention of compromised
software usage before official notifications. This work has the potential to
enhance the security of open-source software ecosystems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CallNavi: A Study and Challenge on Function Calling Routing and
  Invocation in <span class="highlight-title">Large Language Model</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yewei Song, Cedric Lothritz, Xunzhu Tang, Saad Ezzini, Jacques Klein, Tegawendé F. Bissyandé, Andrey Boytsov, Ulrick Ble, Anne Goujon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interacting with a software system via a chatbot can be challenging,
especially when the chatbot needs to generate API calls, in the right order and
with the right parameters, to communicate with the system. API calling in
chatbot systems poses significant challenges, particularly in complex,
multi-step tasks requiring accurate API selection and execution. We contribute
to this domain in three ways: first, by introducing a novel dataset designed to
assess models on API function selection, parameter generation, and nested API
calls; second, by benchmarking state-of-the-art language models across varying
levels of complexity to evaluate their performance in API function generation
and parameter accuracy; and third, by proposing an enhanced API routing method
that combines general-purpose large language models for API selection with
fine-tuned models for parameter generation and some prompt engineering
approach. These approaches lead to substantial improvements in handling complex
API tasks, offering practical advancements for real-world API-driven chatbot
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Scientific Texts to Verifiable <span class="highlight-title">Code</span>: Automating the Process with
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changjie Wang, Mariano Scazzariello, Marco Chiesa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the vast body of research literature proposing algorithms with formal
guarantees, the amount of verifiable code in today's systems remains minimal.
This discrepancy stems from the inherent difficulty of verifying code,
particularly due to the time-consuming nature and strict formalism of proof
details that formal verification tools require. However, the emergence of
transformers in Large Language Models presents a promising solution to this
challenge. In this position paper, we believe that transformers have the
potential to read research papers that propose algorithms with formal proofs
and translate these proofs into verifiable code. We leverage transformers to
first build a formal structure of the proof using the original text from the
paper, and then to handle the tedious, low-level aspects of proofs that are
often omitted by humans. We argue that this approach can significantly reduce
the barrier to formal verification. The above idea of reading papers to write
verifiable code opens new avenues for automating the verification of complex
systems, enabling a future where formally verified algorithms from academic
research can more seamlessly transition into real-world software systems,
thereby improving code reliability and security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient
  Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Puccioni, Alireza Farshin, Mariano Scazzariello, Changjie Wang, Marco Chiesa, Dejan Kostic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated their exceptional performance
in various complex code generation tasks. However, their broader adoption is
limited by significant computational demands and high resource requirements,
particularly memory and processing power. To mitigate such requirements, model
pruning techniques are used to create more compact models with significantly
fewer parameters. However, current approaches do not focus on the efficient
extraction of programming-language-specific sub-models. In this work, we
explore the idea of efficiently deriving coding-specific sub-models through
unstructured pruning (i.e., Wanda). We investigate the impact of different
domain-specific calibration datasets on pruning outcomes across three distinct
domains and extend our analysis to extracting four language-specific
sub-models: Python, Java, C++, and JavaScript. We are the first to efficiently
extract programming-language-specific sub-models using appropriate calibration
datasets while maintaining acceptable accuracy w.r.t. full models. We are also
the first to provide analytical evidence that domain-specific tasks activate
distinct regions within LLMs, supporting the creation of specialized sub-models
through unstructured pruning. We believe that this work has significant
potential to enhance LLM accessibility for coding by reducing computational
requirements to enable local execution on consumer-grade hardware, and
supporting faster inference times critical for real-time development feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online <span class="highlight-title">Prompt</span> and Solver Selection for Program Synthesis <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Li, Lewis Frampton, Federico Mora, Elizabeth Polgreen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate impressive capabilities in the
domain of program synthesis. This level of performance is not, however,
universal across all tasks, all LLMs and all prompting styles. There are many
areas where one LLM dominates, one prompting style dominates, or where calling
a symbolic solver is a better choice than an LLM. A key challenge for the user
then, is to identify not only when an LLM is the right choice of solver, and
the appropriate LLM to call for a given synthesis task, but also the right way
to call it. A non-expert user who makes the wrong choice, incurs a cost both in
terms of results (number of tasks solved, and the time it takes to solve them)
and financial cost, if using a closed-source language model via a commercial
API. We frame this choice as an online learning problem. We use a multi-armed
bandit algorithm to select which symbolic solver, or LLM and prompt combination
to deploy in order to maximize a given reward function (which may prioritize
solving time, number of synthesis tasks solved, or financial cost of solving).
We implement an instance of this approach, called CYANEA, and evaluate it on
synthesis queries from the literature in ranking function synthesis, from the
syntax-guided synthesis competition, and fresh, unseen queries generated from
SMT problems. CYANEA solves 37.2\% more queries than the best single solver and
achieves results within 4\% of the virtual best solver.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 39th AAAI Conference on Artificial Intelligence
  (AAAI-25) Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Assessment of <span class="highlight-title">Code</span> <span class="highlight-title">Review</span> Generation Approaches: Beyond Lexical
  Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjie Jiang, Hui Liu, Tianyi Chen, Fu Fan, Chunhao Dong, Kui Liu, Lu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code review is a standard practice for ensuring the quality of software
projects, and recent research has focused extensively on automated code review.
While significant advancements have been made in generating code reviews, the
automated assessment of these reviews remains less explored, with existing
approaches and metrics often proving inaccurate. Current metrics, such as BLEU,
primarily rely on lexical similarity between generated and reference reviews.
However, such metrics tend to underestimate reviews that articulate the
expected issues in ways different from the references. In this paper, we
explore how semantic similarity between generated and reference reviews can
enhance the automated assessment of code reviews. We first present a benchmark
called \textit{GradedReviews}, which is constructed by collecting real-world
code reviews from open-source projects, generating reviews using
state-of-the-art approaches, and manually assessing their quality. We then
evaluate existing metrics for code review assessment using this benchmark,
revealing their limitations. To address these limitations, we propose two novel
semantic-based approaches for assessing code reviews. The first approach
involves converting both the generated review and its reference into digital
vectors using a deep learning model and then measuring their semantic
similarity through Cosine similarity. The second approach generates a prompt
based on the generated review and its reference, submits this prompt to
ChatGPT, and requests ChatGPT to rate the generated review according to
explicitly defined criteria. Our evaluation on the \textit{GradedReviews}
benchmark indicates that the proposed semantic-based approaches significantly
outperform existing state-of-the-art metrics in assessing generated code
review, improving the correlation coefficient between the resulting scores and
human scores from 0.22 to 0.47.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in
  Secure Software Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Esposito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context. Developing secure and reliable software remains a key challenge in
software engineering (SE). The ever-evolving technological landscape offers
both opportunities and threats, creating a dynamic space where chaos and order
compete. Secure software engineering (SSE) must continuously address
vulnerabilities that endanger software systems and carry broader socio-economic
risks, such as compromising critical national infrastructure and causing
significant financial losses. Researchers and practitioners have explored
methodologies like Static Application Security Testing Tools (SASTTs) and
artificial intelligence (AI) approaches, including machine learning (ML) and
large language models (LLMs), to detect and mitigate these vulnerabilities.
Each method has unique strengths and limitations.
  Aim. This thesis seeks to bring order to the chaos in SSE by addressing
domain-specific differences that impact AI accuracy.
  Methodology. The research employs a mix of empirical strategies, such as
evaluating effort-aware metrics, analyzing SASTTs, conducting method-level
analysis, and leveraging evidence-based techniques like systematic dataset
reviews. These approaches help characterize vulnerability prediction datasets.
  Results. Key findings include limitations in static analysis tools for
identifying vulnerabilities, gaps in SASTT coverage of vulnerability types,
weak relationships among vulnerability severity scores, improved defect
prediction accuracy using just-in-time modeling, and threats posed by untouched
methods.
  Conclusions. This thesis highlights the complexity of SSE and the importance
of contextual knowledge in improving AI-driven vulnerability and defect
prediction. The comprehensive analysis advances effective prediction models,
benefiting both researchers and practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-based <span class="highlight-title">Code</span> Completion: On the Impact on Performance of
  Contextual Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Ciniselli, Luca Pascarella, Gabriele Bavota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code completion aims at speeding up code writing by recommending to
developers the next tokens they are likely to type. Deep Learning (DL) models
pushed the boundaries of code completion by redefining what these coding
assistants can do: We moved from predicting few code tokens to automatically
generating entire functions. One important factor impacting the performance of
DL-based code completion techniques is the context provided as input. With
"context" we refer to what the model knows about the code to complete. In a
simple scenario, the DL model might be fed with a partially implemented
function to complete. In this case, the context is represented by the
incomplete function and, based on it, the model must generate a prediction. It
is however possible to expand such a context to include additional information,
like the whole source code file containing the function to complete, which
could be useful to boost the prediction performance. In this work, we present
an empirical study investigating how the performance of a DL-based code
completion technique is affected by different contexts. We experiment with 8
types of contexts and their combinations. These contexts include: (i) coding
contexts, featuring information extracted from the code base in which the code
completion is invoked (e.g., code components structurally related to the one to
"complete"); (ii) process context, with information aimed at depicting the
current status of the project in which a code completion task is triggered
(e.g., a textual representation of open issues relevant for the code to
complete); and (iii) developer contexts, capturing information about the
developer invoking the code completion (e.g., the APIs frequently used). Our
results show that additional contextual information can benefit the performance
of DL-based code completion, with relative improvements up to +22% in terms of
correct predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Generalizability of <span class="highlight-title">Transformer</span> Models to <span class="highlight-title">Code</span> Completions of
  Different Lengths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Cooper, Rosalia Tufano, Gabriele Bavota, Denys Poshyvanyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The programming landscape is nowadays being reshaped by the advent of Large
Language Models (LLMs) able to automate code-related tasks related to code
implementation (e.g., code completion) and comprehension (e.g., code
summarization). Such a paradigm shift comes with a number of implications
related to how software will be written, maintained, and evolved. Also, these
LLMs are extremely expensive to train, posing questions on their sustainability
over time. Given their training cost, their ability to generalize, namely their
ability to work on task instances different from those on which they have been
trained, is an aspect worth being investigated. Previous work already showed
that transformer models can successfully support code completion in a
cross-project setting. However, it is unclear whether LLM are able to
generalize to inputs having lengths not seen during training. For example, it
is known that training a model on short instances allows to substantially
reduce the training cost. However, the extent to which such a model would
provide good performance on sequences having lengths not seen during training
is not known. Many recent works in Natural Language Processing (NLP) tackled
this problem in the context of decoder-only LLMs, i.e., xPOS and ALiBi. To
assess if these solutions extend to encoder-decoder LLMs usually adopted in the
code-related tasks, we present a large empirical study evaluating this
generalization property of these and other encoding schemes proposed in the
literature, namely Sinusoidal, xPOS, ALiBi, and T5. We found that none of these
solutions successfully generalize to unseen lengths and that the only safe
solution is to ensure the representativeness in the training set of all lengths
likely to be encountered at inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICSME 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Diagnosis of Flaky Job Failures: Understanding and Prioritizing
  Failure Categories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henri Aïdasso, Francis Bordeleau, Ali Tizghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The continuous delivery of modern software requires the execution of many
automated pipeline jobs. These jobs ensure the frequent release of new software
versions while detecting code problems at an early stage. For TELUS, our
industrial partner in the telecommunications field, reliable job execution is
crucial to minimize wasted time and streamline Continuous Deployment (CD). In
this context, flaky job failures are one of the main issues hindering CD. Prior
studies proposed techniques based on machine learning to automate the detection
of flaky jobs. While valuable, these solutions are insufficient to address the
waste associated with the diagnosis of flaky failures, which remain largely
unexplored due to the wide range of underlying causes. This study examines
4,511 flaky job failures at TELUS to identify the different categories of flaky
failures that we prioritize based on Recency, Frequency, and Monetary (RFM)
measures. We identified 46 flaky failure categories that we analyzed using
clustering and RFM measures to determine 14 priority categories for future
automated diagnosis and repair research. Our findings also provide valuable
insights into the evolution and impact of these categories. The identification
and prioritization of flaky failure categories using RFM analysis introduce a
novel approach that can be used in other contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at the 47th International Conference on
  Software Engineering: Software Engineering in Practice 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Developer-written Unit Test Case Reduction for Java -- A
  Replication Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan D Le, Brandon Wilber, Arpit Christi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract: Failing test case reduction can promote efficient debugging because
a developer may not need to observe components that are not relevant to
inducing failure. Failing test case reduction can also improve the efficiency
of fault localization. These considerations have prompted researchers to study
the reduction process, the reduction output, and the removed entities. Christi
et al. studied test reduction using a tool called ReduSharptor for C# tests.
They considered the test to be an Abstract Syntax Tree (AST). Based on that,
they studied the reduction outcome and removed entities in terms of Leaf nodes
and Non-Leaf nodes of the AST. They claimed that (1) leaf nodes are removed in
large numbers, and (2) the probability of removal is slightly higher than
non-leaf nodes. We replicate their results using a different test case
reduction tool, ReduJavator, for Java unit tests. We evaluate test reduction
using 30 randomly chosen bugs from the Defects4J database and 30 mutants for 6
open-source projects. Our results confirm their first claim: leaf nodes are
removed in large numbers. Our results are inconclusive regarding their second
claim; we cannot confirm that the probability of removal is higher for non-leaf
nodes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages and 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Architecture Frameworks by Including Modern Stakeholders and
  their Views/Viewpoints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05239v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05239v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Moin, Atta Badii, Stephan Günnemann, Moharram Challenger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various architecture frameworks for software, systems, and enterprises have
been proposed in the literature. They identified several stakeholders and
defined modeling perspectives, architecture viewpoints, and views to frame and
address stakeholder concerns. However, the stakeholders with data science and
Machine Learning (ML) related concerns, such as data scientists and data
engineers, are yet to be included in existing architecture frameworks. Only
this way can we envision a holistic system architecture description of an
ML-enabled system. Note that the ML component behavior and functionalities are
special and should be distinguished from traditional software system behavior
and functionalities. The main reason is that the actual functionality should be
inferred from data instead of being specified at design time. Additionally, the
structural models of ML components, such as ML model architectures, are
typically specified using different notations and formalisms from what the
Software Engineering (SE) community uses for software structural models. Yet,
these two aspects, namely ML and non-ML, are becoming so intertwined that it
necessitates an extension of software architecture frameworks and modeling
practices toward supporting ML-enabled system architectures. In this paper, we
address this gap through an empirical study using an online survey instrument.
We surveyed 61 subject matter experts from over 25 organizations in 10
countries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICICT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Software Fairness Debt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02490v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02490v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ronnie de Souza Santos, Felipe Fronchetti, Savio Freire, Rodrigo Spinola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As software systems continue to play a significant role in modern society,
ensuring their fairness has become a critical concern in software engineering.
Motivated by this scenario, this paper focused on exploring the multifaceted
nature of bias in software systems, aiming to provide a comprehensive
understanding of its origins, manifestations, and impacts. Through a scoping
study, we identified the primary causes of fairness deficiency in software
development and highlighted their adverse effects on individuals and
communities, including instances of discrimination and the perpetuation of
inequalities. Our investigation culminated in the introduction of the concept
of software fairness debt, which complements the notions of technical and
social debt, encapsulating the accumulation of biases in software engineering
practices while emphasizing the societal ramifications of bias embedded within
software systems. Our study contributes to a deeper understanding of fairness
in software engineering and paves the way for the development of more equitable
and socially responsible software systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AgentForge: A Flexible Low-<span class="highlight-title">Code</span> Platform for Reinforcement Learning
  Agent Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19528v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19528v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francisco Erivaldo Fernandes Junior, Antti Oulasvirta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing a reinforcement learning (RL) agent often involves identifying
values for numerous parameters, covering the policy, reward function,
environment, and agent-internal architecture. Since these parameters are
interrelated in complex ways, optimizing them is a black-box problem that
proves especially challenging for nonexperts. Although existing
optimization-as-a-service platforms (e.g., Vizier and Optuna) can handle such
problems, they are impractical for RL systems, since the need for manual user
mapping of each parameter to distinct components makes the effort cumbersome.
It also requires understanding of the optimization process, limiting the
systems' application beyond the machine learning field and restricting access
in areas such as cognitive science, which models human decision-making. To
tackle these challenges, the paper presents AgentForge, a flexible low-code
platform to optimize any parameter set across an RL system. Available at
https://github.com/feferna/AgentForge, it allows an optimization problem to be
defined in a few lines of code and handed to any of the interfaced optimizers.
With AgentForge, the user can optimize the parameters either individually or
jointly. The paper presents an evaluation of its performance for a challenging
vision-based RL problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at the 17th International Conference on
  Agents and Artificial Intelligence (ICAART 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Automated Fixes Truly Mitigate Smart Contract Exploits? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofia Bobadilla, Monica Jin, Martin Monperrus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated Program Repair (APR) for smart contract security promises to
automatically mitigate smart contract vulnerabilities responsible for billions
in financial losses. However, the true effectiveness of this research in
addressing smart contract exploits remains uncharted territory. This paper
bridges this critical gap by introducing a novel and systematic experimental
framework for evaluating exploit mitigation of program repair tools for smart
contracts. We qualitatively and quantitatively analyze 20 state-of-the-art APR
tools using a dataset of 143 vulnerable smart contracts, for which we manually
craft 91 executable exploits. We are the very first to define and measure the
essential "exploit mitigation rate", giving researchers and practitioners and
real sense of effectiveness of cutting edge techniques. Our findings reveal
substantial disparities in the state of the art, with an exploit mitigation
rate ranging from a low of 27% to a high of 73%, a result that nobody would
guess from reading the original papers. Our study identifies systemic
limitations, such as inconsistent functionality preservation, that must be
addressed in future research on program repair for smart contracts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LogLM: From Task-based to Instruction-based Automated Log Analysis <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Liu, Yuhe Ji, Shimin Tao, Minggui He, Weibin Meng, Shenglin Zhang, Yongqian Sun, Yuming Xie, Boxing Chen, Hao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic log analysis is essential for the efficient Operation and
Maintenance (O&M) of software systems, providing critical insights into system
behaviors. However, existing approaches mostly treat log analysis as training a
model to perform an isolated task ( e.g., anomaly detection, log parsing, etc.)
using task-specific log-label pairs. These task-based approaches are inflexible
in generalizing to complex scenarios, depend on task-specific training data,
and cost significantly when deploying multiple models. In this paper, we
propose an instruction-based training approach that transforms log-label pairs
from multiple tasks and domains into a unified format of instruction-response
pairs. Our trained model, LogLM, can follow complex user instructions and
generalize better across different tasks, thereby increasing flexibility and
reducing the dependence on task-specific training data. By integrating major
log analysis tasks into a single model, our approach also relieves model
deployment burden. Experimentally, LogLM outperforms existing approaches across
five log analysis capabilities, and exhibits strong generalization abilities on
complex instructions and unseen tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICSE 2025 (SEIP Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning-Driven Programming: A <span class="highlight-title">Large Language Model</span> Programming Workflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Lei, Yanchuan Chang, Nir Lipovetzky, Krista A. Ehinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The strong performance of large language models (LLMs) raises extensive
discussion on their application to code generation. Recent research suggests
continuous program refinements through visible tests to improve code generation
accuracy in LLMs. However, these methods suffer from LLMs' inefficiency and
limited reasoning capacity. In this work, we propose an LLM programming
workflow (LPW) designed to improve both initial code generation and subsequent
refinements within a structured two-phase workflow. Specifically, the solution
generation phase formulates a solution plan, which is then verified through
visible tests to specify the intended natural language solution. Subsequently,
the code implementation phase drafts an initial code according to the solution
plan and its verification. If the generated code fails the visible tests, the
plan verification serves as the intended solution to consistently inform the
refinement process for correcting bugs. Compared to state-of-the-art methods
across various existing LLMs, LPW significantly improves the Pass@1 accuracy by
up to 16.4% on well-established text-to-code generation benchmarks. LPW also
sets new state-of-the-art Pass@1 accuracy, achieving 98.2% on HumanEval, 84.8%
on MBPP, 59.3% on LiveCode, 62.6% on APPS, and 34.7% on CodeContest, using
GPT-4o as the backbone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing the Power of LLM to Support Binary Taint Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08275v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08275v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puzhuo Liu, Chengnian Sun, Yaowen Zheng, Xuan Feng, Chuan Qin, Yuncheng Wang, Zhenyang Xu, Zhi Li, Peng Di, Yu Jiang, Limin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes LATTE, the first static binary taint analysis that is
powered by a large language model (LLM). LATTE is superior to the state of the
art (e.g., Emtaint, Arbiter, Karonte) in three aspects. First, LATTE is fully
automated while prior static binary taint analyzers need rely on human
expertise to manually customize taint propagation rules and vulnerability
inspection rules. Second, LATTE is significantly effective in vulnerability
detection, demonstrated by our comprehensive evaluations. For example, LATTE
has found 37 new bugs in real-world firmware which the baselines failed to
find, and 7 of them have been assigned CVE numbers. Lastly, LATTE incurs
remarkably low engineering cost, making it a cost-efficient and scalable
solution for security researchers and practitioners. We strongly believe that
LATTE opens up a new direction to harness the recent advance in LLMs to improve
vulnerability analysis for binary programs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages,16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAPGen: An Approach for Fixing <span class="highlight-title">Code</span> Inefficiencies in Zero-Shot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17077v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17077v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spandan Garg, Roshanak Zilouchian Moghaddam, Neel Sundaresan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance bugs are non-functional bugs that can even manifest in
well-tested commercial products. Fixing these performance bugs is an important
yet challenging problem. In this work, we address this challenge and present a
new approach called Retrieval-Augmented Prompt Generation (RAPGen). Given a
code snippet with a performance issue, RAPGen first retrieves a prompt
instruction from a pre-constructed knowledge-base of previous performance bug
fixes and then generates a prompt using the retrieved instruction. It then uses
this prompt on a Large Language Model (such as Codex) in zero-shot to generate
a fix. We compare our approach with the various prompt variations and state of
the art methods in the task of performance bug fixing. Our evaluation shows
that RAPGen can generate performance improvement suggestions equivalent or
better than a developer in ~60% of the cases, getting ~42% of them verbatim, in
an expert-verified dataset of past performance changes made by C# developers.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Human-Computer Interaction <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Employing Social Media to Improve Mental Health Outcomes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Munmun De Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As social media platforms are increasingly adopted, the data the data people
leave behind is shining new light into our understanding of phenomena, ranging
from socio-economic-political events to the spread of infectious diseases. This
chapter presents research conducted in the past decade that has harnessed
social media data in the service of mental health and well-being. The
discussion is organized along three thrusts: a first that highlights how social
media data has been utilized to detect and predict risk to varied mental health
concerns; a second thrust that focuses on translation paradigms that can enable
to use of such social media based algorithms in the real-world; and the final
thrust that brings to the fore the ethical considerations and challenges that
engender the conduct of this research as well as its translation. The chapter
concludes by noting open questions and problems in this emergent area,
emphasizing the need for deeper interdisciplinary collaborations and
participatory research design, incorporating and centering on human agency, and
attention to societal inequities and harms that may result from or be
exacerbated in this line of computational social science research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Probabilistic Inference of Human Motor Intentions by Assistive
  Mobile Robots Controlled via a Brain-Computer Interface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshan Zhou, Carol M. Menassa, Vineet R. Kamat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive mobile robots are a transformative technology that helps persons
with disabilities regain the ability to move freely. Although autonomous
wheelchairs significantly reduce user effort, they still require human input to
allow users to maintain control and adapt to changing environments. Brain
Computer Interface (BCI) stands out as a highly user-friendly option that does
not require physical movement. Current BCI systems can understand whether users
want to accelerate or decelerate, but they implement these changes in discrete
speed steps rather than allowing for smooth, continuous velocity adjustments.
This limitation prevents the systems from mimicking the natural, fluid speed
changes seen in human self-paced motion. The authors aim to address this
limitation by redesigning the perception-action cycle in a BCI controlled
robotic system: improving how the robotic agent interprets the user's motion
intentions (world state) and implementing these actions in a way that better
reflects natural physical properties of motion, such as inertia and damping.
The scope of this paper focuses on the perception aspect. We asked and answered
a normative question "what computation should the robotic agent carry out to
optimally perceive incomplete or noisy sensory observations?" Empirical EEG
data were collected, and probabilistic representation that served as world
state distributions were learned and evaluated in a Generative Adversarial
Network framework. The ROS framework was established that connected with a
Gazebo environment containing a digital twin of an indoor space and a virtual
model of a robotic wheelchair. Signal processing and statistical analyses were
implemented to identity the most discriminative features in the
spatial-spectral-temporal dimensions, which are then used to construct the
world model for the robotic agent to interpret user motion intentions as a
Bayesian observer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Multifaceted Nature of Mentoring in OSS: Strategies, Qualities, and
  Ideal Outcomes <span class="chip">ASE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Feng, Igor Steinmacher, Marco Gerosa, Tyler Menezes, Alexander Serebrenik, Reed Milewicz, Anita Sarma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mentorship in open source software (OSS) is a vital, multifaceted process
that includes onboarding newcomers, fostering skill development, and enhancing
community building. This study examines task-focused mentoring strategies that
help mentees complete their tasks and the ideal personal qualities and outcomes
of good mentorship in OSS communities. We conducted two surveys to gather
contributor perceptions: the first survey, with 70 mentors, mapped 17 mentoring
challenges to 21 strategies that help support mentees. The second survey, with
85 contributors, assessed the importance of personal qualities and ideal
mentorship outcomes. Our findings not only provide actionable strategies to
help mentees overcome challenges and become successful contributors but also
guide current and future mentors and OSS communities in understanding the
personal qualities that are the cornerstone of good mentorship and the outcomes
that mentor-mentee pairs should aspire to achieve.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, the 18th International Conference on Cooperative and Human
  Aspects of Software Engineering (CHASE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-centered Geospatial Data Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This entry provides an overview of Human-centered Geospatial Data Science,
highlighting the gaps it aims to bridge, its significance, and its key topics
and research. Geospatial Data Science, which derives geographic knowledge and
insights from large volumes of geospatial big data using advanced Geospatial
Artificial Intelligence (GeoAI), has been widely used to tackle a wide range of
geographic problems. However, it often overlooks the subjective human
experiences that fundamentally influence human-environment interactions, and
few strategies have been developed to ensure that these technologies follow
ethical guidelines and prioritize human values. Human-centered Geospatial Data
Science advocates for two primary focuses. First, it advances our understanding
of human-environment interactions by leveraging Geospatial Data Science to
measure and analyze human subjective experiences at place including emotion,
perception, cognition, and creativity. Second, it advocates for the development
of responsible and ethical Geospatial Data Science methods that protect
geoprivacy, enhance fairness and reduce bias, and improve the explainability
and transparency of geospatial technologies. With these two missions,
Human-centered Geospatial Data Sciences brings a fresh perspective to develop
and utilize geospatial technologies that positively impact society and benefit
human well-being and the humanities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LGL-BCI: A Motor-Imagery-Based Brain-Computer Interface with Geometric
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianchao Lu, Yuzhe Tian, Yang Zhang, Quan Z. Sheng, Xi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain--computer interfaces are groundbreaking technology whereby brain
signals are used to control external devices. Despite some advances in recent
years, electroencephalogram (EEG)-based motor-imagery tasks face challenges,
such as amplitude and phase variability and complex spatial correlations, with
a need for smaller models and faster inference. In this study, we develop a
prototype, called the Lightweight Geometric Learning Brain--Computer Interface
(LGL-BCI), which uses our customized geometric deep learning architecture for
swift model inference without sacrificing accuracy. LGL-BCI contains an EEG
channel selection module via a feature decomposition algorithm to reduce the
dimensionality of a symmetric positive definite matrix, providing adaptiveness
among the continuously changing EEG signal. Meanwhile, a built-in lossless
transformation helps boost the inference speed. The performance of our solution
was evaluated using two real-world EEG devices and two public EEG datasets.
LGL-BCI demonstrated significant improvements, achieving an accuracy of 82.54%
compared to 62.22% for the state-of-the-art approach. Furthermore, LGL-BCI uses
fewer parameters (64.9K vs. 183.7K), highlighting its computational efficiency.
These findings underscore both the superior accuracy and computational
efficiency of LGL-BCI, demonstrating the feasibility and robustness of
geometric deep learning in motor-imagery brain--computer interface
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2310.08051</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The evolution of volumetric video: A <span class="highlight-title">survey</span> of smart transcoding and
  compression approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Preetish Kakkar, Hariharan Ragothaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Volumetric video, the capture and display of three-dimensional (3D) imagery,
has emerged as a revolutionary technology poised to transform the media
landscape, enabling immersive experiences that transcend the limitations of
traditional 2D video. One of the key challenges in this domain is the efficient
delivery of these high-bandwidth, data-intensive volumetric video streams,
which requires innovative transcoding and compression techniques. This research
paper explores the state-of-the-art in volumetric video compression and
delivery, with a focus on the potential of AI-driven solutions to address the
unique challenges posed by this emerging medium.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-08T00:00:00Z">2025-01-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Software Engineering <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do <span class="highlight-title">Code</span> LLMs Understand Design Patterns? <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Pan, Xuefeng Song, Yunkun Wang, Rongyu Cao, Binhua Li, Yongbin Li, Han Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code Large Language Models (LLMs) demonstrate great versatility in adapting
to various downstream tasks, including code generation and completion, as well
as bug detection and fixing. However, Code LLMs often fail to capture existing
coding standards, leading to the generation of code that conflicts with the
required design patterns for a given project. As a result, developers must
post-process to adapt the generated code to the project's design norms. In this
work, we empirically investigate the biases of Code LLMs in software
development. Through carefully designed experiments, we assess the models'
understanding of design patterns across recognition, comprehension, and
generation. Our findings reveal that biases in Code LLMs significantly affect
the reliability of downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accpeted by llm4code workshop in ICSE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast, Fine-Grained Equivalence Checking for Neural Decompilers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Dramko, Claire Le Goues, Edward J. Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural decompilers are machine learning models that reconstruct the source
code from an executable program. Critical to the lifecycle of any machine
learning model is an evaluation of its effectiveness. However, existing
techniques for evaluating neural decompilation models have substantial
weaknesses, especially when it comes to showing the correctness of the neural
decompiler's predictions. To address this, we introduce codealign, a novel
instruction-level code equivalence technique designed for neural decompilers.
We provide a formal definition of a relation between equivalent instructions,
which we term an equivalence alignment. We show how codealign generates
equivalence alignments, then evaluate codealign by comparing it with symbolic
execution. Finally, we show how the information codealign provides-which parts
of the functions are equivalent and how well the variable names match-is
substantially more detailed than existing state-of-the-art evaluation metrics,
which report unitless numbers measuring similarity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Impact of Requirements Smells in <span class="highlight-title">Prompt</span>s: The Case of Automated
  Traceability <span class="chip">ICSE</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Vogelsang, Alexander Korn, Giovanna Broccia, Alessio Ferrari, Jannik Fischbach, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly used to generate software
artifacts, such as source code, tests, and trace links. Requirements play a
central role in shaping the input prompts that guide LLMs, as they are often
used as part of the prompts to synthesize the artifacts. However, the impact of
requirements formulation on LLM performance remains unclear. In this paper, we
investigate the role of requirements smells-indicators of potential issues like
ambiguity and inconsistency-when used in prompts for LLMs. We conducted
experiments using two LLMs focusing on automated trace link generation between
requirements and code. Our results show mixed outcomes: while requirements
smells had a small but significant effect when predicting whether a requirement
was implemented in a piece of code (i.e., a trace link exists), no significant
effect was observed when tracing the requirements with the associated lines of
code. These findings suggest that requirements smells can affect LLM
performance in certain SE tasks but may not uniformly impact all tasks. We
highlight the need for further research to understand these nuances and propose
future work toward developing guidelines for mitigating the negative effects of
requirements smells in AI-driven SE processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2025 IEEE/ACM 47th International Conference on Software
  Engineering: New Ideas and Emerging Results (ICSE-NIER)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teaching Simulation as a Research Method in Empirical Software
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Breno Bernard Nicolau de França, Dietmar Pfahl, Valdemar Vicente Graciano Neto, Nauman bin Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The chapter supports educators and postgraduate students in understanding the
role of simulation in software engineering research based on the authors'
experience. This way, it includes a background positioning simulation-based
studies in software engineering research, the proposition of learning
objectives for teaching simulation as a research method, and presents our
experience when teaching simulation concepts and practice. For educators, it
further provides learning objectives when teaching simulation, considering the
current state of the art in software engineering research and the necessary
guidance and recommended learning activities to achieve these objectives. For
students, it drives the learning path for those interested in learning this
method but had no opportunity to engage in an entire course on simulation in
the context of empirical research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test Smell: A Parasitic Energy Consumer in Software Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Rakib Hossain Misu, Jiawei Li, Adithya Bhattiprolu, Yang Liu, Eduardo Almeida, Iftekhar Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, energy efficiency research has focused on reducing energy
consumption at the hardware level and, more recently, in the design and coding
phases of the software development life cycle. However, software testing's
impact on energy consumption did not receive attention from the research
community. Specifically, how test code design quality and test smell (e.g.,
sub-optimal design and bad practices in test code) impact energy consumption
has not been investigated yet. This study examined 12 Apache projects to
analyze the association between test smell and its effects on energy
consumption in software testing. We conducted a mixed-method empirical analysis
from two dimensions; software (data mining in Apache projects) and developers'
views (a survey of 62 software practitioners). Our findings show that: 1) test
smell is associated with energy consumption in software testing. Specifically
smelly part of a test case consumes 10.92\% more energy compared to the
non-smelly part. 2) certain test smells are more energy-hungry than others, 3)
refactored test cases tend to consume less energy than their smelly
counterparts, and 4) most developers lack knowledge about test smells' impact
on energy consumption. We conclude the paper with several observations that can
direct future research and developments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SOPA: A Framework for Sustainability-Oriented Process Analysis and
  Re-design in Business Process Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01176v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01176v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Finn Klessascheck, Ingo Weber, Luise Pufahl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the continuous global degradation of the Earth's ecosystem due to
unsustainable human activity, it is increasingly important for enterprises to
evaluate the effects they have on the environment. Consequently, assessing the
impact of business processes on sustainability is becoming an important
consideration in the discipline of Business Process Management (BPM). However,
existing practical approaches that aim at a sustainability-oriented analysis of
business processes provide only a limited perspective on the environmental
impact caused. Further, they provide no clear and practically applicable
mechanism for sustainability-driven process analysis and re-design. Following a
design science methodology, we here propose and study SOPA, a framework for
sustainability-oriented process analysis and re-design. SOPA extends the BPM
life cycle by use of Life Cycle Assessment (LCA) for sustainability analysis in
combination with Activity-based Costing (ABC). We evaluate SOPA and its
usefulness with a case study, by means of an implementation to support the
approach, thereby also illustrating the practical applicability of this work.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-01-20T05:29:12.375352718Z">
            2025-01-20 05:29:12 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
